{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "offenseEval  -  A_updated",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv9EBpZ-ngqb",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://)## Imports\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipzYvEpynksu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 890
        },
        "outputId": "5175e0e9-a577-465f-a972-b21878005410"
      },
      "source": [
        "\"\"\"\n",
        "Created on Tue Oct 15 00:14:08 2019\n",
        "\n",
        "@author: anil\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "     \n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from nltk.tokenize import word_tokenize \n",
        "!pip install krovetzstemmer\n",
        "import krovetzstemmer\n",
        "import nltk\n",
        "nltk.download(\"popular\")\n",
        "from nltk.corpus import stopwords\n",
        "from krovetzstemmer import Stemmer\n",
        "from collections import Counter\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "!pip install -q keras\n",
        "import keras\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "print(accelerator)\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras import layers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: krovetzstemmer in /usr/local/lib/python3.6/dist-packages (0.6)\n",
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "cu80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rn7SnEJ5lIUK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "59d2d858-d15b-4669-a6a7-756ceee25d7e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DwnZt8uqSnj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "d2a77d7b-24e3-4130-a0c8-3ddb9f7a7faf"
      },
      "source": [
        "olid_training=pd.read_csv(\"/content/drive/My Drive/OLIDv1.0/olid-training-v1.0.tsv\", sep=\"\\t\")\n",
        "labels_a=pd.read_csv(\"/content/drive/My Drive/OLIDv1.0/labels-levela.csv\", header=None)\n",
        "labels_b=pd.read_csv(\"/content/drive/My Drive/OLIDv1.0/labels-levelb.csv\")\n",
        "labels_c=pd.read_csv(\"/content/drive/My Drive/OLIDv1.0/labels-levelc.csv\")\n",
        "test_a=pd.read_csv(\"/content/drive/My Drive/OLIDv1.0/testset-levela.tsv\",sep = \"\\t\" )\n",
        "test_b=pd.read_csv(\"/content/drive/My Drive/OLIDv1.0/testset-levelb.tsv\" ,sep = \"\\t\")\n",
        "test_c=pd.read_csv(\"/content/drive/My Drive/OLIDv1.0/testset-levelc.tsv\",sep = \"\\t\")\n",
        "import gensim\n",
        "# Load Google's pre-trained Word2Vec model.\n",
        "w2v_model = gensim.models.KeyedVectors.load_word2vec_format(\"/content/drive/My Drive/GoogleNews-vectors-negative300.bin.gz\", binary=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d48684815966>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0molid_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/OLIDv1.0/olid-training-v1.0.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlabels_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/OLIDv1.0/labels-levela.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlabels_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/OLIDv1.0/labels-levelb.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlabels_c\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/OLIDv1.0/labels-levelc.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/OLIDv1.0/testset-levela.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\t\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqqW_PhowI8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train=olid_training[[\"id\",\"tweet\",\"subtask_a\"]] # I will delete subtask_a later in the training part.\n",
        "X_test=test_a\n",
        "y_train=olid_training[\"subtask_a\"]\n",
        "y_test=labels_a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzRS-nFtvhPd",
        "colab_type": "text"
      },
      "source": [
        "## Reprocessing the Data\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNkgp6_2ltyo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "7b740e65-7d75-43d8-bec9-f8abd201dc9d"
      },
      "source": [
        "#1- Make all lowercase.\n",
        "\n",
        "print(\"Lowercase operation begins ! \")\n",
        "\n",
        "\n",
        "X_train.tweet = X_train.tweet.str.lower()\n",
        "X_test.tweet = X_test.tweet.str.lower()\n",
        "\n",
        "print(\"Lowercase operation is done ! \")\n",
        "\n",
        "\n",
        "#For train data\n",
        "filtered_tweets=[]\n",
        "for tweet in X_train[\"tweet\"]:\n",
        "    tweet_tokens = word_tokenize(tweet) \n",
        "\n",
        "    filtered_sentence = [w for w in tweet_tokens if ( w.isalpha()==True)] \n",
        "    filtered_tweets.append(filtered_sentence)\n",
        "  \n",
        "X_train[\"tweet_initial\"]=filtered_tweets\n",
        "\n",
        "\n",
        "\n",
        "#For test data initial tweets\n",
        "filtered_tweets=[]\n",
        "for tweet in X_test[\"tweet\"]:\n",
        "    tweet_tokens = word_tokenize(tweet) \n",
        "\n",
        "    filtered_sentence = [w for w in tweet_tokens if ((w.isalpha()==True))] \n",
        "      \n",
        "    filtered_tweets.append(filtered_sentence)\n",
        "\n",
        "X_test[\"tweet_initial\"]=filtered_tweets\n",
        "\n",
        "#for training data initial tweets\n",
        "\n",
        "#2-Stop word removal\n",
        "\n",
        "print(\"Stop word  and punctuation removal begins !\")\n",
        "\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "#For train data\n",
        "filtered_tweets=[]\n",
        "for tweet in X_train[\"tweet\"]:\n",
        "    tweet_tokens = word_tokenize(tweet) \n",
        "\n",
        "    filtered_sentence = [w for w in tweet_tokens if ((not w in stop_words) and w.isalpha()==True)] \n",
        "      \n",
        "    filtered_tweets.append(filtered_sentence)\n",
        "\n",
        "X_train[\"tweet_after_stopword\"]=filtered_tweets\n",
        "\n",
        "#For test data\n",
        "filtered_tweets=[]\n",
        "for tweet in X_test[\"tweet\"]:\n",
        "    tweet_tokens = word_tokenize(tweet) \n",
        "\n",
        "    filtered_sentence = [w for w in tweet_tokens if ((not w in stop_words) and w.isalpha()==True)] \n",
        "      \n",
        "    filtered_tweets.append(filtered_sentence)\n",
        "\n",
        "X_test[\"tweet_after_stopword\"]=filtered_tweets\n",
        "\n",
        "print(\"Stop word and punctuation removal is done !\")\n",
        "\n",
        "###################################################################333\n",
        "print(\"Stemming begins !\")\n",
        "\n",
        "stemmer = Stemmer()\n",
        "stemmer.stem('utilities')  # got: 'utility'\n",
        "\n",
        "#For stemming tranining data\n",
        "dum=X_train[\"tweet_after_stopword\"]\n",
        "stemmed_tweets=[]\n",
        "for tweet in dum:\n",
        "    tweet_arr=[]\n",
        "    for word in tweet :\n",
        "        #print(stemmer.stem(word))\n",
        "        tweet_arr.append(stemmer.stem(word))\n",
        "    stemmed_tweets.append(tweet_arr)\n",
        "    \n",
        "X_train[\"tweet_after_stemmed\"]=stemmed_tweets\n",
        "\n",
        "#For stemming Test data\n",
        "dum=X_test[\"tweet_after_stopword\"]\n",
        "stemmed_tweets=[]\n",
        "for tweet in dum:\n",
        "    tweet_arr=[]\n",
        "    for word in tweet :\n",
        "        #print(stemmer.stem(word))\n",
        "        tweet_arr.append(stemmer.stem(word))\n",
        "    stemmed_tweets.append(tweet_arr)\n",
        "    \n",
        "X_test[\"tweet_after_stemmed\"]=stemmed_tweets\n",
        "print(\"Stemming Done\")\n",
        "print(\"Reprocessing Done!\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lowercase operation begins ! \n",
            "Lowercase operation is done ! \n",
            "Stop word  and punctuation removal begins !\n",
            "Stop word and punctuation removal is done !\n",
            "Stemming begins !\n",
            "Stemming Done\n",
            "Reprocessing Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTLYWyxcwkrc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "2805b46c-606a-4509-a623-e90e48f55f78"
      },
      "source": [
        "X_train.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>subtask_a</th>\n",
              "      <th>tweet_initial</th>\n",
              "      <th>tweet_after_stopword</th>\n",
              "      <th>tweet_after_stemmed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>86426</td>\n",
              "      <td>@user she should ask a few native americans wh...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>[user, she, should, ask, a, few, native, ameri...</td>\n",
              "      <td>[user, ask, native, americans, take]</td>\n",
              "      <td>[user, ask, native, america, take]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>90194</td>\n",
              "      <td>@user @user go home you’re drunk!!! @user #mag...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>[user, user, go, home, you, re, drunk, user, m...</td>\n",
              "      <td>[user, user, go, home, drunk, user, maga, url]</td>\n",
              "      <td>[user, user, go, home, drunk, user, maga, url]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id  ...                             tweet_after_stemmed\n",
              "0  86426  ...              [user, ask, native, america, take]\n",
              "1  90194  ...  [user, user, go, home, drunk, user, maga, url]\n",
              "\n",
              "[2 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDdBEcVrwFZT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "210e9340-6824-4311-d699-69a2b1110b38"
      },
      "source": [
        "X_test.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>tweet_initial</th>\n",
              "      <th>tweet_after_stopword</th>\n",
              "      <th>tweet_after_stemmed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>15923</td>\n",
              "      <td>#whoisq #wherestheserver #dumpnike #declasfisa...</td>\n",
              "      <td>[whoisq, wherestheserver, dumpnike, declasfisa...</td>\n",
              "      <td>[whoisq, wherestheserver, dumpnike, declasfisa...</td>\n",
              "      <td>[whoisq, wherestheserver, dumpnike, declasfisa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>27014</td>\n",
              "      <td>#constitutionday is revered by conservatives, ...</td>\n",
              "      <td>[constitutionday, is, revered, by, conservativ...</td>\n",
              "      <td>[constitutionday, revered, conservatives, hate...</td>\n",
              "      <td>[constitutionday, revere, conservative, hate, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id  ...                                tweet_after_stemmed\n",
              "0  15923  ...  [whoisq, wherestheserver, dumpnike, declasfisa...\n",
              "1  27014  ...  [constitutionday, revere, conservative, hate, ...\n",
              "\n",
              "[2 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnRo1yGBxzUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjsobqQ9vZvj",
        "colab_type": "text"
      },
      "source": [
        "## ML Baseline : Naive Bayes\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v21Y4xkjn1aQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "fc63dba9-2185-4576-ca7e-235b7e6c03ec"
      },
      "source": [
        "print(\"ML models begins for Task1 !!\")\n",
        "print(\"Naive Bayes begins for Task 1\")\n",
        "\n",
        "# First create bag of words for both classes (off or not off)\n",
        "from collections import Counter\n",
        "# Create bag of words for class 1 in subtask-a\n",
        "bow_class_1=X_train[X_train[\"subtask_a\"]==\"OFF\"]\n",
        "bow_class_2=X_train[X_train[\"subtask_a\"]==\"NOT\"]\n",
        "#####################################################3\n",
        "bow_off_initial= Counter([word for tweets in bow_class_1[\"tweet_initial\"] for word in tweets])\n",
        "bow_not_initial= Counter([word for tweets in bow_class_2[\"tweet_initial\"] for word in tweets])\n",
        "########################################################\n",
        "\n",
        "bow_off_stopword= Counter([word for tweets in bow_class_1[\"tweet_after_stopword\"] for word in tweets])\n",
        "bow_not_stopword= Counter([word for tweets in bow_class_2[\"tweet_after_stopword\"] for word in tweets])\n",
        "\n",
        "print(\"Bag of word for stopword removed tweets are created !\")\n",
        "bow_off_stemmed = Counter([word for tweets in bow_class_1[\"tweet_after_stemmed\"] for word in tweets])\n",
        "bow_not_stemmed = Counter([word for tweets in bow_class_2[\"tweet_after_stemmed\"] for word in tweets])\n",
        "print(\"Bag of word for stemmed tweets are created !\")\n",
        "\n",
        "print(\"Naive Bayes training starting...\")\n",
        "\n",
        "# Find vocab size and total word size\n",
        "vocab_off_initial=len(bow_off_initial)\n",
        "vocab_not_initial=len(bow_not_initial)\n",
        "\n",
        "vocab_off_stemmed=len(bow_off_stemmed)\n",
        "vocab_not_stemmed=len(bow_not_stemmed)\n",
        "\n",
        "vocab_off_before_stemmed=len(bow_off_stopword)\n",
        "vocab_not_before_stemmed=len(bow_not_stopword)\n",
        "\n",
        "#################################33 for initial tweets\n",
        "\n",
        "total_count_off_initial=sum(bow_off_initial.values())\n",
        "total_count_not_initial=sum(bow_not_initial.values())\n",
        "\n",
        "#####################################\n",
        "\n",
        "\n",
        "print(\"Vocabulary size of stemmed data class OFF is :\",vocab_off_stemmed)\n",
        "print(\"Vocabulary size of stemmed data class NOT is :\",vocab_not_stemmed)\n",
        "\n",
        "total_count_off_stemmed =sum(bow_off_stemmed.values())\n",
        "total_count_not_stemmed =sum(bow_not_stemmed.values())\n",
        "\n",
        "total_count_off_before_stemmed =sum(bow_off_stopword.values())\n",
        "total_count_not_before_stemmed =sum(bow_not_stopword.values())\n",
        "\n",
        "print(\"Total occurence of words in stemmed data class OFF is :\",total_count_off_stemmed)\n",
        "print(\"Total occurence of words instemmed data class NOT is :\",total_count_not_stemmed)\n",
        "print(\"Naive Bayes Training is Done!\")\n",
        "\n",
        "#TO DO'S -> HASTHAGLERIN TEMIZLENMESI LAZIM , #USER BILGISI\n",
        "#test_a_copy\n",
        "\n",
        "print(\"Naive Bayes Testing starting...\")\n",
        "\n",
        "# p(c|i)=p(i|c)*p(c)\n",
        "# (i belonging class c ) = product( p of a test word j in class c ) * p of class c\n",
        "\n",
        "#class probabilities\n",
        "\n",
        "def NBClassifier(data,bow1,bow2,total_count_off,total_count_not,vocab_off,vocab_not): #parameters are important according to the vocab size , vocab count along the classes\n",
        "    predicted_labels=[]\n",
        "    p_off= len(X_train[X_train[\"subtask_a\"]==\"OFF\"]) /len(X_train) \n",
        "    p_not= len(X_train[X_train[\"subtask_a\"]==\"NOT\"]) /len(X_train) \n",
        "    for tweets in data: # i'th test instances\n",
        "        # every tweet has j word\n",
        "        # prob of test word j in  in class OFF\n",
        "        \n",
        "        result_prob_class_off=1\n",
        "        result_prob_class_not=1\n",
        "        for word in tweets:\n",
        "            #apply laplace smooting as well\n",
        "            #prob of word in class off\n",
        "            \n",
        "            #counts of word \"j\" in class c  + 1 / counts of word in class c + |v| + 1 -> \n",
        "            x1=bow1.get(word)\n",
        "            if x1 == None:\n",
        "                x1=0\n",
        "            result_prob_class_off= result_prob_class_off* ((x1 + 1 )/ (total_count_off + vocab_off + 1))\n",
        "            #prob off word in class not\n",
        "            #print(result_prob_class_off)\n",
        "            x2=bow2.get(word)\n",
        "            if x2 == None:\n",
        "                x2=0\n",
        "            \n",
        "            result_prob_class_not= result_prob_class_not* ((x2 + 1 )/ (total_count_not + vocab_not + 1))\n",
        "            #print(result_prob_class_not)\n",
        "        #Resulting probabilities , than we compare it to five a result\n",
        "        result_prob_class_off=result_prob_class_off*p_off\n",
        "        result_prob_class_not=result_prob_class_not*p_not\n",
        "        \n",
        "        if result_prob_class_off >  result_prob_class_not:\n",
        "            predicted_labels.append(\"OFF\")\n",
        "        else:\n",
        "            predicted_labels.append(\"NOT\")\n",
        "        \n",
        "        \n",
        "    print (\"Naive Bayes Test is completed...\")\n",
        "    \n",
        "    \n",
        "    evaluate=X_test[[\"id\",\"tweet\"]]\n",
        "    evaluate[\"Predicted_Labels\"]=predicted_labels\n",
        "    evaluate[\"Actual_Labels\"]=labels_a[1]\n",
        "    \n",
        "    predict_arr = [1 if i == \"OFF\" else 0 for i in evaluate[\"Predicted_Labels\"]]\n",
        "    actual_arr = [1 if i == \"OFF\" else 0 for i in evaluate[\"Actual_Labels\"]]\n",
        "       # print(\"ex1 prob of belong class off\",result_prob_class_off)\n",
        "        \n",
        "        #print(\"ex1 prob of belong class not\",result_prob_class_not)\n",
        "        \n",
        "        \n",
        "    from sklearn.metrics import classification_report\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    \n",
        "    a=classification_report(actual_arr, predict_arr)\n",
        "    b=confusion_matrix(actual_arr, predict_arr)\n",
        "    return a,b\n",
        "\n",
        "res1,re1=NBClassifier(X_test[\"tweet_after_stemmed\"],bow_off_stemmed,bow_not_stemmed,total_count_off_stemmed,total_count_not_stemmed,vocab_off_stemmed,vocab_not_stemmed)\n",
        "res2,re2=NBClassifier(X_test[\"tweet_after_stopword\"],bow_off_stopword,bow_not_stopword,total_count_off_before_stemmed,total_count_not_before_stemmed,vocab_off_before_stemmed,vocab_not_before_stemmed)\n",
        "res3,re3=NBClassifier(X_test[\"tweet_initial\"],bow_off_initial,bow_not_initial,total_count_off_initial,total_count_not_initial,vocab_off_initial,vocab_not_initial)\n",
        "print(\"Naive Bayes Confusion Matrix on only tokenizing for offensEval Task 1\")\n",
        "print(re3)\n",
        "print(res3)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Naive Bayes Confusion Matrix after Stopword Removal for offensEval Task 2\")\n",
        "print(re2)\n",
        "print(res2)\n",
        "\n",
        "print(\"Naive BayesConfusion Matrix after Stopword Removal + Stemming for offensEval Task 2\")\n",
        "\n",
        "print(re1)\n",
        "print(res1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ML models begins for Task1 !!\n",
            "Naive Bayes begins for Task 1\n",
            "Bag of word for stopword removed tweets are created !\n",
            "Bag of word for stemmed tweets are created !\n",
            "Naive Bayes training starting...\n",
            "Vocabulary size of stemmed data class OFF is : 7845\n",
            "Vocabulary size of stemmed data class NOT is : 11455\n",
            "Total occurence of words in stemmed data class OFF is : 57600\n",
            "Total occurence of words instemmed data class NOT is : 108903\n",
            "Naive Bayes Training is Done!\n",
            "Naive Bayes Testing starting...\n",
            "Naive Bayes Test is completed...\n",
            "Naive Bayes Test is completed...\n",
            "Naive Bayes Test is completed...\n",
            "Naive Bayes Confusion Matrix on only tokenizing for offensEval Task 1\n",
            "[[359 261]\n",
            " [ 46 194]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.58      0.70       620\n",
            "           1       0.43      0.81      0.56       240\n",
            "\n",
            "    accuracy                           0.64       860\n",
            "   macro avg       0.66      0.69      0.63       860\n",
            "weighted avg       0.76      0.64      0.66       860\n",
            "\n",
            "Naive Bayes Confusion Matrix after Stopword Removal for offensEval Task 2\n",
            "[[360 260]\n",
            " [ 49 191]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.58      0.70       620\n",
            "           1       0.42      0.80      0.55       240\n",
            "\n",
            "    accuracy                           0.64       860\n",
            "   macro avg       0.65      0.69      0.63       860\n",
            "weighted avg       0.75      0.64      0.66       860\n",
            "\n",
            "Naive BayesConfusion Matrix after Stopword Removal + Stemming for offensEval Task 2\n",
            "[[379 241]\n",
            " [ 54 186]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.61      0.72       620\n",
            "           1       0.44      0.78      0.56       240\n",
            "\n",
            "    accuracy                           0.66       860\n",
            "   macro avg       0.66      0.69      0.64       860\n",
            "weighted avg       0.75      0.66      0.67       860\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC99lPnVu7Gt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#confusion_matrix(actual_arr, predict_arr)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiZLDmqOeXur",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://)## More Data Reprocessing before CNN + LSTM\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0zXugJ7n5a-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "2b40655f-6cea-4465-b89a-48666399947b"
      },
      "source": [
        "#TEST Ya\n",
        "print(labels_a.shape)\n",
        "labels_a.head(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(860, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>15923</td>\n",
              "      <td>OFF</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       0    1\n",
              "0  15923  OFF"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5J1DAfNoCTK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# I did some reprocessing again and make data non-tokenize again\n",
        "\n",
        "#some reverse-reprocessingg due to i am decide to use Keras-tokenizer.\n",
        "m=[]\n",
        "for tweet in X_train[\"tweet_after_stemmed\"]:\n",
        "    a=\" \".join(tweet)\n",
        "    m.append(a)\n",
        "X_train[\"tweet_stemmed\"]=m\n",
        "n=[]\n",
        "for tweet in X_train[\"tweet_after_stopword\"]:\n",
        "    b=\" \".join(tweet)\n",
        "    n.append(b)\n",
        "X_train[\"tweet_stopword\"]=n\n",
        "\n",
        "z=[]\n",
        "for tweet in X_train[\"tweet_initial\"]:\n",
        "    b=\" \".join(tweet)\n",
        "    z.append(b)\n",
        "X_train[\"tweet_initial_nontoken\"]=z\n",
        "\n",
        "\n",
        "\n",
        "m=[]\n",
        "for tweet in X_test[\"tweet_after_stemmed\"]:\n",
        "    c=\" \".join(tweet)\n",
        "    m.append(c)\n",
        "X_test[\"tweet_stemmed\"]=m\n",
        "n=[]\n",
        "for tweet in X_test[\"tweet_after_stopword\"]:\n",
        "    d=\" \".join(tweet)\n",
        "    n.append(d)\n",
        "X_test[\"tweet_stopword\"]=n\n",
        "\n",
        "z=[]\n",
        "for tweet in X_test[\"tweet_initial\"]:\n",
        "    d=\" \".join(tweet)\n",
        "    z.append(d)\n",
        "X_test[\"tweet_initial_nontoken\"]=z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzZQdhjkoVMP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "2b682f06-8deb-42b9-acce-7d0441d02c1a"
      },
      "source": [
        "X_train.head(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>subtask_a</th>\n",
              "      <th>tweet_initial</th>\n",
              "      <th>tweet_after_stopword</th>\n",
              "      <th>tweet_after_stemmed</th>\n",
              "      <th>tweet_stemmed</th>\n",
              "      <th>tweet_stopword</th>\n",
              "      <th>tweet_initial_nontoken</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>86426</td>\n",
              "      <td>@user she should ask a few native americans wh...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>[user, she, should, ask, a, few, native, ameri...</td>\n",
              "      <td>[user, ask, native, americans, take]</td>\n",
              "      <td>[user, ask, native, america, take]</td>\n",
              "      <td>user ask native america take</td>\n",
              "      <td>user ask native americans take</td>\n",
              "      <td>user she should ask a few native americans wha...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id  ...                             tweet_initial_nontoken\n",
              "0  86426  ...  user she should ask a few native americans wha...\n",
              "\n",
              "[1 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYoD0d_uoibp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "caf87983-847a-4ed2-8b61-287040b5d38e"
      },
      "source": [
        "X_test.head(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>tweet_initial</th>\n",
              "      <th>tweet_after_stopword</th>\n",
              "      <th>tweet_after_stemmed</th>\n",
              "      <th>tweet_stemmed</th>\n",
              "      <th>tweet_stopword</th>\n",
              "      <th>tweet_initial_nontoken</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>15923</td>\n",
              "      <td>#whoisq #wherestheserver #dumpnike #declasfisa...</td>\n",
              "      <td>[whoisq, wherestheserver, dumpnike, declasfisa...</td>\n",
              "      <td>[whoisq, wherestheserver, dumpnike, declasfisa...</td>\n",
              "      <td>[whoisq, wherestheserver, dumpnike, declasfisa...</td>\n",
              "      <td>whoisq wherestheserver dumpnike declasfisa dem...</td>\n",
              "      <td>whoisq wherestheserver dumpnike declasfisa dem...</td>\n",
              "      <td>whoisq wherestheserver dumpnike declasfisa dem...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id  ...                             tweet_initial_nontoken\n",
              "0  15923  ...  whoisq wherestheserver dumpnike declasfisa dem...\n",
              "\n",
              "[1 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lzNyC1tuJJ4",
        "colab_type": "text"
      },
      "source": [
        "*italicized text*## Word Embedding\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_wkX-yFpArA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "outputId": "c8ab48cd-e7a7-4f71-e3ba-7556f0eeb162"
      },
      "source": [
        "print(\"Word Embedding Starts... for stemmed tweets\")\n",
        "# train your word embeddings during the training of your neural network. \n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=20000)\n",
        "tokenizer.fit_on_texts(X_train[\"tweet_initial_nontoken\"])\n",
        "X_train_initial = tokenizer.texts_to_sequences(X_train[\"tweet_initial_nontoken\"])\n",
        "X_test_initial = tokenizer.texts_to_sequences(X_test[\"tweet_initial_nontoken\"])\n",
        "vocab_size_initial = len(tokenizer.word_index) + 1 \n",
        "\n",
        "tokenizer = Tokenizer(num_words=20000)\n",
        "tokenizer.fit_on_texts(X_train[\"tweet_stopword\"])\n",
        "X_train_stopword=tokenizer.texts_to_sequences(X_train[\"tweet_stopword\"])\n",
        "X_test_stopword=tokenizer.texts_to_sequences(X_test[\"tweet_stopword\"])\n",
        "vocab_size_stopword=  len(tokenizer.word_index) + 1 \n",
        "\n",
        "tokenizer = Tokenizer(num_words=20000)\n",
        "tokenizer.fit_on_texts(X_train[\"tweet_stemmed\"])\n",
        "X_train_stemmed=tokenizer.texts_to_sequences(X_train[\"tweet_stemmed\"])\n",
        "X_test_stemmed=tokenizer.texts_to_sequences(X_test[\"tweet_stemmed\"])\n",
        "vocab_size_stemmed = len(tokenizer.word_index) + 1 \n",
        "\n",
        "\n",
        "y_train=[1 if i == \"OFF\" else 0 for i in X_train[\"subtask_a\"]]\n",
        "y_test= [1 if i == \"OFF\" else 0 for i in labels_a[1]]\n",
        "\n",
        "\n",
        "\n",
        "print(X_train[\"tweet_initial_nontoken\"][10000])\n",
        "print(X_train_initial[10000])\n",
        "print(X_train[\"tweet_stopword\"][10000])\n",
        "print(X_train_stopword[1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word Embedding Starts... for stemmed tweets\n",
            "user user real conservatives are stepping up\n",
            "[1, 1, 208, 48, 10, 3738, 61]\n",
            "user user real conservatives stepping\n",
            "[1, 1, 23, 205, 824, 1, 8, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnEADwqqsPPT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "outputId": "ea6d1248-49a6-431d-d251-250edfa4309e"
      },
      "source": [
        "#implement padding\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "max_len = 100\n",
        "X_train_initial = pad_sequences(X_train_initial, padding='post', maxlen=max_len)\n",
        "X_test_initial = pad_sequences(X_test_initial, padding='post', maxlen=max_len)\n",
        "\n",
        "X_train_stopword=pad_sequences(X_train_stopword, padding='post', maxlen=max_len)\n",
        "X_test_stopword=pad_sequences(X_test_stopword, padding='post', maxlen=max_len)\n",
        "\n",
        "X_train_stemmed=pad_sequences(X_train_stemmed, padding='post', maxlen=max_len)\n",
        "X_test_stemmed=pad_sequences(X_test_stemmed, padding='post', maxlen=max_len)\n",
        "\n",
        "print(X_train[\"tweet\"][30])\n",
        "print(X_train_initial[30])\n",
        "\n",
        "print(X_train[\"tweet_stopword\"][30])\n",
        "print(X_train_stopword[30])\n",
        "\n",
        "print(X_train[\"tweet_stemmed\"][30])\n",
        "print(X_train_stemmed[30])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "@user glad to see your friends are supporting metis rights as much as you are supporting zionists. url\n",
            "[   1  453    4   92   32  564   10  546 8649  284   37  116   37    5\n",
            "   10  546 6125   17    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "user glad see friends supporting metis rights much supporting zionists url\n",
            "[   1  335   25  444  427 8500  179   37  427 5978    2    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "user glad see friends support meti rights much support zionist url\n",
            "[   1  392   31  490   46 7069  199   49   46 5129    2    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJZSIA-mfk1W",
        "colab_type": "text"
      },
      "source": [
        "### Keras Metrics\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fjL7Uc_uk5K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
        "from keras.layers.embeddings import Embedding\n",
        "from sklearn.metrics import classification_report\n",
        "def recall_m(true_Y, pred_Y):\n",
        "        TP = K.sum(K.round(K.clip(true_Y * pred_Y, 0, 1)))\n",
        "        possible_pos = K.sum(K.round(K.clip(true_Y, 0, 1)))\n",
        "        rec = TP / (possible_pos + K.epsilon())\n",
        "        return rec\n",
        "\n",
        "def precision_m(true_Y, pred_Y):\n",
        "        true_positives = K.sum(K.round(K.clip(true_Y * pred_Y, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(pred_Y, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "\n",
        "def f1_m(true_Y, pred_Y):\n",
        "    pres = precision_m(true_Y, pred_Y)\n",
        "    rec = recall_m(true_Y, pred_Y)\n",
        "    return 2*((pres*rec)/(pres+rec+K.epsilon()))\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS5MbHzN2rmC",
        "colab_type": "text"
      },
      "source": [
        "## Conv Net with Embedding learned as part of  model\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyY6yMwUuwCh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f78f1082-4bd3-40e9-9e26-7aea8ea19ff7"
      },
      "source": [
        "def convnet_embedding_layer(vocab_size,X_train,X_test,y_train,y_test):\n",
        "\n",
        "  print(\"ConvNet Begins...\")\n",
        "  embedding_dim = 100\n",
        "  early_stopping = [EarlyStopping(monitor='val_acc',\n",
        "                          min_delta=0,restore_best_weights=True,\n",
        "                          patience=50,\n",
        "                          verbose=1, mode='auto')]\n",
        "  model = Sequential()\n",
        "  model.add(layers.Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
        "  model.add(layers.Conv1D(128, 3, activation='relu'))\n",
        "  model.add(layers.GlobalMaxPooling1D())\n",
        "  model.add(layers.Dense(100, activation='relu'))\n",
        "  model.add(layers.Dense(1, activation='sigmoid'))\n",
        "  model.compile(optimizer='adam',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['acc',f1_m,precision_m, recall_m])\n",
        "  model.summary()\n",
        "\n",
        "  history = model.fit(X_train, y_train,callbacks=early_stopping,\n",
        "                      epochs=150,\n",
        "                      verbose=1,\n",
        "                      validation_split=0.4,\n",
        "                      batch_size=10)\n",
        "  loss, accuracy, f1_score, precision, recall = model.evaluate(X_train, y_train, verbose=1)\n",
        "  print(\"CNN Training Loss: {:.4f}\".format(loss))\n",
        "  print(\"CNN Training Accuracy: {:.4f}\".format(accuracy))\n",
        "  print(\"CNN Training f1 score: {:.4f}\".format(f1_score))\n",
        "  print(\"CNN Training Precision: {:.4f}\".format(precision))\n",
        "  print(\"CNN Training Recall: {:.4f}\".format(recall))\n",
        "\n",
        "  loss, accuracy, f1_score, precision, recall = model.evaluate(X_test, y_test, verbose=1)\n",
        "  print(\"CNN Test Loss: {:.4f}\".format(loss))\n",
        "  print(\"CNN Test Accuracy: {:.4f}\".format(accuracy))\n",
        "  print(\"CNN Test f1 score: {:.4f}\".format(f1_score))\n",
        "  print(\"CNN Test Precision: {:.4f}\".format(precision))\n",
        "  print(\"CNN Test Recall: {:.4f}\".format(recall))\n",
        "\n",
        "  probs = model.predict(X_test, verbose=1)\n",
        "  predicted_classes = model.predict_classes(X_test, verbose=1)\n",
        "\n",
        "\n",
        "  print(classification_report(y_test, predicted_classes))\n",
        "\n",
        "  print(\"Conv Net ends..\")\n",
        "  \n",
        "convnet_embedding_layer(vocab_size_initial,X_train_initial,X_test_initial,y_train,y_test)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ConvNet Begins...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 100, 100)          1812200   \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 98, 128)           38528     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               12900     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 1,863,729\n",
            "Trainable params: 1,863,729\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 7944 samples, validate on 5296 samples\n",
            "Epoch 1/150\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "7944/7944 [==============================] - 19s 2ms/step - loss: 0.5576 - acc: 0.7261 - f1_m: 0.3359 - precision_m: 0.4461 - recall_m: 0.3036 - val_loss: 0.5361 - val_acc: 0.7366 - val_f1_m: 0.5637 - val_precision_m: 0.5952 - val_recall_m: 0.6005\n",
            "Epoch 2/150\n",
            "7944/7944 [==============================] - 9s 1ms/step - loss: 0.3590 - acc: 0.8511 - f1_m: 0.7228 - precision_m: 0.7953 - recall_m: 0.7152 - val_loss: 0.5743 - val_acc: 0.7657 - val_f1_m: 0.5002 - val_precision_m: 0.6452 - val_recall_m: 0.4516\n",
            "Epoch 3/150\n",
            "7944/7944 [==============================] - 9s 1ms/step - loss: 0.1410 - acc: 0.9509 - f1_m: 0.9047 - precision_m: 0.9214 - recall_m: 0.9136 - val_loss: 0.7483 - val_acc: 0.7319 - val_f1_m: 0.5485 - val_precision_m: 0.5863 - val_recall_m: 0.5798\n",
            "Epoch 4/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0431 - acc: 0.9882 - f1_m: 0.9538 - precision_m: 0.9570 - recall_m: 0.9569 - val_loss: 0.8926 - val_acc: 0.7062 - val_f1_m: 0.5524 - val_precision_m: 0.5401 - val_recall_m: 0.6384\n",
            "Epoch 5/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0189 - acc: 0.9951 - f1_m: 0.9714 - precision_m: 0.9732 - recall_m: 0.9724 - val_loss: 1.2474 - val_acc: 0.7502 - val_f1_m: 0.4920 - val_precision_m: 0.6265 - val_recall_m: 0.4534\n",
            "Epoch 6/150\n",
            "7944/7944 [==============================] - 11s 1ms/step - loss: 0.0152 - acc: 0.9967 - f1_m: 0.9760 - precision_m: 0.9766 - recall_m: 0.9770 - val_loss: 1.1565 - val_acc: 0.7270 - val_f1_m: 0.5135 - val_precision_m: 0.5717 - val_recall_m: 0.5248\n",
            "Epoch 7/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0101 - acc: 0.9980 - f1_m: 0.9791 - precision_m: 0.9797 - recall_m: 0.9795 - val_loss: 1.3283 - val_acc: 0.7441 - val_f1_m: 0.5233 - val_precision_m: 0.6114 - val_recall_m: 0.5133\n",
            "Epoch 8/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0109 - acc: 0.9980 - f1_m: 0.9720 - precision_m: 0.9729 - recall_m: 0.9725 - val_loss: 1.3576 - val_acc: 0.7217 - val_f1_m: 0.5378 - val_precision_m: 0.5566 - val_recall_m: 0.5865\n",
            "Epoch 9/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0323 - acc: 0.9893 - f1_m: 0.9649 - precision_m: 0.9688 - recall_m: 0.9668 - val_loss: 1.3774 - val_acc: 0.7275 - val_f1_m: 0.5090 - val_precision_m: 0.5838 - val_recall_m: 0.5076\n",
            "Epoch 10/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0171 - acc: 0.9956 - f1_m: 0.9741 - precision_m: 0.9762 - recall_m: 0.9744 - val_loss: 1.4709 - val_acc: 0.7181 - val_f1_m: 0.5442 - val_precision_m: 0.5582 - val_recall_m: 0.5951\n",
            "Epoch 11/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0076 - acc: 0.9980 - f1_m: 0.9790 - precision_m: 0.9801 - recall_m: 0.9791 - val_loss: 1.6127 - val_acc: 0.7183 - val_f1_m: 0.5382 - val_precision_m: 0.5605 - val_recall_m: 0.5839\n",
            "Epoch 12/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0065 - acc: 0.9985 - f1_m: 0.9861 - precision_m: 0.9871 - recall_m: 0.9859 - val_loss: 1.6627 - val_acc: 0.7075 - val_f1_m: 0.5428 - val_precision_m: 0.5406 - val_recall_m: 0.6122\n",
            "Epoch 13/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0059 - acc: 0.9989 - f1_m: 0.9827 - precision_m: 0.9824 - recall_m: 0.9835 - val_loss: 1.7648 - val_acc: 0.7290 - val_f1_m: 0.5451 - val_precision_m: 0.5768 - val_recall_m: 0.5805\n",
            "Epoch 14/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0063 - acc: 0.9987 - f1_m: 0.9801 - precision_m: 0.9800 - recall_m: 0.9810 - val_loss: 1.6979 - val_acc: 0.7340 - val_f1_m: 0.5076 - val_precision_m: 0.5881 - val_recall_m: 0.5028\n",
            "Epoch 15/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0098 - acc: 0.9979 - f1_m: 0.9767 - precision_m: 0.9782 - recall_m: 0.9763 - val_loss: 1.8765 - val_acc: 0.7300 - val_f1_m: 0.5097 - val_precision_m: 0.5857 - val_recall_m: 0.5065\n",
            "Epoch 16/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0185 - acc: 0.9937 - f1_m: 0.9681 - precision_m: 0.9691 - recall_m: 0.9699 - val_loss: 1.8480 - val_acc: 0.7266 - val_f1_m: 0.4934 - val_precision_m: 0.5771 - val_recall_m: 0.4869\n",
            "Epoch 17/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0075 - acc: 0.9977 - f1_m: 0.9805 - precision_m: 0.9819 - recall_m: 0.9809 - val_loss: 1.9311 - val_acc: 0.7200 - val_f1_m: 0.5179 - val_precision_m: 0.5663 - val_recall_m: 0.5364\n",
            "Epoch 18/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0038 - acc: 0.9986 - f1_m: 0.9863 - precision_m: 0.9870 - recall_m: 0.9864 - val_loss: 1.9358 - val_acc: 0.7266 - val_f1_m: 0.5148 - val_precision_m: 0.5695 - val_recall_m: 0.5261\n",
            "Epoch 19/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0035 - acc: 0.9989 - f1_m: 0.9832 - precision_m: 0.9833 - recall_m: 0.9836 - val_loss: 1.9165 - val_acc: 0.7188 - val_f1_m: 0.5217 - val_precision_m: 0.5575 - val_recall_m: 0.5487\n",
            "Epoch 20/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0023 - acc: 0.9987 - f1_m: 0.9897 - precision_m: 0.9898 - recall_m: 0.9900 - val_loss: 2.1493 - val_acc: 0.7300 - val_f1_m: 0.4726 - val_precision_m: 0.5847 - val_recall_m: 0.4469\n",
            "Epoch 21/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0021 - acc: 0.9987 - f1_m: 0.9749 - precision_m: 0.9757 - recall_m: 0.9746 - val_loss: 2.2335 - val_acc: 0.7338 - val_f1_m: 0.4901 - val_precision_m: 0.5887 - val_recall_m: 0.4723\n",
            "Epoch 22/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0063 - acc: 0.9977 - f1_m: 0.9809 - precision_m: 0.9820 - recall_m: 0.9812 - val_loss: 2.2769 - val_acc: 0.7035 - val_f1_m: 0.5388 - val_precision_m: 0.5350 - val_recall_m: 0.6056\n",
            "Epoch 23/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0101 - acc: 0.9957 - f1_m: 0.9779 - precision_m: 0.9801 - recall_m: 0.9780 - val_loss: 2.3053 - val_acc: 0.7260 - val_f1_m: 0.4979 - val_precision_m: 0.5696 - val_recall_m: 0.4966\n",
            "Epoch 24/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0042 - acc: 0.9984 - f1_m: 0.9819 - precision_m: 0.9831 - recall_m: 0.9817 - val_loss: 2.3771 - val_acc: 0.7232 - val_f1_m: 0.5009 - val_precision_m: 0.5691 - val_recall_m: 0.5044\n",
            "Epoch 25/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0018 - acc: 0.9990 - f1_m: 0.9799 - precision_m: 0.9813 - recall_m: 0.9793 - val_loss: 2.3921 - val_acc: 0.7224 - val_f1_m: 0.5147 - val_precision_m: 0.5625 - val_recall_m: 0.5324\n",
            "Epoch 26/150\n",
            "7944/7944 [==============================] - 11s 1ms/step - loss: 0.0014 - acc: 0.9991 - f1_m: 0.9882 - precision_m: 0.9889 - recall_m: 0.9881 - val_loss: 2.5821 - val_acc: 0.7258 - val_f1_m: 0.4693 - val_precision_m: 0.5720 - val_recall_m: 0.4469\n",
            "Epoch 27/150\n",
            "7944/7944 [==============================] - 11s 1ms/step - loss: 0.0029 - acc: 0.9991 - f1_m: 0.9788 - precision_m: 0.9792 - recall_m: 0.9786 - val_loss: 2.4171 - val_acc: 0.7175 - val_f1_m: 0.4560 - val_precision_m: 0.5553 - val_recall_m: 0.4375\n",
            "Epoch 28/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0079 - acc: 0.9976 - f1_m: 0.9657 - precision_m: 0.9661 - recall_m: 0.9669 - val_loss: 2.3371 - val_acc: 0.6922 - val_f1_m: 0.5359 - val_precision_m: 0.5175 - val_recall_m: 0.6266\n",
            "Epoch 29/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0082 - acc: 0.9975 - f1_m: 0.9831 - precision_m: 0.9843 - recall_m: 0.9833 - val_loss: 2.4135 - val_acc: 0.7283 - val_f1_m: 0.4696 - val_precision_m: 0.5851 - val_recall_m: 0.4417\n",
            "Epoch 30/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0025 - acc: 0.9987 - f1_m: 0.9843 - precision_m: 0.9846 - recall_m: 0.9845 - val_loss: 2.2958 - val_acc: 0.7260 - val_f1_m: 0.5280 - val_precision_m: 0.5678 - val_recall_m: 0.5549\n",
            "Epoch 31/150\n",
            "7944/7944 [==============================] - 9s 1ms/step - loss: 0.0016 - acc: 0.9991 - f1_m: 0.9885 - precision_m: 0.9889 - recall_m: 0.9885 - val_loss: 2.3758 - val_acc: 0.7319 - val_f1_m: 0.5075 - val_precision_m: 0.5866 - val_recall_m: 0.5025\n",
            "Epoch 32/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0017 - acc: 0.9991 - f1_m: 0.9849 - precision_m: 0.9865 - recall_m: 0.9841 - val_loss: 2.4287 - val_acc: 0.7279 - val_f1_m: 0.5246 - val_precision_m: 0.5752 - val_recall_m: 0.5439\n",
            "Epoch 33/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0033 - acc: 0.9985 - f1_m: 0.9764 - precision_m: 0.9768 - recall_m: 0.9768 - val_loss: 2.5638 - val_acc: 0.7279 - val_f1_m: 0.4625 - val_precision_m: 0.5852 - val_recall_m: 0.4330\n",
            "Epoch 34/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0053 - acc: 0.9982 - f1_m: 0.9741 - precision_m: 0.9758 - recall_m: 0.9737 - val_loss: 2.2119 - val_acc: 0.7190 - val_f1_m: 0.5300 - val_precision_m: 0.5600 - val_recall_m: 0.5662\n",
            "Epoch 35/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0019 - acc: 0.9990 - f1_m: 0.9729 - precision_m: 0.9733 - recall_m: 0.9732 - val_loss: 2.6456 - val_acc: 0.7336 - val_f1_m: 0.4608 - val_precision_m: 0.5893 - val_recall_m: 0.4258\n",
            "Epoch 36/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0015 - acc: 0.9991 - f1_m: 0.9753 - precision_m: 0.9762 - recall_m: 0.9750 - val_loss: 2.4355 - val_acc: 0.7294 - val_f1_m: 0.5002 - val_precision_m: 0.5781 - val_recall_m: 0.4962\n",
            "Epoch 37/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0013 - acc: 0.9991 - f1_m: 0.9758 - precision_m: 0.9765 - recall_m: 0.9756 - val_loss: 2.5867 - val_acc: 0.7151 - val_f1_m: 0.5208 - val_precision_m: 0.5508 - val_recall_m: 0.5576\n",
            "Epoch 38/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0067 - acc: 0.9975 - f1_m: 0.9740 - precision_m: 0.9755 - recall_m: 0.9735 - val_loss: 2.3705 - val_acc: 0.7188 - val_f1_m: 0.4601 - val_precision_m: 0.5598 - val_recall_m: 0.4428\n",
            "Epoch 39/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0030 - acc: 0.9987 - f1_m: 0.9855 - precision_m: 0.9865 - recall_m: 0.9852 - val_loss: 2.5908 - val_acc: 0.7173 - val_f1_m: 0.4835 - val_precision_m: 0.5566 - val_recall_m: 0.4830\n",
            "Epoch 40/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0015 - acc: 0.9992 - f1_m: 0.9835 - precision_m: 0.9846 - recall_m: 0.9830 - val_loss: 2.6309 - val_acc: 0.7141 - val_f1_m: 0.4783 - val_precision_m: 0.5472 - val_recall_m: 0.4802\n",
            "Epoch 41/150\n",
            "7944/7944 [==============================] - 11s 1ms/step - loss: 0.0012 - acc: 0.9994 - f1_m: 0.9804 - precision_m: 0.9809 - recall_m: 0.9800 - val_loss: 2.6996 - val_acc: 0.7147 - val_f1_m: 0.4736 - val_precision_m: 0.5525 - val_recall_m: 0.4694\n",
            "Epoch 42/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0011 - acc: 0.9995 - f1_m: 0.9751 - precision_m: 0.9761 - recall_m: 0.9745 - val_loss: 2.7760 - val_acc: 0.7124 - val_f1_m: 0.4795 - val_precision_m: 0.5456 - val_recall_m: 0.4830\n",
            "Epoch 43/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0012 - acc: 0.9995 - f1_m: 0.9891 - precision_m: 0.9896 - recall_m: 0.9888 - val_loss: 2.8686 - val_acc: 0.7253 - val_f1_m: 0.4596 - val_precision_m: 0.5690 - val_recall_m: 0.4336\n",
            "Epoch 44/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0035 - acc: 0.9986 - f1_m: 0.9826 - precision_m: 0.9838 - recall_m: 0.9822 - val_loss: 3.1349 - val_acc: 0.7268 - val_f1_m: 0.4066 - val_precision_m: 0.5652 - val_recall_m: 0.3544\n",
            "Epoch 45/150\n",
            "7944/7944 [==============================] - 11s 1ms/step - loss: 0.0051 - acc: 0.9984 - f1_m: 0.9797 - precision_m: 0.9806 - recall_m: 0.9798 - val_loss: 2.8401 - val_acc: 0.7171 - val_f1_m: 0.4713 - val_precision_m: 0.5501 - val_recall_m: 0.4631\n",
            "Epoch 46/150\n",
            "7944/7944 [==============================] - 11s 1ms/step - loss: 0.0012 - acc: 0.9992 - f1_m: 0.9797 - precision_m: 0.9804 - recall_m: 0.9794 - val_loss: 2.9449 - val_acc: 0.7179 - val_f1_m: 0.4690 - val_precision_m: 0.5555 - val_recall_m: 0.4567\n",
            "Epoch 47/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0011 - acc: 0.9994 - f1_m: 0.9789 - precision_m: 0.9794 - recall_m: 0.9786 - val_loss: 3.0047 - val_acc: 0.7179 - val_f1_m: 0.4714 - val_precision_m: 0.5552 - val_recall_m: 0.4607\n",
            "Epoch 48/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0010 - acc: 0.9995 - f1_m: 0.9864 - precision_m: 0.9874 - recall_m: 0.9858 - val_loss: 3.0436 - val_acc: 0.7171 - val_f1_m: 0.4750 - val_precision_m: 0.5515 - val_recall_m: 0.4687\n",
            "Epoch 49/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0011 - acc: 0.9995 - f1_m: 0.9850 - precision_m: 0.9856 - recall_m: 0.9845 - val_loss: 3.0559 - val_acc: 0.7185 - val_f1_m: 0.4705 - val_precision_m: 0.5558 - val_recall_m: 0.4584\n",
            "Epoch 50/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0010 - acc: 0.9994 - f1_m: 0.9829 - precision_m: 0.9834 - recall_m: 0.9826 - val_loss: 3.1143 - val_acc: 0.7158 - val_f1_m: 0.4831 - val_precision_m: 0.5492 - val_recall_m: 0.4847\n",
            "Epoch 51/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0041 - acc: 0.9990 - f1_m: 0.9803 - precision_m: 0.9810 - recall_m: 0.9800 - val_loss: 2.8776 - val_acc: 0.7230 - val_f1_m: 0.5106 - val_precision_m: 0.5652 - val_recall_m: 0.5201\n",
            "Epoch 52/150\n",
            "7944/7944 [==============================] - 10s 1ms/step - loss: 0.0020 - acc: 0.9989 - f1_m: 0.9733 - precision_m: 0.9736 - recall_m: 0.9734 - val_loss: 3.1955 - val_acc: 0.7253 - val_f1_m: 0.4880 - val_precision_m: 0.5687 - val_recall_m: 0.4798\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00052: early stopping\n",
            "13240/13240 [==============================] - 1s 96us/step\n",
            "CNN Training Loss: 0.3244\n",
            "CNN Training Accuracy: 0.8751\n",
            "CNN Training f1 score: 0.7704\n",
            "CNN Training Precision: 0.8676\n",
            "CNN Training Recall: 0.7140\n",
            "860/860 [==============================] - 0s 198us/step\n",
            "CNN Test Loss: 0.4961\n",
            "CNN Test Accuracy: 0.7988\n",
            "CNN Test f1 score: 0.5423\n",
            "CNN Test Precision: 0.7052\n",
            "CNN Test Recall: 0.4516\n",
            "860/860 [==============================] - 0s 100us/step\n",
            "860/860 [==============================] - 0s 47us/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       620\n",
            "           1       0.72      0.46      0.56       240\n",
            "\n",
            "    accuracy                           0.80       860\n",
            "   macro avg       0.77      0.70      0.72       860\n",
            "weighted avg       0.79      0.80      0.78       860\n",
            "\n",
            "Conv Net ends..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-VQFNGR_zlJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFRrG3LC7dE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nINR8EXB3Jtj",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## LSTM with Embedding learned as part of  model\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kFZ8Ur05a8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
        "from keras.layers.embeddings import Embedding\n",
        "from sklearn.metrics import classification_report\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cpqWd3R245P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f1370ffa-63f4-4286-e7ed-d4f036df8073"
      },
      "source": [
        "def lstm_embedding_layer(vocab_size,X_train,X_test,y_train,y_test):\n",
        "  early_stopping = [EarlyStopping(monitor='val_loss',\n",
        "                          min_delta=0,restore_best_weights=True,\n",
        "                          patience=50,\n",
        "                          verbose=1, mode='auto')]\n",
        "  ## Network architecture buraya ekstradann convolutionoal layer eklendi.\n",
        "  model = keras.Sequential()\n",
        "  model.add(keras.layers.Embedding(vocab_size, 100, input_length=100))\n",
        "  model.add(keras.layers.Dropout(0.2))\n",
        "  model.add(keras.layers.Conv1D(128, 3, activation='relu'))\n",
        "  model.add(keras.layers.MaxPooling1D(pool_size=4))\n",
        "  model.add(keras.layers.LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "  model.add(keras.layers.Dense(100, activation='relu'))\n",
        "  model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc',f1_m,precision_m, recall_m])\n",
        "  ## Fit the model\n",
        "  model.fit(X_train, y_train, validation_split=0.4, epochs=150,callbacks=early_stopping)\n",
        "  loss, accuracy, f1_score, precision, recall = model.evaluate(X_train, y_train, verbose=1)\n",
        "  print(\"LSTM Training Loss: {:.4f}\".format(loss))\n",
        "  print(\"LSTM Training Accuracy: {:.4f}\".format(accuracy))\n",
        "  print(\"LSTM Training f1 score: {:.4f}\".format(f1_score))\n",
        "  print(\"LSTM Training Precision: {:.4f}\".format(precision))\n",
        "  print(\"LSTM Training Recall: {:.4f}\".format(recall))\n",
        "\n",
        "  loss, accuracy, f1_score, precision, recall = model.evaluate(X_test, y_test, verbose=1)\n",
        "  print(\"LSTM Test Loss: {:.4f}\".format(loss))\n",
        "  print(\"LSTM Test Accuracy: {:.4f}\".format(accuracy))\n",
        "  print(\"LSTM Test f1 score: {:.4f}\".format(f1_score))\n",
        "  print(\"LSTM Test Precision: {:.4f}\".format(precision))\n",
        "  print(\"LSTM Test Recall: {:.4f}\".format(recall))\n",
        "\n",
        "  probs = model.predict(X_test, verbose=1)\n",
        "  predicted_classes = model.predict_classes(X_test, verbose=1)\n",
        "\n",
        "\n",
        "  print(classification_report(y_test, predicted_classes))\n",
        "\n",
        "  print(\"LSTM  ends..\")\n",
        "  #print(predicted_classes)\n",
        "lstm_embedding_layer(vocab_size_initial,X_train_initial,X_test_initial,y_train,y_test)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "Train on 7944 samples, validate on 5296 samples\n",
            "Epoch 1/150\n",
            "7944/7944 [==============================] - 27s 3ms/step - loss: 0.6420 - acc: 0.6665 - f1_m: 0.0019 - precision_m: 0.0015 - recall_m: 0.0027 - val_loss: 0.6378 - val_acc: 0.6682 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 2/150\n",
            "7944/7944 [==============================] - 25s 3ms/step - loss: 0.5577 - acc: 0.7360 - f1_m: 0.4652 - precision_m: 0.6163 - recall_m: 0.4061 - val_loss: 0.5369 - val_acc: 0.7292 - val_f1_m: 0.5923 - val_precision_m: 0.5844 - val_recall_m: 0.6262\n",
            "Epoch 3/150\n",
            "7944/7944 [==============================] - 26s 3ms/step - loss: 0.3956 - acc: 0.8420 - f1_m: 0.7362 - precision_m: 0.8050 - recall_m: 0.7007 - val_loss: 0.5399 - val_acc: 0.7460 - val_f1_m: 0.5941 - val_precision_m: 0.6237 - val_recall_m: 0.5892\n",
            "Epoch 4/150\n",
            "7944/7944 [==============================] - 26s 3ms/step - loss: 0.2578 - acc: 0.9100 - f1_m: 0.8579 - precision_m: 0.8647 - recall_m: 0.8654 - val_loss: 0.6825 - val_acc: 0.7494 - val_f1_m: 0.5677 - val_precision_m: 0.6509 - val_recall_m: 0.5259\n",
            "Epoch 5/150\n",
            "7944/7944 [==============================] - 26s 3ms/step - loss: 0.1764 - acc: 0.9440 - f1_m: 0.9122 - precision_m: 0.9309 - recall_m: 0.9040 - val_loss: 0.7179 - val_acc: 0.7255 - val_f1_m: 0.5794 - val_precision_m: 0.5790 - val_recall_m: 0.6023\n",
            "Epoch 6/150\n",
            "7944/7944 [==============================] - 26s 3ms/step - loss: 0.1270 - acc: 0.9655 - f1_m: 0.9449 - precision_m: 0.9555 - recall_m: 0.9400 - val_loss: 0.9076 - val_acc: 0.7298 - val_f1_m: 0.5695 - val_precision_m: 0.5947 - val_recall_m: 0.5678\n",
            "Epoch 7/150\n",
            "7944/7944 [==============================] - 25s 3ms/step - loss: 0.1018 - acc: 0.9714 - f1_m: 0.9537 - precision_m: 0.9648 - recall_m: 0.9476 - val_loss: 0.8255 - val_acc: 0.7313 - val_f1_m: 0.5459 - val_precision_m: 0.6096 - val_recall_m: 0.5141\n",
            "Epoch 8/150\n",
            "7944/7944 [==============================] - 26s 3ms/step - loss: 0.0830 - acc: 0.9785 - f1_m: 0.9651 - precision_m: 0.9782 - recall_m: 0.9561 - val_loss: 1.0495 - val_acc: 0.7285 - val_f1_m: 0.5594 - val_precision_m: 0.5963 - val_recall_m: 0.5486\n",
            "Epoch 9/150\n",
            "7944/7944 [==============================] - 26s 3ms/step - loss: 0.0648 - acc: 0.9841 - f1_m: 0.9750 - precision_m: 0.9858 - recall_m: 0.9667 - val_loss: 1.1475 - val_acc: 0.7264 - val_f1_m: 0.5641 - val_precision_m: 0.5872 - val_recall_m: 0.5628\n",
            "Epoch 10/150\n",
            "7944/7944 [==============================] - 26s 3ms/step - loss: 0.0564 - acc: 0.9863 - f1_m: 0.9785 - precision_m: 0.9887 - recall_m: 0.9706 - val_loss: 0.9724 - val_acc: 0.7309 - val_f1_m: 0.5385 - val_precision_m: 0.6109 - val_recall_m: 0.5023\n",
            "Epoch 11/150\n",
            "7944/7944 [==============================] - 26s 3ms/step - loss: 0.0519 - acc: 0.9883 - f1_m: 0.9813 - precision_m: 0.9915 - recall_m: 0.9728 - val_loss: 1.1450 - val_acc: 0.7324 - val_f1_m: 0.5451 - val_precision_m: 0.6180 - val_recall_m: 0.5086\n",
            "Epoch 12/150\n",
            "7944/7944 [==============================] - 26s 3ms/step - loss: 0.0475 - acc: 0.9877 - f1_m: 0.9804 - precision_m: 0.9876 - recall_m: 0.9753 - val_loss: 1.4275 - val_acc: 0.7190 - val_f1_m: 0.5424 - val_precision_m: 0.5836 - val_recall_m: 0.5278\n",
            "Epoch 13/150\n",
            "7944/7944 [==============================] - 26s 3ms/step - loss: 0.0426 - acc: 0.9898 - f1_m: 0.9830 - precision_m: 0.9916 - recall_m: 0.9763 - val_loss: 1.3134 - val_acc: 0.7166 - val_f1_m: 0.5454 - val_precision_m: 0.5744 - val_recall_m: 0.5392\n",
            "Epoch 14/150\n",
            "7944/7944 [==============================] - 26s 3ms/step - loss: 0.0392 - acc: 0.9904 - f1_m: 0.9851 - precision_m: 0.9919 - recall_m: 0.9798 - val_loss: 1.3746 - val_acc: 0.7228 - val_f1_m: 0.5326 - val_precision_m: 0.5934 - val_recall_m: 0.5029\n",
            "Epoch 15/150\n",
            "7944/7944 [==============================] - 26s 3ms/step - loss: 0.0436 - acc: 0.9897 - f1_m: 0.9840 - precision_m: 0.9896 - recall_m: 0.9803 - val_loss: 1.1963 - val_acc: 0.7241 - val_f1_m: 0.5228 - val_precision_m: 0.6055 - val_recall_m: 0.4799\n",
            "Epoch 16/150\n",
            "7944/7944 [==============================] - 26s 3ms/step - loss: 0.0346 - acc: 0.9931 - f1_m: 0.9887 - precision_m: 0.9951 - recall_m: 0.9835 - val_loss: 1.4005 - val_acc: 0.7079 - val_f1_m: 0.5437 - val_precision_m: 0.5562 - val_recall_m: 0.5533\n",
            "Epoch 17/150\n",
            "7944/7944 [==============================] - 25s 3ms/step - loss: 0.0354 - acc: 0.9921 - f1_m: 0.9883 - precision_m: 0.9927 - recall_m: 0.9849 - val_loss: 1.3473 - val_acc: 0.7219 - val_f1_m: 0.5374 - val_precision_m: 0.5900 - val_recall_m: 0.5158\n",
            "Epoch 18/150\n",
            "7944/7944 [==============================] - 26s 3ms/step - loss: 0.0320 - acc: 0.9924 - f1_m: 0.9884 - precision_m: 0.9924 - recall_m: 0.9855 - val_loss: 1.2684 - val_acc: 0.7275 - val_f1_m: 0.4941 - val_precision_m: 0.6317 - val_recall_m: 0.4262\n",
            "Epoch 19/150\n",
            "7944/7944 [==============================] - 26s 3ms/step - loss: 0.0315 - acc: 0.9923 - f1_m: 0.9881 - precision_m: 0.9931 - recall_m: 0.9843 - val_loss: 1.3409 - val_acc: 0.7160 - val_f1_m: 0.5247 - val_precision_m: 0.5832 - val_recall_m: 0.4969\n",
            "Epoch 20/150\n",
            "7944/7944 [==============================] - 26s 3ms/step - loss: 0.0277 - acc: 0.9931 - f1_m: 0.9895 - precision_m: 0.9935 - recall_m: 0.9867 - val_loss: 1.3308 - val_acc: 0.7268 - val_f1_m: 0.5154 - val_precision_m: 0.6213 - val_recall_m: 0.4613\n",
            "Epoch 21/150\n",
            "7944/7944 [==============================] - 25s 3ms/step - loss: 0.0270 - acc: 0.9946 - f1_m: 0.9909 - precision_m: 0.9969 - recall_m: 0.9860 - val_loss: 1.4531 - val_acc: 0.7285 - val_f1_m: 0.5401 - val_precision_m: 0.6092 - val_recall_m: 0.5042\n",
            "Epoch 22/150\n",
            "7944/7944 [==============================] - 25s 3ms/step - loss: 0.0270 - acc: 0.9935 - f1_m: 0.9903 - precision_m: 0.9957 - recall_m: 0.9857 - val_loss: 1.3469 - val_acc: 0.7255 - val_f1_m: 0.5180 - val_precision_m: 0.6112 - val_recall_m: 0.4723\n",
            "Epoch 23/150\n",
            "7944/7944 [==============================] - 25s 3ms/step - loss: 0.0235 - acc: 0.9940 - f1_m: 0.9902 - precision_m: 0.9946 - recall_m: 0.9868 - val_loss: 1.4486 - val_acc: 0.7219 - val_f1_m: 0.5225 - val_precision_m: 0.5983 - val_recall_m: 0.4834\n",
            "Epoch 24/150\n",
            "7944/7944 [==============================] - 26s 3ms/step - loss: 0.0210 - acc: 0.9937 - f1_m: 0.9902 - precision_m: 0.9946 - recall_m: 0.9866 - val_loss: 1.7647 - val_acc: 0.7232 - val_f1_m: 0.5117 - val_precision_m: 0.6083 - val_recall_m: 0.4640\n",
            "Epoch 25/150\n",
            "7944/7944 [==============================] - 26s 3ms/step - loss: 0.0224 - acc: 0.9945 - f1_m: 0.9911 - precision_m: 0.9961 - recall_m: 0.9870 - val_loss: 1.6950 - val_acc: 0.7266 - val_f1_m: 0.5455 - val_precision_m: 0.6049 - val_recall_m: 0.5190\n",
            "Epoch 26/150\n",
            "7944/7944 [==============================] - 27s 3ms/step - loss: 0.0275 - acc: 0.9927 - f1_m: 0.9876 - precision_m: 0.9938 - recall_m: 0.9832 - val_loss: 1.5686 - val_acc: 0.7153 - val_f1_m: 0.5259 - val_precision_m: 0.5804 - val_recall_m: 0.5018\n",
            "Epoch 27/150\n",
            "7944/7944 [==============================] - 26s 3ms/step - loss: 0.0213 - acc: 0.9945 - f1_m: 0.9915 - precision_m: 0.9963 - recall_m: 0.9876 - val_loss: 1.7172 - val_acc: 0.7158 - val_f1_m: 0.5402 - val_precision_m: 0.5778 - val_recall_m: 0.5286\n",
            "Epoch 28/150\n",
            "7944/7944 [==============================] - 25s 3ms/step - loss: 0.0193 - acc: 0.9953 - f1_m: 0.9933 - precision_m: 0.9969 - recall_m: 0.9902 - val_loss: 1.4516 - val_acc: 0.7124 - val_f1_m: 0.5354 - val_precision_m: 0.5698 - val_recall_m: 0.5268\n",
            "Epoch 29/150\n",
            "7944/7944 [==============================] - 26s 3ms/step - loss: 0.0193 - acc: 0.9953 - f1_m: 0.9919 - precision_m: 0.9966 - recall_m: 0.9882 - val_loss: 1.5791 - val_acc: 0.7158 - val_f1_m: 0.5142 - val_precision_m: 0.5808 - val_recall_m: 0.4827\n",
            "Epoch 30/150\n",
            "7944/7944 [==============================] - 26s 3ms/step - loss: 0.0206 - acc: 0.9947 - f1_m: 0.9914 - precision_m: 0.9945 - recall_m: 0.9893 - val_loss: 1.5566 - val_acc: 0.7100 - val_f1_m: 0.5278 - val_precision_m: 0.5674 - val_recall_m: 0.5166\n",
            "Epoch 31/150\n",
            "7944/7944 [==============================] - 25s 3ms/step - loss: 0.0158 - acc: 0.9965 - f1_m: 0.9940 - precision_m: 0.9977 - recall_m: 0.9911 - val_loss: 1.6501 - val_acc: 0.7068 - val_f1_m: 0.5234 - val_precision_m: 0.5613 - val_recall_m: 0.5132\n",
            "Epoch 32/150\n",
            "7944/7944 [==============================] - 26s 3ms/step - loss: 0.0126 - acc: 0.9967 - f1_m: 0.9947 - precision_m: 0.9960 - recall_m: 0.9940 - val_loss: 1.7701 - val_acc: 0.7136 - val_f1_m: 0.5167 - val_precision_m: 0.5775 - val_recall_m: 0.4892\n",
            "Epoch 33/150\n",
            "7944/7944 [==============================] - 26s 3ms/step - loss: 0.0115 - acc: 0.9969 - f1_m: 0.9948 - precision_m: 0.9961 - recall_m: 0.9941 - val_loss: 1.9294 - val_acc: 0.6964 - val_f1_m: 0.5412 - val_precision_m: 0.5352 - val_recall_m: 0.5713\n",
            "Epoch 34/150\n",
            "7944/7944 [==============================] - 27s 3ms/step - loss: 0.0186 - acc: 0.9952 - f1_m: 0.9925 - precision_m: 0.9933 - recall_m: 0.9923 - val_loss: 1.4393 - val_acc: 0.7062 - val_f1_m: 0.5269 - val_precision_m: 0.5598 - val_recall_m: 0.5215\n",
            "Epoch 35/150\n",
            "7944/7944 [==============================] - 25s 3ms/step - loss: 0.0172 - acc: 0.9953 - f1_m: 0.9928 - precision_m: 0.9946 - recall_m: 0.9918 - val_loss: 1.7398 - val_acc: 0.7124 - val_f1_m: 0.5182 - val_precision_m: 0.5743 - val_recall_m: 0.4929\n",
            "Epoch 36/150\n",
            "7944/7944 [==============================] - 26s 3ms/step - loss: 0.0111 - acc: 0.9966 - f1_m: 0.9949 - precision_m: 0.9966 - recall_m: 0.9936 - val_loss: 1.7902 - val_acc: 0.7134 - val_f1_m: 0.5057 - val_precision_m: 0.5821 - val_recall_m: 0.4689\n",
            "Epoch 37/150\n",
            "7944/7944 [==============================] - 26s 3ms/step - loss: 0.0096 - acc: 0.9976 - f1_m: 0.9962 - precision_m: 0.9969 - recall_m: 0.9957 - val_loss: 1.8686 - val_acc: 0.7088 - val_f1_m: 0.5050 - val_precision_m: 0.5747 - val_recall_m: 0.4753\n",
            "Epoch 38/150\n",
            "7944/7944 [==============================] - 26s 3ms/step - loss: 0.0056 - acc: 0.9982 - f1_m: 0.9970 - precision_m: 0.9978 - recall_m: 0.9965 - val_loss: 2.1228 - val_acc: 0.7149 - val_f1_m: 0.4987 - val_precision_m: 0.5875 - val_recall_m: 0.4585\n",
            "Epoch 39/150\n",
            "7944/7944 [==============================] - 26s 3ms/step - loss: 0.0110 - acc: 0.9971 - f1_m: 0.9944 - precision_m: 0.9970 - recall_m: 0.9926 - val_loss: 1.6200 - val_acc: 0.6947 - val_f1_m: 0.5402 - val_precision_m: 0.5362 - val_recall_m: 0.5677\n",
            "Epoch 40/150\n",
            "7944/7944 [==============================] - 26s 3ms/step - loss: 0.0111 - acc: 0.9966 - f1_m: 0.9946 - precision_m: 0.9955 - recall_m: 0.9941 - val_loss: 1.6704 - val_acc: 0.7039 - val_f1_m: 0.5146 - val_precision_m: 0.5575 - val_recall_m: 0.5008\n",
            "Epoch 41/150\n",
            "7944/7944 [==============================] - 26s 3ms/step - loss: 0.0100 - acc: 0.9977 - f1_m: 0.9966 - precision_m: 0.9976 - recall_m: 0.9960 - val_loss: 1.8502 - val_acc: 0.7120 - val_f1_m: 0.5140 - val_precision_m: 0.5733 - val_recall_m: 0.4903\n",
            "Epoch 42/150\n",
            "7944/7944 [==============================] - 27s 3ms/step - loss: 0.0063 - acc: 0.9980 - f1_m: 0.9967 - precision_m: 0.9966 - recall_m: 0.9972 - val_loss: 2.0011 - val_acc: 0.7049 - val_f1_m: 0.5197 - val_precision_m: 0.5549 - val_recall_m: 0.5135\n",
            "Epoch 43/150\n",
            "7944/7944 [==============================] - 26s 3ms/step - loss: 0.0045 - acc: 0.9986 - f1_m: 0.9979 - precision_m: 0.9983 - recall_m: 0.9977 - val_loss: 2.5120 - val_acc: 0.7130 - val_f1_m: 0.4909 - val_precision_m: 0.5870 - val_recall_m: 0.4449\n",
            "Epoch 44/150\n",
            "7944/7944 [==============================] - 25s 3ms/step - loss: 0.0061 - acc: 0.9979 - f1_m: 0.9966 - precision_m: 0.9968 - recall_m: 0.9967 - val_loss: 2.3765 - val_acc: 0.6979 - val_f1_m: 0.4984 - val_precision_m: 0.5497 - val_recall_m: 0.4790\n",
            "Epoch 45/150\n",
            "7944/7944 [==============================] - 25s 3ms/step - loss: 0.0037 - acc: 0.9985 - f1_m: 0.9972 - precision_m: 0.9985 - recall_m: 0.9963 - val_loss: 2.4774 - val_acc: 0.7009 - val_f1_m: 0.4916 - val_precision_m: 0.5584 - val_recall_m: 0.4623\n",
            "Epoch 46/150\n",
            "7944/7944 [==============================] - 26s 3ms/step - loss: 0.0034 - acc: 0.9986 - f1_m: 0.9976 - precision_m: 0.9980 - recall_m: 0.9974 - val_loss: 2.6052 - val_acc: 0.7030 - val_f1_m: 0.5057 - val_precision_m: 0.5587 - val_recall_m: 0.4845\n",
            "Epoch 47/150\n",
            "7944/7944 [==============================] - 26s 3ms/step - loss: 0.0041 - acc: 0.9982 - f1_m: 0.9971 - precision_m: 0.9977 - recall_m: 0.9969 - val_loss: 2.6071 - val_acc: 0.7107 - val_f1_m: 0.5037 - val_precision_m: 0.5780 - val_recall_m: 0.4689\n",
            "Epoch 48/150\n",
            "7944/7944 [==============================] - 26s 3ms/step - loss: 0.0339 - acc: 0.9889 - f1_m: 0.9813 - precision_m: 0.9862 - recall_m: 0.9789 - val_loss: 1.6422 - val_acc: 0.7137 - val_f1_m: 0.5311 - val_precision_m: 0.5754 - val_recall_m: 0.5164\n",
            "Epoch 49/150\n",
            "7944/7944 [==============================] - 26s 3ms/step - loss: 0.0236 - acc: 0.9924 - f1_m: 0.9886 - precision_m: 0.9904 - recall_m: 0.9881 - val_loss: 1.5575 - val_acc: 0.7034 - val_f1_m: 0.5334 - val_precision_m: 0.5459 - val_recall_m: 0.5438\n",
            "Epoch 50/150\n",
            "7944/7944 [==============================] - 26s 3ms/step - loss: 0.0117 - acc: 0.9965 - f1_m: 0.9949 - precision_m: 0.9964 - recall_m: 0.9939 - val_loss: 1.8767 - val_acc: 0.6939 - val_f1_m: 0.5426 - val_precision_m: 0.5327 - val_recall_m: 0.5778\n",
            "Epoch 51/150\n",
            "7944/7944 [==============================] - 25s 3ms/step - loss: 0.0099 - acc: 0.9966 - f1_m: 0.9948 - precision_m: 0.9960 - recall_m: 0.9942 - val_loss: 1.9053 - val_acc: 0.7058 - val_f1_m: 0.5268 - val_precision_m: 0.5543 - val_recall_m: 0.5264\n",
            "Epoch 52/150\n",
            "7944/7944 [==============================] - 26s 3ms/step - loss: 0.0086 - acc: 0.9979 - f1_m: 0.9968 - precision_m: 0.9981 - recall_m: 0.9958 - val_loss: 1.7533 - val_acc: 0.7115 - val_f1_m: 0.5206 - val_precision_m: 0.5675 - val_recall_m: 0.5048\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00052: early stopping\n",
            "13240/13240 [==============================] - 12s 899us/step\n",
            "LSTM Training Loss: 0.4617\n",
            "LSTM Training Accuracy: 0.7942\n",
            "LSTM Training f1 score: 0.6903\n",
            "LSTM Training Precision: 0.6804\n",
            "LSTM Training Recall: 0.7242\n",
            "860/860 [==============================] - 1s 974us/step\n",
            "LSTM Test Loss: 0.4918\n",
            "LSTM Test Accuracy: 0.7756\n",
            "LSTM Test f1 score: 0.6068\n",
            "LSTM Test Precision: 0.5871\n",
            "LSTM Test Recall: 0.6512\n",
            "860/860 [==============================] - 1s 1ms/step\n",
            "860/860 [==============================] - 1s 907us/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.82      0.84       620\n",
            "           1       0.59      0.65      0.62       240\n",
            "\n",
            "    accuracy                           0.78       860\n",
            "   macro avg       0.72      0.74      0.73       860\n",
            "weighted avg       0.78      0.78      0.78       860\n",
            "\n",
            "LSTM  ends..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SunmVH1n3ZRn",
        "colab_type": "text"
      },
      "source": [
        "## Using Pre-Trained Embedding (Google's trained Word2Vec model)\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfCVaBuj7u2R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        },
        "outputId": "0829737d-04a0-4194-c523-d2d54dcc2dab"
      },
      "source": [
        "print(\"Word Embedding Starts... for stemmed tweets\")\n",
        "# train your word embeddings during the training of your neural network. \n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=20000)\n",
        "tokenizer.fit_on_texts(X_train[\"tweet_initial_nontoken\"])\n",
        "X_train_initial = tokenizer.texts_to_sequences(X_train[\"tweet_initial_nontoken\"])\n",
        "X_test_initial = tokenizer.texts_to_sequences(X_test[\"tweet_initial_nontoken\"])\n",
        "vocab_size_initial = len(tokenizer.word_index) + 1 \n",
        "wordIndex_initial=tokenizer.word_index # it is my index\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(num_words=20000)\n",
        "tokenizer.fit_on_texts(X_train[\"tweet_stopword\"])\n",
        "X_train_stopword=tokenizer.texts_to_sequences(X_train[\"tweet_stopword\"])\n",
        "X_test_stopword=tokenizer.texts_to_sequences(X_test[\"tweet_stopword\"])\n",
        "vocab_size_stopword=  len(tokenizer.word_index) + 1 \n",
        "wordIndex_stopword=tokenizer.word_index # it is my index\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(num_words=20000)\n",
        "tokenizer.fit_on_texts(X_train[\"tweet_stemmed\"])\n",
        "X_train_stemmed=tokenizer.texts_to_sequences(X_train[\"tweet_stemmed\"])\n",
        "X_test_stemmed=tokenizer.texts_to_sequences(X_test[\"tweet_stemmed\"])\n",
        "vocab_size_stemmed = len(tokenizer.word_index) + 1 \n",
        "wordIndex_stemmed=tokenizer.word_index # it is my index\n",
        "\n",
        "y_train=[1 if i == \"OFF\" else 0 for i in X_train[\"subtask_a\"]]\n",
        "y_test= [1 if i == \"OFF\" else 0 for i in labels_a[1]]\n",
        "\n",
        "\n",
        "\n",
        "print(X_train[\"tweet_initial_nontoken\"][1])\n",
        "print(X_train_initial[1])\n",
        "print(X_train[\"tweet_stopword\"][1])\n",
        "print(X_train_stopword[1])\n",
        "print(X_train[\"tweet_stemmed\"][1])\n",
        "print(X_test_stemmed[1])\n",
        "\n",
        "\n",
        "#implement padding\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "max_len = 100\n",
        "X_train_initial = pad_sequences(X_train_initial, padding='post', maxlen=max_len)\n",
        "X_test_initial = pad_sequences(X_test_initial, padding='post', maxlen=max_len)\n",
        "\n",
        "X_train_stopword=pad_sequences(X_train_stopword, padding='post', maxlen=max_len)\n",
        "X_test_stopword=pad_sequences(X_test_stopword, padding='post', maxlen=max_len)\n",
        "\n",
        "X_train_stemmed=pad_sequences(X_train_stemmed, padding='post', maxlen=max_len)\n",
        "X_test_stemmed=pad_sequences(X_test_stemmed, padding='post', maxlen=max_len)\n",
        "\n",
        "print(X_train[\"tweet_initial_nontoken\"][20])\n",
        "print(X_train_initial[20])\n",
        "\n",
        "print(X_train[\"tweet_stopword\"][20])\n",
        "print(X_train_stopword[20])\n",
        "\n",
        "print(X_train[\"tweet_stemmed\"][20])\n",
        "print(X_train_stemmed[20])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word Embedding Starts... for stemmed tweets\n",
            "user user go home you re drunk user maga url\n",
            "[1, 1, 90, 310, 5, 196, 949, 1, 42, 17]\n",
            "user user go home drunk user maga url\n",
            "[1, 1, 23, 205, 824, 1, 8, 2]\n",
            "user user go home drunk user maga url\n",
            "[2260, 8, 61, 22, 131]\n",
            "user user user user lol throwing the bullshit flag on such nonsense putuporshutup kavanaugh maga callthevotealready url\n",
            "[   1    1    1    1  155 1449    2  535  811   20  192  975 8643  137\n",
            "   42 8644   17    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "user user user user lol throwing bullshit flag nonsense putuporshutup kavanaugh maga callthevotealready url\n",
            "[   1    1    1    1   66 1314  416  687  849 8494   52    8 8495    2\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "user user user user lol throw bullshit flag nonsense putuporshutup kavanaugh maga callthevotealready url\n",
            "[   1    1    1    1   80  476  456  601  892 7064   63    9 7065    2\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VodndFQTMn82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  def createEmbeddingLayer(wordIndex,not_static):\n",
        "    embedding_dim=300\n",
        "    vocabulary_size=min(len(wordIndex)+1,25000)\n",
        "    embedding_matrix = np.zeros((vocabulary_size, embedding_dim))\n",
        "    for word, i in wordIndex.items():\n",
        "        if i>=25000:\n",
        "          continue\n",
        "        try:\n",
        "            embedding_vector = w2v_model[word]\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "        except KeyError:\n",
        "          embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),embedding_dim)\n",
        "\n",
        "    custom_embedding_layer = Embedding(vocabulary_size,\n",
        "                                embedding_dim,\n",
        "                                weights=[embedding_matrix],\n",
        "                                trainable=not_static)\n",
        "    return custom_embedding_layer\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWbzU5CXwdLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McYZ1c3LTwZs",
        "colab_type": "text"
      },
      "source": [
        "## Conv Net with word2vec not static.\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44vUJasBRRmo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1cfae5ab-bbf4-4994-de90-ca1127ea8564"
      },
      "source": [
        "def convnet(vocab_size,X_train,X_test,y_train,y_test,wordIndex):\n",
        "  print(\"ConvNet Begins...\")\n",
        "  embedding_dim = 100\n",
        "  early_stopping = [EarlyStopping(monitor='val_loss',\n",
        "                          min_delta=0,restore_best_weights=False,\n",
        "                          patience=50,\n",
        "                          verbose=1, mode='auto')]\n",
        "  model = Sequential()\n",
        "  model.add(createEmbeddingLayer(wordIndex,True))\n",
        "  model.add(layers.Conv1D(128, 3, activation='relu'))\n",
        "  model.add(layers.GlobalMaxPooling1D())\n",
        "  model.add(layers.Dense(100, activation='relu'))\n",
        "  model.add(layers.Dense(1, activation='sigmoid'))\n",
        "  model.compile(optimizer='adam',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['acc',f1_m,precision_m, recall_m])\n",
        "  model.summary()\n",
        "\n",
        "  history = model.fit(X_train, y_train,\n",
        "                      epochs=150,\n",
        "                      verbose=1,\n",
        "                      validation_split=0.4,callbacks=early_stopping,\n",
        "                      batch_size=10)\n",
        "  loss, accuracy, f1_score, precision, recall = model.evaluate(X_train, y_train, verbose=1)\n",
        "  print(\"CNN Training Loss: {:.4f}\".format(loss))\n",
        "  print(\"CNN Training Accuracy: {:.4f}\".format(accuracy))\n",
        "  print(\"CNN Training f1 score: {:.4f}\".format(f1_score))\n",
        "  print(\"CNN Training Precision: {:.4f}\".format(precision))\n",
        "  print(\"CNN Training Recall: {:.4f}\".format(recall))\n",
        "\n",
        "  loss, accuracy, f1_score, precision, recall = model.evaluate(X_test, y_test, verbose=1)\n",
        "  print(\"CNN Test Loss: {:.4f}\".format(loss))\n",
        "  print(\"CNN Test Accuracy: {:.4f}\".format(accuracy))\n",
        "  print(\"CNN Test f1 score: {:.4f}\".format(f1_score))\n",
        "  print(\"CNN Test Precision: {:.4f}\".format(precision))\n",
        "  print(\"CNN Test Recall: {:.4f}\".format(recall))\n",
        "\n",
        "  probs = model.predict(X_test, verbose=1)\n",
        "  predicted_classes = model.predict_classes(X_test, verbose=1)\n",
        "\n",
        "  #filename = 'finalized_model_cnn.sav'\n",
        "  #pickle.dump(model, open(filename, 'wb'))\n",
        "\n",
        "  print(classification_report(y_test, predicted_classes))\n",
        "\n",
        "  print(\"Conv Net ends..\")\n",
        "  \n",
        "convnet(vocab_size_initial,X_train_initial,X_test_initial,y_train,y_test,wordIndex_initial)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ConvNet Begins...\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, None, 300)         5436600   \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, None, 128)         115328    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_2 (Glob (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 100)               12900     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 5,564,929\n",
            "Trainable params: 5,564,929\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 7944 samples, validate on 5296 samples\n",
            "Epoch 1/150\n",
            "7944/7944 [==============================] - 14s 2ms/step - loss: 0.5242 - acc: 0.7514 - f1_m: 0.4658 - precision_m: 0.5826 - recall_m: 0.4367 - val_loss: 0.4727 - val_acc: 0.7830 - val_f1_m: 0.6004 - val_precision_m: 0.6635 - val_recall_m: 0.6052\n",
            "Epoch 2/150\n",
            "7944/7944 [==============================] - 14s 2ms/step - loss: 0.3403 - acc: 0.8554 - f1_m: 0.7357 - precision_m: 0.7939 - recall_m: 0.7418 - val_loss: 0.5041 - val_acc: 0.7738 - val_f1_m: 0.5862 - val_precision_m: 0.6496 - val_recall_m: 0.5908\n",
            "Epoch 3/150\n",
            "7944/7944 [==============================] - 14s 2ms/step - loss: 0.1498 - acc: 0.9468 - f1_m: 0.8870 - precision_m: 0.9016 - recall_m: 0.8961 - val_loss: 0.7168 - val_acc: 0.7290 - val_f1_m: 0.5957 - val_precision_m: 0.5670 - val_recall_m: 0.7013\n",
            "Epoch 4/150\n",
            "7944/7944 [==============================] - 14s 2ms/step - loss: 0.0614 - acc: 0.9797 - f1_m: 0.9471 - precision_m: 0.9523 - recall_m: 0.9534 - val_loss: 1.0154 - val_acc: 0.7574 - val_f1_m: 0.5026 - val_precision_m: 0.6388 - val_recall_m: 0.4602\n",
            "Epoch 5/150\n",
            "7944/7944 [==============================] - 13s 2ms/step - loss: 0.0229 - acc: 0.9938 - f1_m: 0.9744 - precision_m: 0.9775 - recall_m: 0.9747 - val_loss: 1.0702 - val_acc: 0.7523 - val_f1_m: 0.5716 - val_precision_m: 0.6185 - val_recall_m: 0.5957\n",
            "Epoch 6/150\n",
            "7944/7944 [==============================] - 13s 2ms/step - loss: 0.0140 - acc: 0.9962 - f1_m: 0.9780 - precision_m: 0.9802 - recall_m: 0.9777 - val_loss: 1.1982 - val_acc: 0.7264 - val_f1_m: 0.5623 - val_precision_m: 0.5649 - val_recall_m: 0.6283\n",
            "Epoch 7/150\n",
            "7944/7944 [==============================] - 14s 2ms/step - loss: 0.0163 - acc: 0.9947 - f1_m: 0.9748 - precision_m: 0.9761 - recall_m: 0.9764 - val_loss: 1.4730 - val_acc: 0.7504 - val_f1_m: 0.5561 - val_precision_m: 0.6210 - val_recall_m: 0.5626\n",
            "Epoch 8/150\n",
            "7944/7944 [==============================] - 13s 2ms/step - loss: 0.0274 - acc: 0.9899 - f1_m: 0.9649 - precision_m: 0.9678 - recall_m: 0.9669 - val_loss: 1.2487 - val_acc: 0.7356 - val_f1_m: 0.5690 - val_precision_m: 0.5782 - val_recall_m: 0.6277\n",
            "Epoch 9/150\n",
            "7944/7944 [==============================] - 13s 2ms/step - loss: 0.0116 - acc: 0.9952 - f1_m: 0.9784 - precision_m: 0.9790 - recall_m: 0.9802 - val_loss: 1.5840 - val_acc: 0.7474 - val_f1_m: 0.5408 - val_precision_m: 0.6105 - val_recall_m: 0.5447\n",
            "Epoch 10/150\n",
            "7944/7944 [==============================] - 13s 2ms/step - loss: 0.0115 - acc: 0.9955 - f1_m: 0.9792 - precision_m: 0.9816 - recall_m: 0.9790 - val_loss: 1.7681 - val_acc: 0.7445 - val_f1_m: 0.5080 - val_precision_m: 0.6202 - val_recall_m: 0.4827\n",
            "Epoch 11/150\n",
            "7944/7944 [==============================] - 13s 2ms/step - loss: 0.0063 - acc: 0.9977 - f1_m: 0.9809 - precision_m: 0.9818 - recall_m: 0.9809 - val_loss: 1.9164 - val_acc: 0.7328 - val_f1_m: 0.5655 - val_precision_m: 0.5855 - val_recall_m: 0.6171\n",
            "Epoch 12/150\n",
            "7944/7944 [==============================] - 13s 2ms/step - loss: 0.0029 - acc: 0.9986 - f1_m: 0.9778 - precision_m: 0.9777 - recall_m: 0.9785 - val_loss: 1.9795 - val_acc: 0.7451 - val_f1_m: 0.5288 - val_precision_m: 0.6185 - val_recall_m: 0.5198\n",
            "Epoch 13/150\n",
            "7944/7944 [==============================] - 13s 2ms/step - loss: 0.0164 - acc: 0.9942 - f1_m: 0.9775 - precision_m: 0.9790 - recall_m: 0.9789 - val_loss: 1.7777 - val_acc: 0.7311 - val_f1_m: 0.5748 - val_precision_m: 0.5693 - val_recall_m: 0.6500\n",
            "Epoch 14/150\n",
            "7944/7944 [==============================] - 13s 2ms/step - loss: 0.0064 - acc: 0.9979 - f1_m: 0.9825 - precision_m: 0.9851 - recall_m: 0.9811 - val_loss: 1.9693 - val_acc: 0.7253 - val_f1_m: 0.5600 - val_precision_m: 0.5717 - val_recall_m: 0.6176\n",
            "Epoch 15/150\n",
            "7944/7944 [==============================] - 14s 2ms/step - loss: 0.0040 - acc: 0.9979 - f1_m: 0.9779 - precision_m: 0.9794 - recall_m: 0.9783 - val_loss: 2.2486 - val_acc: 0.7462 - val_f1_m: 0.4872 - val_precision_m: 0.6245 - val_recall_m: 0.4443\n",
            "Epoch 16/150\n",
            "7944/7944 [==============================] - 14s 2ms/step - loss: 0.0046 - acc: 0.9984 - f1_m: 0.9761 - precision_m: 0.9773 - recall_m: 0.9754 - val_loss: 2.3202 - val_acc: 0.7470 - val_f1_m: 0.5453 - val_precision_m: 0.6135 - val_recall_m: 0.5465\n",
            "Epoch 17/150\n",
            "7944/7944 [==============================] - 13s 2ms/step - loss: 0.0134 - acc: 0.9960 - f1_m: 0.9696 - precision_m: 0.9716 - recall_m: 0.9695 - val_loss: 1.9931 - val_acc: 0.7445 - val_f1_m: 0.5345 - val_precision_m: 0.6070 - val_recall_m: 0.5367\n",
            "Epoch 18/150\n",
            "7944/7944 [==============================] - 12s 2ms/step - loss: 0.0031 - acc: 0.9989 - f1_m: 0.9737 - precision_m: 0.9748 - recall_m: 0.9731 - val_loss: 2.3535 - val_acc: 0.7547 - val_f1_m: 0.5205 - val_precision_m: 0.6382 - val_recall_m: 0.4890\n",
            "Epoch 19/150\n",
            "7944/7944 [==============================] - 13s 2ms/step - loss: 0.0052 - acc: 0.9981 - f1_m: 0.9683 - precision_m: 0.9694 - recall_m: 0.9679 - val_loss: 2.1981 - val_acc: 0.7389 - val_f1_m: 0.5288 - val_precision_m: 0.6014 - val_recall_m: 0.5334\n",
            "Epoch 20/150\n",
            "7944/7944 [==============================] - 13s 2ms/step - loss: 0.0066 - acc: 0.9979 - f1_m: 0.9715 - precision_m: 0.9717 - recall_m: 0.9724 - val_loss: 2.3112 - val_acc: 0.7264 - val_f1_m: 0.5434 - val_precision_m: 0.5766 - val_recall_m: 0.5801\n",
            "Epoch 21/150\n",
            "7944/7944 [==============================] - 13s 2ms/step - loss: 0.0067 - acc: 0.9975 - f1_m: 0.9912 - precision_m: 0.9909 - recall_m: 0.9925 - val_loss: 2.2394 - val_acc: 0.7379 - val_f1_m: 0.5246 - val_precision_m: 0.6020 - val_recall_m: 0.5250\n",
            "Epoch 22/150\n",
            "7944/7944 [==============================] - 13s 2ms/step - loss: 0.0049 - acc: 0.9975 - f1_m: 0.9831 - precision_m: 0.9843 - recall_m: 0.9833 - val_loss: 2.5109 - val_acc: 0.7272 - val_f1_m: 0.5566 - val_precision_m: 0.5658 - val_recall_m: 0.6170\n",
            "Epoch 23/150\n",
            "7944/7944 [==============================] - 14s 2ms/step - loss: 0.0033 - acc: 0.9986 - f1_m: 0.9815 - precision_m: 0.9828 - recall_m: 0.9810 - val_loss: 2.3841 - val_acc: 0.7387 - val_f1_m: 0.5373 - val_precision_m: 0.5984 - val_recall_m: 0.5507\n",
            "Epoch 24/150\n",
            "7944/7944 [==============================] - 14s 2ms/step - loss: 0.0018 - acc: 0.9991 - f1_m: 0.9797 - precision_m: 0.9808 - recall_m: 0.9791 - val_loss: 2.3257 - val_acc: 0.7351 - val_f1_m: 0.5425 - val_precision_m: 0.5878 - val_recall_m: 0.5674\n",
            "Epoch 25/150\n",
            "7944/7944 [==============================] - 14s 2ms/step - loss: 0.0014 - acc: 0.9991 - f1_m: 0.9733 - precision_m: 0.9745 - recall_m: 0.9726 - val_loss: 2.4680 - val_acc: 0.7389 - val_f1_m: 0.5402 - val_precision_m: 0.5977 - val_recall_m: 0.5555\n",
            "Epoch 26/150\n",
            "7944/7944 [==============================] - 13s 2ms/step - loss: 0.0014 - acc: 0.9991 - f1_m: 0.9834 - precision_m: 0.9846 - recall_m: 0.9827 - val_loss: 2.4863 - val_acc: 0.7292 - val_f1_m: 0.5518 - val_precision_m: 0.5759 - val_recall_m: 0.5973\n",
            "Epoch 27/150\n",
            "7944/7944 [==============================] - 14s 2ms/step - loss: 0.0097 - acc: 0.9977 - f1_m: 0.9729 - precision_m: 0.9734 - recall_m: 0.9734 - val_loss: 2.5801 - val_acc: 0.7136 - val_f1_m: 0.5458 - val_precision_m: 0.5542 - val_recall_m: 0.6088\n",
            "Epoch 28/150\n",
            "7944/7944 [==============================] - 13s 2ms/step - loss: 0.0094 - acc: 0.9970 - f1_m: 0.9823 - precision_m: 0.9848 - recall_m: 0.9817 - val_loss: 2.3178 - val_acc: 0.7466 - val_f1_m: 0.5354 - val_precision_m: 0.6204 - val_recall_m: 0.5289\n",
            "Epoch 29/150\n",
            "7944/7944 [==============================] - 13s 2ms/step - loss: 0.0060 - acc: 0.9985 - f1_m: 0.9790 - precision_m: 0.9790 - recall_m: 0.9797 - val_loss: 2.4685 - val_acc: 0.7449 - val_f1_m: 0.5397 - val_precision_m: 0.6195 - val_recall_m: 0.5393\n",
            "Epoch 30/150\n",
            "7944/7944 [==============================] - 14s 2ms/step - loss: 0.0022 - acc: 0.9987 - f1_m: 0.9808 - precision_m: 0.9815 - recall_m: 0.9809 - val_loss: 2.5246 - val_acc: 0.7409 - val_f1_m: 0.5340 - val_precision_m: 0.6144 - val_recall_m: 0.5323\n",
            "Epoch 31/150\n",
            "7944/7944 [==============================] - 15s 2ms/step - loss: 0.0015 - acc: 0.9992 - f1_m: 0.9799 - precision_m: 0.9811 - recall_m: 0.9790 - val_loss: 2.5525 - val_acc: 0.7415 - val_f1_m: 0.5356 - val_precision_m: 0.6177 - val_recall_m: 0.5326\n",
            "Epoch 32/150\n",
            "7944/7944 [==============================] - 13s 2ms/step - loss: 0.0013 - acc: 0.9990 - f1_m: 0.9815 - precision_m: 0.9821 - recall_m: 0.9815 - val_loss: 2.6019 - val_acc: 0.7434 - val_f1_m: 0.5353 - val_precision_m: 0.6227 - val_recall_m: 0.5289\n",
            "Epoch 33/150\n",
            "7944/7944 [==============================] - 12s 2ms/step - loss: 0.0012 - acc: 0.9992 - f1_m: 0.9813 - precision_m: 0.9821 - recall_m: 0.9809 - val_loss: 2.7431 - val_acc: 0.7455 - val_f1_m: 0.5274 - val_precision_m: 0.6322 - val_recall_m: 0.5096\n",
            "Epoch 34/150\n",
            "7944/7944 [==============================] - 13s 2ms/step - loss: 0.0021 - acc: 0.9989 - f1_m: 0.9722 - precision_m: 0.9731 - recall_m: 0.9721 - val_loss: 2.7427 - val_acc: 0.7449 - val_f1_m: 0.5070 - val_precision_m: 0.6357 - val_recall_m: 0.4746\n",
            "Epoch 35/150\n",
            "7944/7944 [==============================] - 13s 2ms/step - loss: 0.0137 - acc: 0.9965 - f1_m: 0.9791 - precision_m: 0.9805 - recall_m: 0.9797 - val_loss: 2.2082 - val_acc: 0.7294 - val_f1_m: 0.5370 - val_precision_m: 0.5896 - val_recall_m: 0.5525\n",
            "Epoch 36/150\n",
            "7944/7944 [==============================] - 13s 2ms/step - loss: 0.0093 - acc: 0.9970 - f1_m: 0.9716 - precision_m: 0.9722 - recall_m: 0.9724 - val_loss: 2.2915 - val_acc: 0.7262 - val_f1_m: 0.5489 - val_precision_m: 0.5740 - val_recall_m: 0.5949\n",
            "Epoch 37/150\n",
            "7944/7944 [==============================] - 13s 2ms/step - loss: 0.0054 - acc: 0.9986 - f1_m: 0.9842 - precision_m: 0.9846 - recall_m: 0.9844 - val_loss: 2.5693 - val_acc: 0.7434 - val_f1_m: 0.4782 - val_precision_m: 0.6147 - val_recall_m: 0.4375\n",
            "Epoch 38/150\n",
            "7944/7944 [==============================] - 13s 2ms/step - loss: 0.0019 - acc: 0.9987 - f1_m: 0.9835 - precision_m: 0.9839 - recall_m: 0.9835 - val_loss: 2.5244 - val_acc: 0.7436 - val_f1_m: 0.5164 - val_precision_m: 0.6134 - val_recall_m: 0.4998\n",
            "Epoch 39/150\n",
            "7944/7944 [==============================] - 13s 2ms/step - loss: 0.0025 - acc: 0.9991 - f1_m: 0.9861 - precision_m: 0.9871 - recall_m: 0.9854 - val_loss: 2.5152 - val_acc: 0.7296 - val_f1_m: 0.5415 - val_precision_m: 0.5881 - val_recall_m: 0.5666\n",
            "Epoch 40/150\n",
            "7944/7944 [==============================] - 13s 2ms/step - loss: 0.0024 - acc: 0.9990 - f1_m: 0.9718 - precision_m: 0.9723 - recall_m: 0.9719 - val_loss: 2.5125 - val_acc: 0.7398 - val_f1_m: 0.5325 - val_precision_m: 0.6089 - val_recall_m: 0.5332\n",
            "Epoch 41/150\n",
            "7944/7944 [==============================] - 13s 2ms/step - loss: 0.0016 - acc: 0.9994 - f1_m: 0.9871 - precision_m: 0.9871 - recall_m: 0.9874 - val_loss: 2.4702 - val_acc: 0.7370 - val_f1_m: 0.5275 - val_precision_m: 0.6031 - val_recall_m: 0.5271\n",
            "Epoch 42/150\n",
            "7944/7944 [==============================] - 13s 2ms/step - loss: 0.0014 - acc: 0.9992 - f1_m: 0.9827 - precision_m: 0.9832 - recall_m: 0.9825 - val_loss: 2.6617 - val_acc: 0.7313 - val_f1_m: 0.5374 - val_precision_m: 0.5939 - val_recall_m: 0.5546\n",
            "Epoch 43/150\n",
            "7944/7944 [==============================] - 13s 2ms/step - loss: 0.0026 - acc: 0.9990 - f1_m: 0.9819 - precision_m: 0.9822 - recall_m: 0.9821 - val_loss: 2.4650 - val_acc: 0.7302 - val_f1_m: 0.5462 - val_precision_m: 0.5883 - val_recall_m: 0.5763\n",
            "Epoch 44/150\n",
            "7944/7944 [==============================] - 13s 2ms/step - loss: 0.0017 - acc: 0.9992 - f1_m: 0.9861 - precision_m: 0.9872 - recall_m: 0.9854 - val_loss: 2.7299 - val_acc: 0.7445 - val_f1_m: 0.4870 - val_precision_m: 0.6136 - val_recall_m: 0.4524\n",
            "Epoch 45/150\n",
            "7944/7944 [==============================] - 13s 2ms/step - loss: 0.0109 - acc: 0.9972 - f1_m: 0.9825 - precision_m: 0.9840 - recall_m: 0.9824 - val_loss: 2.6657 - val_acc: 0.7415 - val_f1_m: 0.5453 - val_precision_m: 0.6154 - val_recall_m: 0.5469\n",
            "Epoch 46/150\n",
            "7944/7944 [==============================] - 14s 2ms/step - loss: 0.0037 - acc: 0.9989 - f1_m: 0.9808 - precision_m: 0.9815 - recall_m: 0.9806 - val_loss: 2.7105 - val_acc: 0.7462 - val_f1_m: 0.5342 - val_precision_m: 0.6174 - val_recall_m: 0.5279\n",
            "Epoch 47/150\n",
            "7944/7944 [==============================] - 13s 2ms/step - loss: 0.0018 - acc: 0.9992 - f1_m: 0.9824 - precision_m: 0.9826 - recall_m: 0.9822 - val_loss: 2.7171 - val_acc: 0.7453 - val_f1_m: 0.4971 - val_precision_m: 0.6074 - val_recall_m: 0.4688\n",
            "Epoch 48/150\n",
            "7944/7944 [==============================] - 13s 2ms/step - loss: 0.0013 - acc: 0.9992 - f1_m: 0.9702 - precision_m: 0.9708 - recall_m: 0.9697 - val_loss: 2.7529 - val_acc: 0.7453 - val_f1_m: 0.5370 - val_precision_m: 0.6217 - val_recall_m: 0.5291\n",
            "Epoch 49/150\n",
            "7944/7944 [==============================] - 12s 2ms/step - loss: 0.0014 - acc: 0.9992 - f1_m: 0.9828 - precision_m: 0.9828 - recall_m: 0.9830 - val_loss: 2.7628 - val_acc: 0.7466 - val_f1_m: 0.5365 - val_precision_m: 0.6217 - val_recall_m: 0.5284\n",
            "Epoch 50/150\n",
            "7944/7944 [==============================] - 12s 2ms/step - loss: 0.0012 - acc: 0.9990 - f1_m: 0.9842 - precision_m: 0.9847 - recall_m: 0.9841 - val_loss: 2.7710 - val_acc: 0.7470 - val_f1_m: 0.5311 - val_precision_m: 0.6202 - val_recall_m: 0.5176\n",
            "Epoch 51/150\n",
            "7944/7944 [==============================] - 12s 2ms/step - loss: 0.0017 - acc: 0.9991 - f1_m: 0.9864 - precision_m: 0.9868 - recall_m: 0.9862 - val_loss: 2.6812 - val_acc: 0.7432 - val_f1_m: 0.5526 - val_precision_m: 0.6192 - val_recall_m: 0.5585\n",
            "Epoch 00051: early stopping\n",
            "13240/13240 [==============================] - 1s 93us/step\n",
            "CNN Training Loss: 1.0731\n",
            "CNN Training Accuracy: 0.8969\n",
            "CNN Training f1 score: 0.8323\n",
            "CNN Training Precision: 0.8500\n",
            "CNN Training Recall: 0.8251\n",
            "860/860 [==============================] - 0s 125us/step\n",
            "CNN Test Loss: 2.1932\n",
            "CNN Test Accuracy: 0.7895\n",
            "CNN Test f1 score: 0.5581\n",
            "CNN Test Precision: 0.6560\n",
            "CNN Test Recall: 0.5122\n",
            "860/860 [==============================] - 0s 346us/step\n",
            "860/860 [==============================] - 0s 67us/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       620\n",
            "           1       0.65      0.52      0.58       240\n",
            "\n",
            "    accuracy                           0.79       860\n",
            "   macro avg       0.74      0.71      0.72       860\n",
            "weighted avg       0.78      0.79      0.78       860\n",
            "\n",
            "Conv Net ends..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2gDwCL3amPc",
        "colab_type": "text"
      },
      "source": [
        "## CONV + LSTM Net with word2vec not static\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWx_jra3lZjT",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlDgwRuMaroE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a327bc61-98c9-4cb6-bc7c-38786108ed6d"
      },
      "source": [
        "  ## Network architecture buraya ekstradann convolutionoal layer eklendi.\n",
        "  def lstm(vocab_size,X_train,X_test,y_train,y_test,wordIndex):\n",
        "    early_stopping = [EarlyStopping(monitor='val_loss',\n",
        "                          min_delta=0,restore_best_weights=True,\n",
        "                          patience=50,\n",
        "                          verbose=1, mode='auto')]\n",
        "    model = Sequential()\n",
        "    model.add(createEmbeddingLayer(wordIndex,True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Conv1D(128, 3, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=4))\n",
        "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(layers.Dense(100, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc',f1_m,precision_m, recall_m])\n",
        "    ## Fit the model\n",
        "    model.fit(X_train, y_train, validation_split=0.4, epochs=150,callbacks=early_stopping)\n",
        "    loss, accuracy, f1_score, precision, recall = model.evaluate(X_train, y_train, verbose=1)\n",
        "    print(\"LSTM Training Loss: {:.4f}\".format(loss))\n",
        "    print(\"LSTM Training Accuracy: {:.4f}\".format(accuracy))\n",
        "    print(\"LSTM Training f1 score: {:.4f}\".format(f1_score))\n",
        "    print(\"LSTM Training Precision: {:.4f}\".format(precision))\n",
        "    print(\"LSTM Training Recall: {:.4f}\".format(recall))\n",
        "\n",
        "    loss, accuracy, f1_score, precision, recall = model.evaluate(X_test, y_test, verbose=1)\n",
        "    print(\"LSTM Test Loss: {:.4f}\".format(loss))\n",
        "    print(\"LSTM Test Accuracy: {:.4f}\".format(accuracy))\n",
        "    print(\"LSTM Test f1 score: {:.4f}\".format(f1_score))\n",
        "    print(\"LSTM Test Precision: {:.4f}\".format(precision))\n",
        "    print(\"LSTM Test Recall: {:.4f}\".format(recall))\n",
        "\n",
        "    probs = model.predict(X_test, verbose=1)\n",
        "    predicted_classes = model.predict_classes(X_test, verbose=1)\n",
        "\n",
        "\n",
        "    # = 'finalized_model_lstm.sav'\n",
        "    #pickle.dump(model, open(filename, 'wb'))\n",
        "\n",
        "    print(classification_report(y_test, predicted_classes))\n",
        "\n",
        "    print(\"LSTM  ends..\")\n",
        "\n",
        "lstm(vocab_size_initial,X_train_initial,X_test_initial,y_train,y_test,wordIndex_initial)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7944 samples, validate on 5296 samples\n",
            "Epoch 1/150\n",
            "7944/7944 [==============================] - 24s 3ms/step - loss: 0.6121 - acc: 0.6814 - f1_m: 0.1453 - precision_m: 0.1960 - recall_m: 0.1341 - val_loss: 0.5413 - val_acc: 0.7273 - val_f1_m: 0.6156 - val_precision_m: 0.5717 - val_recall_m: 0.6900\n",
            "Epoch 2/150\n",
            "7944/7944 [==============================] - 21s 3ms/step - loss: 0.4618 - acc: 0.7973 - f1_m: 0.6648 - precision_m: 0.7362 - recall_m: 0.6439 - val_loss: 0.4727 - val_acc: 0.7927 - val_f1_m: 0.6475 - val_precision_m: 0.7161 - val_recall_m: 0.6112\n",
            "Epoch 3/150\n",
            "7944/7944 [==============================] - 20s 3ms/step - loss: 0.3397 - acc: 0.8692 - f1_m: 0.7874 - precision_m: 0.8184 - recall_m: 0.7806 - val_loss: 0.5167 - val_acc: 0.7881 - val_f1_m: 0.6236 - val_precision_m: 0.7412 - val_recall_m: 0.5543\n",
            "Epoch 4/150\n",
            "7944/7944 [==============================] - 20s 3ms/step - loss: 0.2432 - acc: 0.9119 - f1_m: 0.8617 - precision_m: 0.8713 - recall_m: 0.8684 - val_loss: 0.5611 - val_acc: 0.7725 - val_f1_m: 0.6156 - val_precision_m: 0.6833 - val_recall_m: 0.5791\n",
            "Epoch 5/150\n",
            "7944/7944 [==============================] - 22s 3ms/step - loss: 0.1796 - acc: 0.9383 - f1_m: 0.9034 - precision_m: 0.9032 - recall_m: 0.9121 - val_loss: 0.7045 - val_acc: 0.7642 - val_f1_m: 0.6275 - val_precision_m: 0.6456 - val_recall_m: 0.6293\n",
            "Epoch 6/150\n",
            "7944/7944 [==============================] - 21s 3ms/step - loss: 0.1413 - acc: 0.9563 - f1_m: 0.9313 - precision_m: 0.9442 - recall_m: 0.9262 - val_loss: 0.8738 - val_acc: 0.7502 - val_f1_m: 0.6147 - val_precision_m: 0.6156 - val_recall_m: 0.6335\n",
            "Epoch 7/150\n",
            "7944/7944 [==============================] - 22s 3ms/step - loss: 0.1145 - acc: 0.9668 - f1_m: 0.9462 - precision_m: 0.9478 - recall_m: 0.9508 - val_loss: 0.7574 - val_acc: 0.7542 - val_f1_m: 0.5988 - val_precision_m: 0.6411 - val_recall_m: 0.5806\n",
            "Epoch 8/150\n",
            "7944/7944 [==============================] - 20s 3ms/step - loss: 0.0952 - acc: 0.9736 - f1_m: 0.9582 - precision_m: 0.9662 - recall_m: 0.9545 - val_loss: 0.8986 - val_acc: 0.7421 - val_f1_m: 0.6056 - val_precision_m: 0.6021 - val_recall_m: 0.6303\n",
            "Epoch 9/150\n",
            "7944/7944 [==============================] - 20s 3ms/step - loss: 0.0867 - acc: 0.9776 - f1_m: 0.9649 - precision_m: 0.9677 - recall_m: 0.9654 - val_loss: 0.8125 - val_acc: 0.7458 - val_f1_m: 0.5863 - val_precision_m: 0.6255 - val_recall_m: 0.5723\n",
            "Epoch 10/150\n",
            "7944/7944 [==============================] - 22s 3ms/step - loss: 0.0750 - acc: 0.9821 - f1_m: 0.9719 - precision_m: 0.9778 - recall_m: 0.9685 - val_loss: 1.0945 - val_acc: 0.7415 - val_f1_m: 0.5973 - val_precision_m: 0.6084 - val_recall_m: 0.6076\n",
            "Epoch 11/150\n",
            "7944/7944 [==============================] - 20s 3ms/step - loss: 0.0666 - acc: 0.9844 - f1_m: 0.9753 - precision_m: 0.9811 - recall_m: 0.9719 - val_loss: 1.0773 - val_acc: 0.7389 - val_f1_m: 0.5830 - val_precision_m: 0.6122 - val_recall_m: 0.5761\n",
            "Epoch 12/150\n",
            "7944/7944 [==============================] - 21s 3ms/step - loss: 0.0640 - acc: 0.9835 - f1_m: 0.9742 - precision_m: 0.9809 - recall_m: 0.9700 - val_loss: 1.2006 - val_acc: 0.7160 - val_f1_m: 0.5956 - val_precision_m: 0.5527 - val_recall_m: 0.6683\n",
            "Epoch 13/150\n",
            "7944/7944 [==============================] - 20s 3ms/step - loss: 0.0580 - acc: 0.9865 - f1_m: 0.9785 - precision_m: 0.9854 - recall_m: 0.9739 - val_loss: 1.0287 - val_acc: 0.7381 - val_f1_m: 0.5847 - val_precision_m: 0.6051 - val_recall_m: 0.5867\n",
            "Epoch 14/150\n",
            "7944/7944 [==============================] - 21s 3ms/step - loss: 0.0528 - acc: 0.9873 - f1_m: 0.9795 - precision_m: 0.9864 - recall_m: 0.9750 - val_loss: 1.2080 - val_acc: 0.7162 - val_f1_m: 0.5868 - val_precision_m: 0.5582 - val_recall_m: 0.6419\n",
            "Epoch 15/150\n",
            "7944/7944 [==============================] - 21s 3ms/step - loss: 0.0482 - acc: 0.9885 - f1_m: 0.9812 - precision_m: 0.9865 - recall_m: 0.9780 - val_loss: 1.3201 - val_acc: 0.7343 - val_f1_m: 0.5815 - val_precision_m: 0.5970 - val_recall_m: 0.5878\n",
            "Epoch 16/150\n",
            "7944/7944 [==============================] - 21s 3ms/step - loss: 0.0431 - acc: 0.9893 - f1_m: 0.9824 - precision_m: 0.9871 - recall_m: 0.9795 - val_loss: 1.2353 - val_acc: 0.7275 - val_f1_m: 0.5712 - val_precision_m: 0.5857 - val_recall_m: 0.5775\n",
            "Epoch 17/150\n",
            "7944/7944 [==============================] - 22s 3ms/step - loss: 0.0475 - acc: 0.9892 - f1_m: 0.9828 - precision_m: 0.9874 - recall_m: 0.9804 - val_loss: 1.2430 - val_acc: 0.7177 - val_f1_m: 0.5898 - val_precision_m: 0.5572 - val_recall_m: 0.6486\n",
            "Epoch 18/150\n",
            "7944/7944 [==============================] - 20s 3ms/step - loss: 0.0369 - acc: 0.9904 - f1_m: 0.9851 - precision_m: 0.9895 - recall_m: 0.9824 - val_loss: 1.4357 - val_acc: 0.7187 - val_f1_m: 0.5820 - val_precision_m: 0.5630 - val_recall_m: 0.6248\n",
            "Epoch 19/150\n",
            "7944/7944 [==============================] - 20s 3ms/step - loss: 0.0356 - acc: 0.9901 - f1_m: 0.9844 - precision_m: 0.9872 - recall_m: 0.9832 - val_loss: 1.2969 - val_acc: 0.7266 - val_f1_m: 0.5798 - val_precision_m: 0.5799 - val_recall_m: 0.6010\n",
            "Epoch 20/150\n",
            "7944/7944 [==============================] - 22s 3ms/step - loss: 0.0337 - acc: 0.9917 - f1_m: 0.9872 - precision_m: 0.9892 - recall_m: 0.9865 - val_loss: 1.3233 - val_acc: 0.7285 - val_f1_m: 0.5793 - val_precision_m: 0.5843 - val_recall_m: 0.5947\n",
            "Epoch 21/150\n",
            "7944/7944 [==============================] - 21s 3ms/step - loss: 0.0276 - acc: 0.9926 - f1_m: 0.9886 - precision_m: 0.9915 - recall_m: 0.9871 - val_loss: 1.2489 - val_acc: 0.7323 - val_f1_m: 0.5657 - val_precision_m: 0.5984 - val_recall_m: 0.5573\n",
            "Epoch 22/150\n",
            "7944/7944 [==============================] - 21s 3ms/step - loss: 0.0245 - acc: 0.9942 - f1_m: 0.9910 - precision_m: 0.9944 - recall_m: 0.9885 - val_loss: 1.5441 - val_acc: 0.7115 - val_f1_m: 0.5803 - val_precision_m: 0.5509 - val_recall_m: 0.6342\n",
            "Epoch 23/150\n",
            "7944/7944 [==============================] - 21s 3ms/step - loss: 0.0298 - acc: 0.9931 - f1_m: 0.9888 - precision_m: 0.9913 - recall_m: 0.9876 - val_loss: 1.3652 - val_acc: 0.7239 - val_f1_m: 0.5653 - val_precision_m: 0.5805 - val_recall_m: 0.5712\n",
            "Epoch 24/150\n",
            "7944/7944 [==============================] - 20s 3ms/step - loss: 0.0277 - acc: 0.9933 - f1_m: 0.9892 - precision_m: 0.9921 - recall_m: 0.9874 - val_loss: 1.3987 - val_acc: 0.7302 - val_f1_m: 0.5684 - val_precision_m: 0.5949 - val_recall_m: 0.5643\n",
            "Epoch 25/150\n",
            "7944/7944 [==============================] - 22s 3ms/step - loss: 0.0292 - acc: 0.9913 - f1_m: 0.9855 - precision_m: 0.9889 - recall_m: 0.9838 - val_loss: 1.5170 - val_acc: 0.7283 - val_f1_m: 0.5537 - val_precision_m: 0.5968 - val_recall_m: 0.5358\n",
            "Epoch 26/150\n",
            "7944/7944 [==============================] - 21s 3ms/step - loss: 0.0253 - acc: 0.9940 - f1_m: 0.9908 - precision_m: 0.9936 - recall_m: 0.9888 - val_loss: 1.3697 - val_acc: 0.7317 - val_f1_m: 0.5478 - val_precision_m: 0.6117 - val_recall_m: 0.5160\n",
            "Epoch 27/150\n",
            "7944/7944 [==============================] - 21s 3ms/step - loss: 0.0174 - acc: 0.9958 - f1_m: 0.9934 - precision_m: 0.9970 - recall_m: 0.9906 - val_loss: 1.6134 - val_acc: 0.7202 - val_f1_m: 0.5730 - val_precision_m: 0.5680 - val_recall_m: 0.5971\n",
            "Epoch 28/150\n",
            "7944/7944 [==============================] - 20s 3ms/step - loss: 0.0273 - acc: 0.9922 - f1_m: 0.9882 - precision_m: 0.9917 - recall_m: 0.9860 - val_loss: 1.4663 - val_acc: 0.7170 - val_f1_m: 0.5627 - val_precision_m: 0.5674 - val_recall_m: 0.5799\n",
            "Epoch 29/150\n",
            "7944/7944 [==============================] - 21s 3ms/step - loss: 0.0224 - acc: 0.9945 - f1_m: 0.9915 - precision_m: 0.9941 - recall_m: 0.9898 - val_loss: 1.6593 - val_acc: 0.7230 - val_f1_m: 0.5668 - val_precision_m: 0.5784 - val_recall_m: 0.5763\n",
            "Epoch 30/150\n",
            "7944/7944 [==============================] - 22s 3ms/step - loss: 0.0233 - acc: 0.9938 - f1_m: 0.9911 - precision_m: 0.9943 - recall_m: 0.9887 - val_loss: 1.5118 - val_acc: 0.7315 - val_f1_m: 0.5799 - val_precision_m: 0.5906 - val_recall_m: 0.5892\n",
            "Epoch 31/150\n",
            "7944/7944 [==============================] - 21s 3ms/step - loss: 0.0222 - acc: 0.9946 - f1_m: 0.9911 - precision_m: 0.9952 - recall_m: 0.9882 - val_loss: 1.5958 - val_acc: 0.7207 - val_f1_m: 0.5732 - val_precision_m: 0.5703 - val_recall_m: 0.5963\n",
            "Epoch 32/150\n",
            "7944/7944 [==============================] - 21s 3ms/step - loss: 0.0182 - acc: 0.9958 - f1_m: 0.9937 - precision_m: 0.9948 - recall_m: 0.9932 - val_loss: 1.5478 - val_acc: 0.7264 - val_f1_m: 0.5648 - val_precision_m: 0.5879 - val_recall_m: 0.5631\n",
            "Epoch 33/150\n",
            "7944/7944 [==============================] - 20s 3ms/step - loss: 0.0195 - acc: 0.9951 - f1_m: 0.9927 - precision_m: 0.9951 - recall_m: 0.9910 - val_loss: 1.4666 - val_acc: 0.7302 - val_f1_m: 0.5736 - val_precision_m: 0.5901 - val_recall_m: 0.5780\n",
            "Epoch 34/150\n",
            "7944/7944 [==============================] - 21s 3ms/step - loss: 0.0161 - acc: 0.9962 - f1_m: 0.9946 - precision_m: 0.9960 - recall_m: 0.9937 - val_loss: 1.4800 - val_acc: 0.7294 - val_f1_m: 0.5724 - val_precision_m: 0.5921 - val_recall_m: 0.5734\n",
            "Epoch 35/150\n",
            "7944/7944 [==============================] - 22s 3ms/step - loss: 0.0189 - acc: 0.9946 - f1_m: 0.9919 - precision_m: 0.9933 - recall_m: 0.9914 - val_loss: 1.5188 - val_acc: 0.7247 - val_f1_m: 0.5785 - val_precision_m: 0.5781 - val_recall_m: 0.6000\n",
            "Epoch 36/150\n",
            "7944/7944 [==============================] - 20s 3ms/step - loss: 0.0190 - acc: 0.9952 - f1_m: 0.9925 - precision_m: 0.9958 - recall_m: 0.9900 - val_loss: 1.4389 - val_acc: 0.7294 - val_f1_m: 0.5525 - val_precision_m: 0.6006 - val_recall_m: 0.5312\n",
            "Epoch 37/150\n",
            "7944/7944 [==============================] - 20s 3ms/step - loss: 0.0235 - acc: 0.9935 - f1_m: 0.9896 - precision_m: 0.9921 - recall_m: 0.9883 - val_loss: 1.4738 - val_acc: 0.7234 - val_f1_m: 0.5673 - val_precision_m: 0.5786 - val_recall_m: 0.5766\n",
            "Epoch 38/150\n",
            "7944/7944 [==============================] - 21s 3ms/step - loss: 0.0187 - acc: 0.9950 - f1_m: 0.9920 - precision_m: 0.9953 - recall_m: 0.9895 - val_loss: 1.6342 - val_acc: 0.7296 - val_f1_m: 0.5615 - val_precision_m: 0.5959 - val_recall_m: 0.5502\n",
            "Epoch 39/150\n",
            "7944/7944 [==============================] - 21s 3ms/step - loss: 0.0224 - acc: 0.9937 - f1_m: 0.9897 - precision_m: 0.9934 - recall_m: 0.9871 - val_loss: 1.4824 - val_acc: 0.7313 - val_f1_m: 0.5634 - val_precision_m: 0.5990 - val_recall_m: 0.5509\n",
            "Epoch 40/150\n",
            "7944/7944 [==============================] - 22s 3ms/step - loss: 0.0142 - acc: 0.9961 - f1_m: 0.9938 - precision_m: 0.9957 - recall_m: 0.9926 - val_loss: 1.6658 - val_acc: 0.7283 - val_f1_m: 0.5644 - val_precision_m: 0.5941 - val_recall_m: 0.5567\n",
            "Epoch 41/150\n",
            "7944/7944 [==============================] - 20s 3ms/step - loss: 0.0124 - acc: 0.9966 - f1_m: 0.9950 - precision_m: 0.9965 - recall_m: 0.9940 - val_loss: 1.5319 - val_acc: 0.7353 - val_f1_m: 0.5669 - val_precision_m: 0.6109 - val_recall_m: 0.5477\n",
            "Epoch 42/150\n",
            "7944/7944 [==============================] - 21s 3ms/step - loss: 0.0107 - acc: 0.9971 - f1_m: 0.9958 - precision_m: 0.9985 - recall_m: 0.9935 - val_loss: 1.7158 - val_acc: 0.7270 - val_f1_m: 0.5783 - val_precision_m: 0.5827 - val_recall_m: 0.5931\n",
            "Epoch 43/150\n",
            "7944/7944 [==============================] - 21s 3ms/step - loss: 0.0147 - acc: 0.9969 - f1_m: 0.9951 - precision_m: 0.9972 - recall_m: 0.9935 - val_loss: 1.7474 - val_acc: 0.7260 - val_f1_m: 0.5753 - val_precision_m: 0.5853 - val_recall_m: 0.5864\n",
            "Epoch 44/150\n",
            "7944/7944 [==============================] - 21s 3ms/step - loss: 0.0105 - acc: 0.9975 - f1_m: 0.9960 - precision_m: 0.9966 - recall_m: 0.9958 - val_loss: 1.8156 - val_acc: 0.7243 - val_f1_m: 0.5812 - val_precision_m: 0.5780 - val_recall_m: 0.6039\n",
            "Epoch 45/150\n",
            "7944/7944 [==============================] - 21s 3ms/step - loss: 0.0093 - acc: 0.9977 - f1_m: 0.9967 - precision_m: 0.9987 - recall_m: 0.9950 - val_loss: 1.7237 - val_acc: 0.7323 - val_f1_m: 0.5759 - val_precision_m: 0.5991 - val_recall_m: 0.5741\n",
            "Epoch 46/150\n",
            "7944/7944 [==============================] - 20s 3ms/step - loss: 0.0078 - acc: 0.9982 - f1_m: 0.9972 - precision_m: 0.9993 - recall_m: 0.9954 - val_loss: 2.0947 - val_acc: 0.7226 - val_f1_m: 0.5827 - val_precision_m: 0.5747 - val_recall_m: 0.6106\n",
            "Epoch 47/150\n",
            "7944/7944 [==============================] - 21s 3ms/step - loss: 0.0163 - acc: 0.9962 - f1_m: 0.9941 - precision_m: 0.9965 - recall_m: 0.9923 - val_loss: 1.2360 - val_acc: 0.7268 - val_f1_m: 0.5690 - val_precision_m: 0.5912 - val_recall_m: 0.5678\n",
            "Epoch 48/150\n",
            "7944/7944 [==============================] - 21s 3ms/step - loss: 0.0143 - acc: 0.9967 - f1_m: 0.9936 - precision_m: 0.9943 - recall_m: 0.9936 - val_loss: 1.2438 - val_acc: 0.7345 - val_f1_m: 0.5622 - val_precision_m: 0.6173 - val_recall_m: 0.5370\n",
            "Epoch 49/150\n",
            "7944/7944 [==============================] - 21s 3ms/step - loss: 0.0180 - acc: 0.9952 - f1_m: 0.9922 - precision_m: 0.9940 - recall_m: 0.9914 - val_loss: 1.6445 - val_acc: 0.7209 - val_f1_m: 0.5710 - val_precision_m: 0.5751 - val_recall_m: 0.5878\n",
            "Epoch 50/150\n",
            "7944/7944 [==============================] - 22s 3ms/step - loss: 0.0260 - acc: 0.9933 - f1_m: 0.9891 - precision_m: 0.9909 - recall_m: 0.9885 - val_loss: 1.3965 - val_acc: 0.7317 - val_f1_m: 0.5629 - val_precision_m: 0.6071 - val_recall_m: 0.5433\n",
            "Epoch 51/150\n",
            "7944/7944 [==============================] - 21s 3ms/step - loss: 0.0148 - acc: 0.9970 - f1_m: 0.9953 - precision_m: 0.9957 - recall_m: 0.9954 - val_loss: 1.5906 - val_acc: 0.7253 - val_f1_m: 0.5645 - val_precision_m: 0.5887 - val_recall_m: 0.5601\n",
            "Epoch 52/150\n",
            "7944/7944 [==============================] - 21s 3ms/step - loss: 0.0183 - acc: 0.9961 - f1_m: 0.9939 - precision_m: 0.9941 - recall_m: 0.9943 - val_loss: 1.5563 - val_acc: 0.7188 - val_f1_m: 0.5673 - val_precision_m: 0.5715 - val_recall_m: 0.5827\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00052: early stopping\n",
            "13240/13240 [==============================] - 6s 475us/step\n",
            "LSTM Training Loss: 0.3816\n",
            "LSTM Training Accuracy: 0.8441\n",
            "LSTM Training f1 score: 0.7373\n",
            "LSTM Training Precision: 0.8016\n",
            "LSTM Training Recall: 0.7023\n",
            "860/860 [==============================] - 0s 515us/step\n",
            "LSTM Test Loss: 0.4060\n",
            "LSTM Test Accuracy: 0.8291\n",
            "LSTM Test f1 score: 0.6338\n",
            "LSTM Test Precision: 0.7414\n",
            "LSTM Test Recall: 0.5708\n",
            "860/860 [==============================] - 1s 982us/step\n",
            "860/860 [==============================] - 0s 454us/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.92      0.89       620\n",
            "           1       0.75      0.59      0.66       240\n",
            "\n",
            "    accuracy                           0.83       860\n",
            "   macro avg       0.80      0.76      0.77       860\n",
            "weighted avg       0.82      0.83      0.82       860\n",
            "\n",
            "LSTM  ends..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imQigYQAcBOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMnEqraSlboA",
        "colab_type": "text"
      },
      "source": [
        "## Conv Net with word2vec static\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYjMBDCLfw0M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "aceab5b2-e13d-4ffd-c39f-1dae2aad2c78"
      },
      "source": [
        "def convnet(vocab_size,X_train,X_test,y_train,y_test,wordIndex):\n",
        "  print(\"ConvNet Begins...\")\n",
        "  embedding_dim = 100\n",
        "  early_stopping = [EarlyStopping(monitor='val_loss',\n",
        "                          min_delta=0,restore_best_weights=True,\n",
        "                          patience=50,\n",
        "                          verbose=1, mode='auto')]\n",
        "  model = Sequential()\n",
        "  model.add(createEmbeddingLayer(wordIndex,False))\n",
        "  model.add(layers.Conv1D(128, 3, activation='relu'))\n",
        "  model.add(layers.GlobalMaxPooling1D())\n",
        "  model.add(layers.Dense(100, activation='relu'))\n",
        "  model.add(layers.Dense(1, activation='sigmoid'))\n",
        "  model.compile(optimizer='adam',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['acc',f1_m,precision_m, recall_m])\n",
        "  model.summary()\n",
        "\n",
        "  history = model.fit(X_train, y_train,\n",
        "                      epochs=150,\n",
        "                      verbose=1,\n",
        "                      validation_split=0.4,\n",
        "                      batch_size=10,callbacks=early_stopping)\n",
        "  loss, accuracy, f1_score, precision, recall = model.evaluate(X_train, y_train, verbose=1)\n",
        "  print(\"CNN Training Loss: {:.4f}\".format(loss))\n",
        "  print(\"CNN Training Accuracy: {:.4f}\".format(accuracy))\n",
        "  print(\"CNN Training f1 score: {:.4f}\".format(f1_score))\n",
        "  print(\"CNN Training Precision: {:.4f}\".format(precision))\n",
        "  print(\"CNN Training Recall: {:.4f}\".format(recall))\n",
        "\n",
        "  loss, accuracy, f1_score, precision, recall = model.evaluate(X_test, y_test, verbose=1)\n",
        "  print(\"CNN Test Loss: {:.4f}\".format(loss))\n",
        "  print(\"CNN Test Accuracy: {:.4f}\".format(accuracy))\n",
        "  print(\"CNN Test f1 score: {:.4f}\".format(f1_score))\n",
        "  print(\"CNN Test Precision: {:.4f}\".format(precision))\n",
        "  print(\"CNN Test Recall: {:.4f}\".format(recall))\n",
        "\n",
        "  probs = model.predict(X_test, verbose=1)\n",
        "  predicted_classes = model.predict_classes(X_test, verbose=1)\n",
        "\n",
        "  #filename = 'finalized_model_cnn.sav'\n",
        "  #pickle.dump(model, open(filename, 'wb'))\n",
        "\n",
        "  print(classification_report(y_test, predicted_classes))\n",
        "\n",
        "  print(\"Conv Net ends..\")\n",
        "  \n",
        "convnet(vocab_size_initial,X_train_initial,X_test_initial,y_train,y_test,wordIndex_initial)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ConvNet Begins...\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, None, 300)         5436600   \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, None, 128)         115328    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_3 (Glob (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 100)               12900     \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 5,564,929\n",
            "Trainable params: 128,329\n",
            "Non-trainable params: 5,436,600\n",
            "_________________________________________________________________\n",
            "Train on 7944 samples, validate on 5296 samples\n",
            "Epoch 1/150\n",
            "7944/7944 [==============================] - 9s 1ms/step - loss: 0.5483 - acc: 0.7302 - f1_m: 0.4410 - precision_m: 0.5702 - recall_m: 0.4128 - val_loss: 0.5032 - val_acc: 0.7664 - val_f1_m: 0.4558 - val_precision_m: 0.6728 - val_recall_m: 0.3791\n",
            "Epoch 2/150\n",
            "7944/7944 [==============================] - 8s 945us/step - loss: 0.4471 - acc: 0.7965 - f1_m: 0.6020 - precision_m: 0.7097 - recall_m: 0.5878 - val_loss: 0.5035 - val_acc: 0.7753 - val_f1_m: 0.5000 - val_precision_m: 0.6896 - val_recall_m: 0.4343\n",
            "Epoch 3/150\n",
            "7944/7944 [==============================] - 7s 943us/step - loss: 0.3852 - acc: 0.8291 - f1_m: 0.6665 - precision_m: 0.7474 - recall_m: 0.6577 - val_loss: 0.5213 - val_acc: 0.7543 - val_f1_m: 0.6126 - val_precision_m: 0.5946 - val_recall_m: 0.7007\n",
            "Epoch 4/150\n",
            "7944/7944 [==============================] - 8s 946us/step - loss: 0.3074 - acc: 0.8693 - f1_m: 0.7512 - precision_m: 0.8091 - recall_m: 0.7521 - val_loss: 0.5564 - val_acc: 0.7693 - val_f1_m: 0.5868 - val_precision_m: 0.6518 - val_recall_m: 0.5957\n",
            "Epoch 5/150\n",
            "7944/7944 [==============================] - 8s 954us/step - loss: 0.2261 - acc: 0.9100 - f1_m: 0.8282 - precision_m: 0.8619 - recall_m: 0.8381 - val_loss: 0.6595 - val_acc: 0.7460 - val_f1_m: 0.6023 - val_precision_m: 0.5911 - val_recall_m: 0.6880\n",
            "Epoch 6/150\n",
            "7944/7944 [==============================] - 8s 981us/step - loss: 0.1469 - acc: 0.9450 - f1_m: 0.8825 - precision_m: 0.9060 - recall_m: 0.8864 - val_loss: 0.7759 - val_acc: 0.7538 - val_f1_m: 0.5301 - val_precision_m: 0.6316 - val_recall_m: 0.5140\n",
            "Epoch 7/150\n",
            "7944/7944 [==============================] - 8s 1ms/step - loss: 0.0921 - acc: 0.9675 - f1_m: 0.9203 - precision_m: 0.9316 - recall_m: 0.9256 - val_loss: 0.9064 - val_acc: 0.7593 - val_f1_m: 0.5281 - val_precision_m: 0.6348 - val_recall_m: 0.5081\n",
            "Epoch 8/150\n",
            "7944/7944 [==============================] - 8s 1ms/step - loss: 0.0721 - acc: 0.9775 - f1_m: 0.9331 - precision_m: 0.9409 - recall_m: 0.9373 - val_loss: 1.1203 - val_acc: 0.7560 - val_f1_m: 0.5122 - val_precision_m: 0.6309 - val_recall_m: 0.4818\n",
            "Epoch 9/150\n",
            "7944/7944 [==============================] - 8s 1ms/step - loss: 0.0564 - acc: 0.9824 - f1_m: 0.9518 - precision_m: 0.9593 - recall_m: 0.9541 - val_loss: 1.1881 - val_acc: 0.7487 - val_f1_m: 0.5178 - val_precision_m: 0.6200 - val_recall_m: 0.5015\n",
            "Epoch 10/150\n",
            "7944/7944 [==============================] - 8s 993us/step - loss: 0.0433 - acc: 0.9846 - f1_m: 0.9557 - precision_m: 0.9611 - recall_m: 0.9592 - val_loss: 1.2424 - val_acc: 0.7483 - val_f1_m: 0.5592 - val_precision_m: 0.6135 - val_recall_m: 0.5768\n",
            "Epoch 11/150\n",
            "7944/7944 [==============================] - 8s 993us/step - loss: 0.0617 - acc: 0.9787 - f1_m: 0.9389 - precision_m: 0.9468 - recall_m: 0.9423 - val_loss: 1.2472 - val_acc: 0.7415 - val_f1_m: 0.5850 - val_precision_m: 0.5863 - val_recall_m: 0.6548\n",
            "Epoch 12/150\n",
            "7944/7944 [==============================] - 8s 991us/step - loss: 0.0326 - acc: 0.9902 - f1_m: 0.9610 - precision_m: 0.9656 - recall_m: 0.9618 - val_loss: 1.3251 - val_acc: 0.7511 - val_f1_m: 0.5562 - val_precision_m: 0.6189 - val_recall_m: 0.5684\n",
            "Epoch 13/150\n",
            "7944/7944 [==============================] - 8s 1ms/step - loss: 0.0402 - acc: 0.9867 - f1_m: 0.9534 - precision_m: 0.9584 - recall_m: 0.9550 - val_loss: 1.3795 - val_acc: 0.7443 - val_f1_m: 0.5765 - val_precision_m: 0.5981 - val_recall_m: 0.6243\n",
            "Epoch 14/150\n",
            "7944/7944 [==============================] - 8s 1ms/step - loss: 0.0336 - acc: 0.9879 - f1_m: 0.9699 - precision_m: 0.9739 - recall_m: 0.9719 - val_loss: 1.2487 - val_acc: 0.7394 - val_f1_m: 0.5802 - val_precision_m: 0.5798 - val_recall_m: 0.6497\n",
            "Epoch 15/150\n",
            "7944/7944 [==============================] - 8s 999us/step - loss: 0.0433 - acc: 0.9841 - f1_m: 0.9537 - precision_m: 0.9599 - recall_m: 0.9551 - val_loss: 1.3605 - val_acc: 0.7555 - val_f1_m: 0.5452 - val_precision_m: 0.6236 - val_recall_m: 0.5430\n",
            "Epoch 16/150\n",
            "7944/7944 [==============================] - 8s 954us/step - loss: 0.0271 - acc: 0.9927 - f1_m: 0.9712 - precision_m: 0.9723 - recall_m: 0.9736 - val_loss: 1.5511 - val_acc: 0.7451 - val_f1_m: 0.5795 - val_precision_m: 0.5922 - val_recall_m: 0.6340\n",
            "Epoch 17/150\n",
            "7944/7944 [==============================] - 8s 1ms/step - loss: 0.0303 - acc: 0.9899 - f1_m: 0.9641 - precision_m: 0.9675 - recall_m: 0.9668 - val_loss: 1.5778 - val_acc: 0.7400 - val_f1_m: 0.5398 - val_precision_m: 0.5867 - val_recall_m: 0.5571\n",
            "Epoch 18/150\n",
            "7944/7944 [==============================] - 8s 985us/step - loss: 0.0297 - acc: 0.9908 - f1_m: 0.9672 - precision_m: 0.9716 - recall_m: 0.9682 - val_loss: 1.6085 - val_acc: 0.7358 - val_f1_m: 0.5813 - val_precision_m: 0.5817 - val_recall_m: 0.6540\n",
            "Epoch 19/150\n",
            "7944/7944 [==============================] - 8s 1ms/step - loss: 0.0294 - acc: 0.9924 - f1_m: 0.9617 - precision_m: 0.9638 - recall_m: 0.9647 - val_loss: 1.5030 - val_acc: 0.7438 - val_f1_m: 0.5534 - val_precision_m: 0.5904 - val_recall_m: 0.5866\n",
            "Epoch 20/150\n",
            "7944/7944 [==============================] - 8s 1ms/step - loss: 0.0146 - acc: 0.9952 - f1_m: 0.9715 - precision_m: 0.9719 - recall_m: 0.9733 - val_loss: 1.6124 - val_acc: 0.7491 - val_f1_m: 0.4894 - val_precision_m: 0.6091 - val_recall_m: 0.4608\n",
            "Epoch 21/150\n",
            "7944/7944 [==============================] - 8s 995us/step - loss: 0.0291 - acc: 0.9902 - f1_m: 0.9752 - precision_m: 0.9770 - recall_m: 0.9784 - val_loss: 1.4726 - val_acc: 0.7462 - val_f1_m: 0.4989 - val_precision_m: 0.6090 - val_recall_m: 0.4727\n",
            "Epoch 22/150\n",
            "7944/7944 [==============================] - 9s 1ms/step - loss: 0.0348 - acc: 0.9863 - f1_m: 0.9511 - precision_m: 0.9530 - recall_m: 0.9560 - val_loss: 1.5785 - val_acc: 0.7521 - val_f1_m: 0.5139 - val_precision_m: 0.6151 - val_recall_m: 0.4978\n",
            "Epoch 23/150\n",
            "7944/7944 [==============================] - 9s 1ms/step - loss: 0.0192 - acc: 0.9937 - f1_m: 0.9730 - precision_m: 0.9753 - recall_m: 0.9742 - val_loss: 1.7200 - val_acc: 0.7517 - val_f1_m: 0.5294 - val_precision_m: 0.6139 - val_recall_m: 0.5256\n",
            "Epoch 24/150\n",
            "7944/7944 [==============================] - 8s 1ms/step - loss: 0.0317 - acc: 0.9892 - f1_m: 0.9684 - precision_m: 0.9718 - recall_m: 0.9712 - val_loss: 1.6958 - val_acc: 0.7370 - val_f1_m: 0.5476 - val_precision_m: 0.5805 - val_recall_m: 0.5822\n",
            "Epoch 25/150\n",
            "7944/7944 [==============================] - 8s 973us/step - loss: 0.0202 - acc: 0.9927 - f1_m: 0.9637 - precision_m: 0.9663 - recall_m: 0.9658 - val_loss: 1.7391 - val_acc: 0.7545 - val_f1_m: 0.5453 - val_precision_m: 0.6243 - val_recall_m: 0.5468\n",
            "Epoch 26/150\n",
            "7944/7944 [==============================] - 8s 953us/step - loss: 0.0136 - acc: 0.9952 - f1_m: 0.9771 - precision_m: 0.9775 - recall_m: 0.9794 - val_loss: 1.9070 - val_acc: 0.7598 - val_f1_m: 0.5043 - val_precision_m: 0.6373 - val_recall_m: 0.4653\n",
            "Epoch 27/150\n",
            "7944/7944 [==============================] - 8s 975us/step - loss: 0.0304 - acc: 0.9901 - f1_m: 0.9697 - precision_m: 0.9736 - recall_m: 0.9708 - val_loss: 1.6184 - val_acc: 0.7373 - val_f1_m: 0.5371 - val_precision_m: 0.5885 - val_recall_m: 0.5557\n",
            "Epoch 28/150\n",
            "7944/7944 [==============================] - 8s 972us/step - loss: 0.0401 - acc: 0.9883 - f1_m: 0.9670 - precision_m: 0.9713 - recall_m: 0.9693 - val_loss: 1.6177 - val_acc: 0.7323 - val_f1_m: 0.5914 - val_precision_m: 0.5732 - val_recall_m: 0.6821\n",
            "Epoch 29/150\n",
            "7944/7944 [==============================] - 8s 945us/step - loss: 0.0168 - acc: 0.9946 - f1_m: 0.9657 - precision_m: 0.9674 - recall_m: 0.9666 - val_loss: 1.6674 - val_acc: 0.7375 - val_f1_m: 0.5673 - val_precision_m: 0.5892 - val_recall_m: 0.6176\n",
            "Epoch 30/150\n",
            "7944/7944 [==============================] - 8s 959us/step - loss: 0.0151 - acc: 0.9945 - f1_m: 0.9723 - precision_m: 0.9730 - recall_m: 0.9749 - val_loss: 2.0636 - val_acc: 0.7139 - val_f1_m: 0.5902 - val_precision_m: 0.5488 - val_recall_m: 0.7175\n",
            "Epoch 31/150\n",
            "7944/7944 [==============================] - 8s 1ms/step - loss: 0.0178 - acc: 0.9941 - f1_m: 0.9784 - precision_m: 0.9807 - recall_m: 0.9790 - val_loss: 1.9090 - val_acc: 0.7576 - val_f1_m: 0.5707 - val_precision_m: 0.6219 - val_recall_m: 0.5849\n",
            "Epoch 32/150\n",
            "7944/7944 [==============================] - 8s 1ms/step - loss: 0.0192 - acc: 0.9927 - f1_m: 0.9703 - precision_m: 0.9729 - recall_m: 0.9717 - val_loss: 1.8311 - val_acc: 0.7498 - val_f1_m: 0.5230 - val_precision_m: 0.6128 - val_recall_m: 0.5146\n",
            "Epoch 33/150\n",
            "7944/7944 [==============================] - 8s 1ms/step - loss: 0.0116 - acc: 0.9963 - f1_m: 0.9773 - precision_m: 0.9784 - recall_m: 0.9783 - val_loss: 1.8567 - val_acc: 0.7373 - val_f1_m: 0.5938 - val_precision_m: 0.5831 - val_recall_m: 0.6782\n",
            "Epoch 34/150\n",
            "7944/7944 [==============================] - 8s 1ms/step - loss: 0.0241 - acc: 0.9932 - f1_m: 0.9692 - precision_m: 0.9706 - recall_m: 0.9726 - val_loss: 1.8209 - val_acc: 0.7451 - val_f1_m: 0.5243 - val_precision_m: 0.6098 - val_recall_m: 0.5184\n",
            "Epoch 35/150\n",
            "7944/7944 [==============================] - 8s 989us/step - loss: 0.0204 - acc: 0.9941 - f1_m: 0.9794 - precision_m: 0.9808 - recall_m: 0.9812 - val_loss: 1.6211 - val_acc: 0.7434 - val_f1_m: 0.5335 - val_precision_m: 0.6045 - val_recall_m: 0.5401\n",
            "Epoch 36/150\n",
            "7944/7944 [==============================] - 8s 1ms/step - loss: 0.0098 - acc: 0.9958 - f1_m: 0.9712 - precision_m: 0.9715 - recall_m: 0.9729 - val_loss: 1.8752 - val_acc: 0.7479 - val_f1_m: 0.5341 - val_precision_m: 0.6037 - val_recall_m: 0.5350\n",
            "Epoch 37/150\n",
            "7944/7944 [==============================] - 8s 1ms/step - loss: 0.0159 - acc: 0.9945 - f1_m: 0.9744 - precision_m: 0.9769 - recall_m: 0.9757 - val_loss: 2.1843 - val_acc: 0.7487 - val_f1_m: 0.4577 - val_precision_m: 0.5983 - val_recall_m: 0.4159\n",
            "Epoch 38/150\n",
            "7944/7944 [==============================] - 8s 1ms/step - loss: 0.0202 - acc: 0.9917 - f1_m: 0.9592 - precision_m: 0.9624 - recall_m: 0.9604 - val_loss: 1.8955 - val_acc: 0.7521 - val_f1_m: 0.5526 - val_precision_m: 0.6089 - val_recall_m: 0.5654\n",
            "Epoch 39/150\n",
            "7944/7944 [==============================] - 8s 967us/step - loss: 0.0156 - acc: 0.9945 - f1_m: 0.9664 - precision_m: 0.9683 - recall_m: 0.9681 - val_loss: 1.7787 - val_acc: 0.7481 - val_f1_m: 0.5101 - val_precision_m: 0.6126 - val_recall_m: 0.4939\n",
            "Epoch 40/150\n",
            "7944/7944 [==============================] - 8s 991us/step - loss: 0.0127 - acc: 0.9965 - f1_m: 0.9752 - precision_m: 0.9759 - recall_m: 0.9763 - val_loss: 1.8959 - val_acc: 0.7494 - val_f1_m: 0.5423 - val_precision_m: 0.5976 - val_recall_m: 0.5580\n",
            "Epoch 41/150\n",
            "7944/7944 [==============================] - 8s 1ms/step - loss: 0.0212 - acc: 0.9931 - f1_m: 0.9652 - precision_m: 0.9666 - recall_m: 0.9676 - val_loss: 1.8866 - val_acc: 0.7475 - val_f1_m: 0.5367 - val_precision_m: 0.6088 - val_recall_m: 0.5413\n",
            "Epoch 42/150\n",
            "7944/7944 [==============================] - 8s 1ms/step - loss: 0.0137 - acc: 0.9952 - f1_m: 0.9776 - precision_m: 0.9794 - recall_m: 0.9797 - val_loss: 1.8873 - val_acc: 0.7500 - val_f1_m: 0.5775 - val_precision_m: 0.6068 - val_recall_m: 0.6164\n",
            "Epoch 43/150\n",
            "7944/7944 [==============================] - 8s 999us/step - loss: 0.0217 - acc: 0.9933 - f1_m: 0.9696 - precision_m: 0.9714 - recall_m: 0.9715 - val_loss: 1.6941 - val_acc: 0.7538 - val_f1_m: 0.5596 - val_precision_m: 0.6187 - val_recall_m: 0.5734\n",
            "Epoch 44/150\n",
            "7944/7944 [==============================] - 8s 988us/step - loss: 0.0137 - acc: 0.9965 - f1_m: 0.9826 - precision_m: 0.9836 - recall_m: 0.9829 - val_loss: 2.0386 - val_acc: 0.7630 - val_f1_m: 0.5346 - val_precision_m: 0.6374 - val_recall_m: 0.5172\n",
            "Epoch 45/150\n",
            "7944/7944 [==============================] - 9s 1ms/step - loss: 0.0086 - acc: 0.9977 - f1_m: 0.9698 - precision_m: 0.9702 - recall_m: 0.9708 - val_loss: 1.9975 - val_acc: 0.7343 - val_f1_m: 0.5908 - val_precision_m: 0.5791 - val_recall_m: 0.6748\n",
            "Epoch 46/150\n",
            "7944/7944 [==============================] - 9s 1ms/step - loss: 0.0271 - acc: 0.9911 - f1_m: 0.9654 - precision_m: 0.9672 - recall_m: 0.9685 - val_loss: 1.9612 - val_acc: 0.7498 - val_f1_m: 0.5347 - val_precision_m: 0.6224 - val_recall_m: 0.5299\n",
            "Epoch 47/150\n",
            "7944/7944 [==============================] - 8s 1ms/step - loss: 0.0208 - acc: 0.9943 - f1_m: 0.9761 - precision_m: 0.9778 - recall_m: 0.9772 - val_loss: 1.8761 - val_acc: 0.7475 - val_f1_m: 0.5652 - val_precision_m: 0.6015 - val_recall_m: 0.5960\n",
            "Epoch 48/150\n",
            "7944/7944 [==============================] - 8s 1ms/step - loss: 0.0142 - acc: 0.9953 - f1_m: 0.9799 - precision_m: 0.9818 - recall_m: 0.9799 - val_loss: 1.9337 - val_acc: 0.7353 - val_f1_m: 0.5732 - val_precision_m: 0.5785 - val_recall_m: 0.6425\n",
            "Epoch 49/150\n",
            "7944/7944 [==============================] - 8s 957us/step - loss: 0.0165 - acc: 0.9953 - f1_m: 0.9598 - precision_m: 0.9621 - recall_m: 0.9608 - val_loss: 2.0272 - val_acc: 0.7413 - val_f1_m: 0.5619 - val_precision_m: 0.5981 - val_recall_m: 0.6037\n",
            "Epoch 50/150\n",
            "7944/7944 [==============================] - 8s 957us/step - loss: 0.0098 - acc: 0.9970 - f1_m: 0.9825 - precision_m: 0.9836 - recall_m: 0.9826 - val_loss: 2.2351 - val_acc: 0.7492 - val_f1_m: 0.4903 - val_precision_m: 0.6091 - val_recall_m: 0.4626\n",
            "Epoch 51/150\n",
            "7944/7944 [==============================] - 8s 965us/step - loss: 0.0206 - acc: 0.9937 - f1_m: 0.9728 - precision_m: 0.9751 - recall_m: 0.9742 - val_loss: 2.2531 - val_acc: 0.7579 - val_f1_m: 0.5174 - val_precision_m: 0.6267 - val_recall_m: 0.4912\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00051: early stopping\n",
            "13240/13240 [==============================] - 1s 100us/step\n",
            "CNN Training Loss: 0.4809\n",
            "CNN Training Accuracy: 0.7740\n",
            "CNN Training f1 score: 0.5201\n",
            "CNN Training Precision: 0.8315\n",
            "CNN Training Recall: 0.3974\n",
            "860/860 [==============================] - 0s 118us/step\n",
            "CNN Test Loss: 0.4276\n",
            "CNN Test Accuracy: 0.8105\n",
            "CNN Test f1 score: 0.5437\n",
            "CNN Test Precision: 0.7994\n",
            "CNN Test Recall: 0.4306\n",
            "860/860 [==============================] - 0s 497us/step\n",
            "860/860 [==============================] - 0s 71us/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.95      0.88       620\n",
            "           1       0.79      0.44      0.56       240\n",
            "\n",
            "    accuracy                           0.81       860\n",
            "   macro avg       0.80      0.70      0.72       860\n",
            "weighted avg       0.81      0.81      0.79       860\n",
            "\n",
            "Conv Net ends..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGWFIiXTljr5",
        "colab_type": "text"
      },
      "source": [
        "## CONV + LSTM Net with word2vec static\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br2tyqtplnLq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8cfd7209-617c-47d4-f195-ef9be7bd3b85"
      },
      "source": [
        "  ## Network architecture buraya ekstradann convolutionoal layer eklendi.\n",
        "  def conv_lstm(vocab_size,X_train,X_test,y_train,y_test,wordIndex):\n",
        "    early_stopping = [EarlyStopping(monitor='val_loss',\n",
        "                          min_delta=0,restore_best_weights=True,\n",
        "                          patience=50,\n",
        "                          verbose=1, mode='auto')]\n",
        "    model = Sequential()\n",
        "    model.add(createEmbeddingLayer(wordIndex,False))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Conv1D(64, 5, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=4))\n",
        "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        ", metrics=['acc',f1_m,precision_m, recall_m])\n",
        "    ## Fit the model\n",
        "    model.fit(X_train, y_train, validation_split=0.4, epochs=150,callbacks=early_stopping)\n",
        "    loss, accuracy, f1_score, precision, recall = model.evaluate(X_train, y_train, verbose=1)\n",
        "    print(\"LSTM Training Loss: {:.4f}\".format(loss))\n",
        "    print(\"LSTM Training Accuracy: {:.4f}\".format(accuracy))\n",
        "    print(\"LSTM Training f1 score: {:.4f}\".format(f1_score))\n",
        "    print(\"LSTM Training Precision: {:.4f}\".format(precision))\n",
        "    print(\"LSTM Training Recall: {:.4f}\".format(recall))\n",
        "\n",
        "    loss, accuracy, f1_score, precision, recall = model.evaluate(X_test, y_test, verbose=1)\n",
        "    print(\"LSTM Test Loss: {:.4f}\".format(loss))\n",
        "    print(\"LSTM Test Accuracy: {:.4f}\".format(accuracy))\n",
        "    print(\"LSTM Test f1 score: {:.4f}\".format(f1_score))\n",
        "    print(\"LSTM Test Precision: {:.4f}\".format(precision))\n",
        "    print(\"LSTM Test Recall: {:.4f}\".format(recall))\n",
        "\n",
        "    probs = model.predict(X_test, verbose=1)\n",
        "    predicted_classes = model.predict_classes(X_test, verbose=1)\n",
        "\n",
        "\n",
        "    # = 'finalized_model_lstm.sav'\n",
        "    #pickle.dump(model, open(filename, 'wb'))\n",
        "\n",
        "    print(classification_report(y_test, predicted_classes))\n",
        "\n",
        "    print(\"LSTM  ends..\")\n",
        "\n",
        "conv_lstm(vocab_size_initial,X_train_initial,X_test_initial,y_train,y_test,wordIndex_initial)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7944 samples, validate on 5296 samples\n",
            "Epoch 1/150\n",
            "7944/7944 [==============================] - 23s 3ms/step - loss: 0.6065 - acc: 0.6813 - f1_m: 0.2214 - precision_m: 0.3729 - recall_m: 0.1929 - val_loss: 0.5256 - val_acc: 0.7398 - val_f1_m: 0.5805 - val_precision_m: 0.6082 - val_recall_m: 0.5760\n",
            "Epoch 2/150\n",
            "7944/7944 [==============================] - 19s 2ms/step - loss: 0.5292 - acc: 0.7486 - f1_m: 0.5683 - precision_m: 0.6695 - recall_m: 0.5394 - val_loss: 0.5084 - val_acc: 0.7481 - val_f1_m: 0.6352 - val_precision_m: 0.5970 - val_recall_m: 0.6996\n",
            "Epoch 3/150\n",
            "7944/7944 [==============================] - 20s 3ms/step - loss: 0.4983 - acc: 0.7742 - f1_m: 0.6135 - precision_m: 0.7054 - recall_m: 0.5735 - val_loss: 0.4823 - val_acc: 0.7827 - val_f1_m: 0.6244 - val_precision_m: 0.7097 - val_recall_m: 0.5769\n",
            "Epoch 4/150\n",
            "7944/7944 [==============================] - 20s 2ms/step - loss: 0.4708 - acc: 0.7835 - f1_m: 0.6416 - precision_m: 0.6954 - recall_m: 0.6292 - val_loss: 0.4949 - val_acc: 0.7432 - val_f1_m: 0.6415 - val_precision_m: 0.5843 - val_recall_m: 0.7316\n",
            "Epoch 5/150\n",
            "7944/7944 [==============================] - 19s 2ms/step - loss: 0.4473 - acc: 0.7966 - f1_m: 0.6557 - precision_m: 0.7269 - recall_m: 0.6233 - val_loss: 0.4828 - val_acc: 0.7757 - val_f1_m: 0.6549 - val_precision_m: 0.6519 - val_recall_m: 0.6763\n",
            "Epoch 6/150\n",
            "7944/7944 [==============================] - 19s 2ms/step - loss: 0.4270 - acc: 0.8087 - f1_m: 0.6805 - precision_m: 0.7584 - recall_m: 0.6554 - val_loss: 0.4978 - val_acc: 0.7492 - val_f1_m: 0.6617 - val_precision_m: 0.5923 - val_recall_m: 0.7734\n",
            "Epoch 7/150\n",
            "7944/7944 [==============================] - 19s 2ms/step - loss: 0.4075 - acc: 0.8233 - f1_m: 0.7093 - precision_m: 0.7690 - recall_m: 0.6859 - val_loss: 0.4866 - val_acc: 0.7853 - val_f1_m: 0.6175 - val_precision_m: 0.7328 - val_recall_m: 0.5512\n",
            "Epoch 8/150\n",
            "7944/7944 [==============================] - 19s 2ms/step - loss: 0.3829 - acc: 0.8343 - f1_m: 0.7247 - precision_m: 0.7934 - recall_m: 0.6932 - val_loss: 0.5102 - val_acc: 0.7795 - val_f1_m: 0.6558 - val_precision_m: 0.6662 - val_recall_m: 0.6647\n",
            "Epoch 9/150\n",
            "7944/7944 [==============================] - 19s 2ms/step - loss: 0.3665 - acc: 0.8445 - f1_m: 0.7471 - precision_m: 0.7967 - recall_m: 0.7332 - val_loss: 0.5120 - val_acc: 0.7694 - val_f1_m: 0.6437 - val_precision_m: 0.6483 - val_recall_m: 0.6582\n",
            "Epoch 10/150\n",
            "7944/7944 [==============================] - 19s 2ms/step - loss: 0.3424 - acc: 0.8583 - f1_m: 0.7699 - precision_m: 0.8137 - recall_m: 0.7515 - val_loss: 0.4901 - val_acc: 0.7802 - val_f1_m: 0.6175 - val_precision_m: 0.7112 - val_recall_m: 0.5675\n",
            "Epoch 11/150\n",
            "7944/7944 [==============================] - 20s 2ms/step - loss: 0.3196 - acc: 0.8706 - f1_m: 0.7871 - precision_m: 0.8365 - recall_m: 0.7662 - val_loss: 0.5549 - val_acc: 0.7694 - val_f1_m: 0.5574 - val_precision_m: 0.7506 - val_recall_m: 0.4629\n",
            "Epoch 12/150\n",
            "7944/7944 [==============================] - 19s 2ms/step - loss: 0.3020 - acc: 0.8826 - f1_m: 0.8151 - precision_m: 0.8433 - recall_m: 0.8116 - val_loss: 0.5582 - val_acc: 0.7438 - val_f1_m: 0.6414 - val_precision_m: 0.5928 - val_recall_m: 0.7202\n",
            "Epoch 13/150\n",
            "7944/7944 [==============================] - 20s 3ms/step - loss: 0.2857 - acc: 0.8872 - f1_m: 0.8122 - precision_m: 0.8584 - recall_m: 0.7937 - val_loss: 0.5723 - val_acc: 0.7796 - val_f1_m: 0.6065 - val_precision_m: 0.7236 - val_recall_m: 0.5411\n",
            "Epoch 14/150\n",
            "7944/7944 [==============================] - 20s 2ms/step - loss: 0.2706 - acc: 0.8914 - f1_m: 0.8183 - precision_m: 0.8634 - recall_m: 0.8053 - val_loss: 0.5722 - val_acc: 0.7706 - val_f1_m: 0.5479 - val_precision_m: 0.7626 - val_recall_m: 0.4476\n",
            "Epoch 15/150\n",
            "7944/7944 [==============================] - 19s 2ms/step - loss: 0.2503 - acc: 0.9065 - f1_m: 0.8493 - precision_m: 0.8713 - recall_m: 0.8429 - val_loss: 0.6343 - val_acc: 0.7619 - val_f1_m: 0.6195 - val_precision_m: 0.6495 - val_recall_m: 0.6158\n",
            "Epoch 16/150\n",
            "7944/7944 [==============================] - 19s 2ms/step - loss: 0.2269 - acc: 0.9155 - f1_m: 0.8609 - precision_m: 0.8845 - recall_m: 0.8545 - val_loss: 0.6611 - val_acc: 0.7568 - val_f1_m: 0.6114 - val_precision_m: 0.6402 - val_recall_m: 0.6094\n",
            "Epoch 17/150\n",
            "7944/7944 [==============================] - 19s 2ms/step - loss: 0.2194 - acc: 0.9207 - f1_m: 0.8722 - precision_m: 0.8973 - recall_m: 0.8627 - val_loss: 0.5946 - val_acc: 0.7708 - val_f1_m: 0.5973 - val_precision_m: 0.7029 - val_recall_m: 0.5432\n",
            "Epoch 18/150\n",
            "7944/7944 [==============================] - 20s 2ms/step - loss: 0.1942 - acc: 0.9320 - f1_m: 0.8905 - precision_m: 0.9098 - recall_m: 0.8846 - val_loss: 0.7247 - val_acc: 0.7653 - val_f1_m: 0.6205 - val_precision_m: 0.6564 - val_recall_m: 0.6107\n",
            "Epoch 19/150\n",
            "7944/7944 [==============================] - 19s 2ms/step - loss: 0.1966 - acc: 0.9281 - f1_m: 0.8830 - precision_m: 0.9021 - recall_m: 0.8764 - val_loss: 0.6921 - val_acc: 0.7373 - val_f1_m: 0.6329 - val_precision_m: 0.5853 - val_recall_m: 0.7118\n",
            "Epoch 20/150\n",
            "7944/7944 [==============================] - 19s 2ms/step - loss: 0.1839 - acc: 0.9334 - f1_m: 0.8932 - precision_m: 0.9059 - recall_m: 0.8909 - val_loss: 0.7921 - val_acc: 0.7600 - val_f1_m: 0.6059 - val_precision_m: 0.6582 - val_recall_m: 0.5839\n",
            "Epoch 21/150\n",
            "7944/7944 [==============================] - 19s 2ms/step - loss: 0.1756 - acc: 0.9361 - f1_m: 0.8989 - precision_m: 0.9152 - recall_m: 0.8950 - val_loss: 0.7289 - val_acc: 0.7611 - val_f1_m: 0.5923 - val_precision_m: 0.6736 - val_recall_m: 0.5489\n",
            "Epoch 22/150\n",
            "7944/7944 [==============================] - 19s 2ms/step - loss: 0.1623 - acc: 0.9444 - f1_m: 0.9124 - precision_m: 0.9350 - recall_m: 0.9008 - val_loss: 0.7168 - val_acc: 0.7593 - val_f1_m: 0.6013 - val_precision_m: 0.6531 - val_recall_m: 0.5800\n",
            "Epoch 23/150\n",
            "7944/7944 [==============================] - 21s 3ms/step - loss: 0.1605 - acc: 0.9412 - f1_m: 0.9032 - precision_m: 0.9278 - recall_m: 0.8937 - val_loss: 0.7782 - val_acc: 0.7642 - val_f1_m: 0.5610 - val_precision_m: 0.7104 - val_recall_m: 0.4841\n",
            "Epoch 24/150\n",
            "7944/7944 [==============================] - 19s 2ms/step - loss: 0.1497 - acc: 0.9480 - f1_m: 0.9160 - precision_m: 0.9305 - recall_m: 0.9100 - val_loss: 0.7671 - val_acc: 0.7662 - val_f1_m: 0.6025 - val_precision_m: 0.6769 - val_recall_m: 0.5651\n",
            "Epoch 25/150\n",
            "7944/7944 [==============================] - 19s 2ms/step - loss: 0.1463 - acc: 0.9507 - f1_m: 0.9235 - precision_m: 0.9370 - recall_m: 0.9177 - val_loss: 0.7110 - val_acc: 0.7532 - val_f1_m: 0.6043 - val_precision_m: 0.6361 - val_recall_m: 0.5960\n",
            "Epoch 26/150\n",
            "7944/7944 [==============================] - 19s 2ms/step - loss: 0.1448 - acc: 0.9495 - f1_m: 0.9191 - precision_m: 0.9408 - recall_m: 0.9083 - val_loss: 0.8134 - val_acc: 0.7511 - val_f1_m: 0.5846 - val_precision_m: 0.6495 - val_recall_m: 0.5531\n",
            "Epoch 27/150\n",
            "7944/7944 [==============================] - 19s 2ms/step - loss: 0.1333 - acc: 0.9527 - f1_m: 0.9234 - precision_m: 0.9363 - recall_m: 0.9202 - val_loss: 0.7933 - val_acc: 0.7549 - val_f1_m: 0.6122 - val_precision_m: 0.6357 - val_recall_m: 0.6147\n",
            "Epoch 28/150\n",
            "7944/7944 [==============================] - 19s 2ms/step - loss: 0.1211 - acc: 0.9587 - f1_m: 0.9330 - precision_m: 0.9367 - recall_m: 0.9371 - val_loss: 0.7980 - val_acc: 0.7398 - val_f1_m: 0.6085 - val_precision_m: 0.6023 - val_recall_m: 0.6368\n",
            "Epoch 29/150\n",
            "7944/7944 [==============================] - 19s 2ms/step - loss: 0.1169 - acc: 0.9592 - f1_m: 0.9360 - precision_m: 0.9414 - recall_m: 0.9387 - val_loss: 0.8852 - val_acc: 0.7525 - val_f1_m: 0.6087 - val_precision_m: 0.6298 - val_recall_m: 0.6104\n",
            "Epoch 30/150\n",
            "7944/7944 [==============================] - 19s 2ms/step - loss: 0.1176 - acc: 0.9573 - f1_m: 0.9336 - precision_m: 0.9418 - recall_m: 0.9323 - val_loss: 0.8238 - val_acc: 0.7574 - val_f1_m: 0.5989 - val_precision_m: 0.6536 - val_recall_m: 0.5739\n",
            "Epoch 31/150\n",
            "7944/7944 [==============================] - 19s 2ms/step - loss: 0.1137 - acc: 0.9588 - f1_m: 0.9356 - precision_m: 0.9445 - recall_m: 0.9341 - val_loss: 0.8189 - val_acc: 0.7555 - val_f1_m: 0.5933 - val_precision_m: 0.6492 - val_recall_m: 0.5691\n",
            "Epoch 32/150\n",
            "7944/7944 [==============================] - 19s 2ms/step - loss: 0.1101 - acc: 0.9622 - f1_m: 0.9397 - precision_m: 0.9548 - recall_m: 0.9316 - val_loss: 0.8058 - val_acc: 0.7542 - val_f1_m: 0.5987 - val_precision_m: 0.6408 - val_recall_m: 0.5828\n",
            "Epoch 33/150\n",
            "7944/7944 [==============================] - 20s 2ms/step - loss: 0.0960 - acc: 0.9658 - f1_m: 0.9461 - precision_m: 0.9589 - recall_m: 0.9394 - val_loss: 0.8501 - val_acc: 0.7576 - val_f1_m: 0.6061 - val_precision_m: 0.6470 - val_recall_m: 0.5886\n",
            "Epoch 34/150\n",
            "7944/7944 [==============================] - 20s 2ms/step - loss: 0.0973 - acc: 0.9661 - f1_m: 0.9470 - precision_m: 0.9613 - recall_m: 0.9383 - val_loss: 0.8560 - val_acc: 0.7583 - val_f1_m: 0.5826 - val_precision_m: 0.6673 - val_recall_m: 0.5382\n",
            "Epoch 35/150\n",
            "7944/7944 [==============================] - 19s 2ms/step - loss: 0.0938 - acc: 0.9671 - f1_m: 0.9468 - precision_m: 0.9562 - recall_m: 0.9438 - val_loss: 0.9682 - val_acc: 0.7538 - val_f1_m: 0.5848 - val_precision_m: 0.6537 - val_recall_m: 0.5521\n",
            "Epoch 36/150\n",
            "7944/7944 [==============================] - 19s 2ms/step - loss: 0.0923 - acc: 0.9676 - f1_m: 0.9485 - precision_m: 0.9551 - recall_m: 0.9474 - val_loss: 0.9049 - val_acc: 0.7445 - val_f1_m: 0.6022 - val_precision_m: 0.6182 - val_recall_m: 0.6101\n",
            "Epoch 37/150\n",
            "7944/7944 [==============================] - 20s 2ms/step - loss: 0.0902 - acc: 0.9688 - f1_m: 0.9510 - precision_m: 0.9548 - recall_m: 0.9528 - val_loss: 0.8791 - val_acc: 0.7611 - val_f1_m: 0.5903 - val_precision_m: 0.6711 - val_recall_m: 0.5480\n",
            "Epoch 38/150\n",
            "7944/7944 [==============================] - 19s 2ms/step - loss: 0.0859 - acc: 0.9707 - f1_m: 0.9546 - precision_m: 0.9602 - recall_m: 0.9534 - val_loss: 0.8481 - val_acc: 0.7636 - val_f1_m: 0.5951 - val_precision_m: 0.6767 - val_recall_m: 0.5540\n",
            "Epoch 39/150\n",
            "7944/7944 [==============================] - 20s 2ms/step - loss: 0.0823 - acc: 0.9709 - f1_m: 0.9538 - precision_m: 0.9641 - recall_m: 0.9491 - val_loss: 0.9038 - val_acc: 0.7540 - val_f1_m: 0.5910 - val_precision_m: 0.6514 - val_recall_m: 0.5612\n",
            "Epoch 40/150\n",
            "7944/7944 [==============================] - 20s 2ms/step - loss: 0.0779 - acc: 0.9719 - f1_m: 0.9547 - precision_m: 0.9640 - recall_m: 0.9505 - val_loss: 1.0125 - val_acc: 0.7581 - val_f1_m: 0.6028 - val_precision_m: 0.6538 - val_recall_m: 0.5805\n",
            "Epoch 41/150\n",
            "7944/7944 [==============================] - 19s 2ms/step - loss: 0.0812 - acc: 0.9717 - f1_m: 0.9555 - precision_m: 0.9624 - recall_m: 0.9532 - val_loss: 1.0331 - val_acc: 0.7508 - val_f1_m: 0.5841 - val_precision_m: 0.6393 - val_recall_m: 0.5585\n",
            "Epoch 42/150\n",
            "7944/7944 [==============================] - 20s 2ms/step - loss: 0.0673 - acc: 0.9785 - f1_m: 0.9658 - precision_m: 0.9686 - recall_m: 0.9669 - val_loss: 0.9595 - val_acc: 0.7479 - val_f1_m: 0.5777 - val_precision_m: 0.6348 - val_recall_m: 0.5512\n",
            "Epoch 43/150\n",
            "7944/7944 [==============================] - 20s 3ms/step - loss: 0.0746 - acc: 0.9739 - f1_m: 0.9571 - precision_m: 0.9671 - recall_m: 0.9531 - val_loss: 0.9906 - val_acc: 0.7496 - val_f1_m: 0.6100 - val_precision_m: 0.6239 - val_recall_m: 0.6198\n",
            "Epoch 44/150\n",
            "7944/7944 [==============================] - 19s 2ms/step - loss: 0.0761 - acc: 0.9737 - f1_m: 0.9578 - precision_m: 0.9680 - recall_m: 0.9526 - val_loss: 1.0521 - val_acc: 0.7324 - val_f1_m: 0.6229 - val_precision_m: 0.5822 - val_recall_m: 0.6971\n",
            "Epoch 45/150\n",
            "7944/7944 [==============================] - 19s 2ms/step - loss: 0.0744 - acc: 0.9729 - f1_m: 0.9575 - precision_m: 0.9657 - recall_m: 0.9547 - val_loss: 0.9908 - val_acc: 0.7536 - val_f1_m: 0.5838 - val_precision_m: 0.6497 - val_recall_m: 0.5533\n",
            "Epoch 46/150\n",
            "7944/7944 [==============================] - 19s 2ms/step - loss: 0.0643 - acc: 0.9790 - f1_m: 0.9655 - precision_m: 0.9721 - recall_m: 0.9629 - val_loss: 1.0362 - val_acc: 0.7557 - val_f1_m: 0.6039 - val_precision_m: 0.6462 - val_recall_m: 0.5901\n",
            "Epoch 47/150\n",
            "7944/7944 [==============================] - 20s 2ms/step - loss: 0.0696 - acc: 0.9756 - f1_m: 0.9613 - precision_m: 0.9669 - recall_m: 0.9602 - val_loss: 1.0222 - val_acc: 0.7545 - val_f1_m: 0.5856 - val_precision_m: 0.6574 - val_recall_m: 0.5473\n",
            "Epoch 48/150\n",
            "7944/7944 [==============================] - 19s 2ms/step - loss: 0.0675 - acc: 0.9756 - f1_m: 0.9612 - precision_m: 0.9724 - recall_m: 0.9553 - val_loss: 1.0853 - val_acc: 0.7213 - val_f1_m: 0.6202 - val_precision_m: 0.5669 - val_recall_m: 0.7133\n",
            "Epoch 49/150\n",
            "7944/7944 [==============================] - 20s 2ms/step - loss: 0.0653 - acc: 0.9776 - f1_m: 0.9642 - precision_m: 0.9714 - recall_m: 0.9606 - val_loss: 0.9844 - val_acc: 0.7623 - val_f1_m: 0.5619 - val_precision_m: 0.7045 - val_recall_m: 0.4849\n",
            "Epoch 50/150\n",
            "7944/7944 [==============================] - 20s 2ms/step - loss: 0.0636 - acc: 0.9795 - f1_m: 0.9673 - precision_m: 0.9739 - recall_m: 0.9641 - val_loss: 1.0498 - val_acc: 0.7523 - val_f1_m: 0.6013 - val_precision_m: 0.6363 - val_recall_m: 0.5887\n",
            "Epoch 51/150\n",
            "7944/7944 [==============================] - 19s 2ms/step - loss: 0.0674 - acc: 0.9771 - f1_m: 0.9632 - precision_m: 0.9651 - recall_m: 0.9653 - val_loss: 0.9612 - val_acc: 0.7579 - val_f1_m: 0.5937 - val_precision_m: 0.6499 - val_recall_m: 0.5661\n",
            "Epoch 52/150\n",
            "7944/7944 [==============================] - 20s 2ms/step - loss: 0.0526 - acc: 0.9816 - f1_m: 0.9713 - precision_m: 0.9733 - recall_m: 0.9720 - val_loss: 1.0907 - val_acc: 0.7491 - val_f1_m: 0.5724 - val_precision_m: 0.6528 - val_recall_m: 0.5319\n",
            "Epoch 53/150\n",
            "7944/7944 [==============================] - 20s 2ms/step - loss: 0.0621 - acc: 0.9786 - f1_m: 0.9660 - precision_m: 0.9746 - recall_m: 0.9611 - val_loss: 1.0573 - val_acc: 0.7474 - val_f1_m: 0.6037 - val_precision_m: 0.6200 - val_recall_m: 0.6101\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00053: early stopping\n",
            "13240/13240 [==============================] - 6s 473us/step\n",
            "LSTM Training Loss: 0.4570\n",
            "LSTM Training Accuracy: 0.8003\n",
            "LSTM Training f1 score: 0.6587\n",
            "LSTM Training Precision: 0.7444\n",
            "LSTM Training Recall: 0.6101\n",
            "860/860 [==============================] - 1s 596us/step\n",
            "LSTM Test Loss: 0.4458\n",
            "LSTM Test Accuracy: 0.8140\n",
            "LSTM Test f1 score: 0.5828\n",
            "LSTM Test Precision: 0.7308\n",
            "LSTM Test Recall: 0.5004\n",
            "860/860 [==============================] - 1s 1ms/step\n",
            "860/860 [==============================] - 0s 429us/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       620\n",
            "           1       0.74      0.52      0.61       240\n",
            "\n",
            "    accuracy                           0.81       860\n",
            "   macro avg       0.79      0.72      0.74       860\n",
            "weighted avg       0.81      0.81      0.80       860\n",
            "\n",
            "LSTM  ends..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hftz1vntken",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4Q6G2P_jz3P",
        "colab_type": "text"
      },
      "source": [
        "# PYTORCH \n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQVGJYBfkKkc",
        "colab_type": "text"
      },
      "source": [
        "## Imports "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_tDSR-qkND7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as npC\n",
        "import torch\n",
        "import torch.utils.data as data_utils\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torchsummary import summary\n",
        "from torch.autograd import Variable\n",
        "from sklearn import metrics\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.nn import ModuleList\n",
        "# we fix the seeds to get consistent results\n",
        "SEED = 234\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "#Seed Settings\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "torch.cuda.manual_seed(123)\n",
        "torch.backends.cudnn.deterministic = True\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wliCbvhsk34e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#glove\n",
        "import io\n",
        "EMBEDDING_FILE = r'/content/drive/My Drive/glove.840B.300d.txt'\n",
        "embeddings_index = {}\n",
        "f = open(EMBEDDING_FILE, encoding='utf8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = ''.join(values[:-300])\n",
        "    coefs = np.asarray(values[-300:], dtype='float32')\n",
        "    embeddings_index[word] = np.asarray(coefs , dtype='float32')\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HTVlaWblFfw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix = np.zeros((len(wordIndex_initial) + 1, 300))\n",
        "for word, i in wordIndex_initial.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzHxdEpZkskI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "845fc7c8-1330-496e-b1cf-33f8240e16ee"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMA6avhQj_5v",
        "colab_type": "text"
      },
      "source": [
        "## Pytorch Metrics\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNpfakA1j2iN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(arr):\n",
        "  x=list(arr)\n",
        "  r=[]\n",
        "  for i in x:\n",
        "    r.append( 1 / (1 + math.exp(-i))) \n",
        "\n",
        "  return np.array(r)\n",
        "\n",
        "def clReport(output,target):\n",
        "    predict = torch.round(torch.sigmoid(output))\n",
        "#     print(predict)\n",
        "    correct = (predict == target).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "\n",
        "    y_true = np.array(target.cpu())\n",
        "\n",
        "    y_pred = predict.detach().cpu().numpy()\n",
        "\n",
        "    return classification_report(target.detach().cpu().numpy(), predict.detach().cpu().numpy())\n",
        "\n",
        "def accuracy2(output, target):\n",
        "    predict = torch.round(torch.sigmoid(output))\n",
        "#     print(predict)\n",
        "    correct = (predict == target).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "\n",
        "    y_true = np.array(target.cpu())\n",
        "    y_pred = predict.detach().cpu().numpy()\n",
        "\n",
        "    matrix = metrics.confusion_matrix(y_true, y_pred)\n",
        "    f1_score = metrics.f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "    return acc,f1_score,matrix\n",
        "\n",
        "def accuracy_train(output, target):\n",
        "    predict = np.round(sigmoid(output))\n",
        "#     print(predict)\n",
        "    correct = (predict == target)\n",
        "    print(predict.shape)\n",
        "\n",
        "    #acc = correct.sum() / len(correct)\n",
        "    acc = metrics.accuracy_score(target,predict)\n",
        "\n",
        "    #print(acc)\n",
        "    \n",
        "    y_true = np.array(target)\n",
        "    y_pred = predict\n",
        "\n",
        "    #matrix = metrics.confusion_matrix(y_true, y_pred)\n",
        "    f1_score = metrics.f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "    return acc,f1_score\n",
        "\n",
        "\n",
        "def accuracy_2(output, target):\n",
        "    print(output.shape)\n",
        "    average = 'macro'\n",
        "    length = len(target)\n",
        "    if output.shape[-1] != 1:\n",
        "        output = torch.argmax(output, dim=1).float()\n",
        "\n",
        "    target = target.float()\n",
        "    output = output.view(target.shape)\n",
        "    predict = torch.round(torch.sigmoid(output))\n",
        "    correct = (predict == target).float()\n",
        "    acc = correct.sum() / length\n",
        "\n",
        "    y_true = np.array(target)\n",
        "    y_pred = predict.detach().numpy()\n",
        "\n",
        "    matrix = metrics.confusion_matrix(y_true, y_pred)\n",
        "    f1_score = metrics.f1_score(y_true, y_pred, average=average)\n",
        "    return acc, f1_score, matrix\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_WxOwHclVzi",
        "colab_type": "text"
      },
      "source": [
        "## Global Train Function\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzNmsfuAlVDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model,epoch,lr,embeddingMatrix,Trainable,train_sent_tensor, train_label_tensor, valid_sent_tensor, valid_label_tensor,device):\n",
        "\n",
        "  epochs=epoch\n",
        "  lr=lr\n",
        "  batch=32\n",
        "  model = model.to(device)\n",
        "\n",
        "  model.embedding.weight.data.copy_(torch.from_numpy(embeddingMatrix)) #use own embedding\n",
        "  model.embedding.weight.require_grad = Trainable\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "  loss_fn = nn.BCEWithLogitsLoss() # Bu değişecek 3.task'ta\n",
        "\n",
        "\n",
        "  batchIteration = int(len(train_label_tensor) / batch)\n",
        "  for epoch in range(1, epochs + 1):\n",
        "\n",
        "    #Randomness\n",
        "    np.random.seed(SEED)\n",
        "    p = np.random.permutation(len(train_label_tensor))\n",
        "    train_sent_tensor, train_label_tensor = train_sent_tensor[p], train_label_tensor[p]\n",
        "\n",
        "    ################################################################\n",
        "    train_sent_tensor=train_sent_tensor.to(device) #This is for cuda\n",
        "    train_label_tensor=train_label_tensor.to(device) #This is for cuda\n",
        "\n",
        "\n",
        "    #################################################################\n",
        "    epoch_loss = []\n",
        "    trainPrediction=[]\n",
        "    for i in range(0,batchIteration):\n",
        "      feature = train_sent_tensor[i * batch:(i+1) * batch]\n",
        "      target = train_label_tensor[i * batch:(i+1) * batch]\n",
        "\n",
        "\n",
        "      #print(feature.shape) # 64,100\n",
        "      #print(target.shape)  # 64,1\n",
        "#             print(feature)\n",
        "      ##############################\n",
        "      model.train().cuda()\n",
        "      ##############################\n",
        "            # we zero the gradients as they are not removed automatically\n",
        "      optimizer.zero_grad()\n",
        "            # queeze is needed as the predictions are initially size (batch size, 1) and we need to remove the dimension of size 1\n",
        "      #print(model(feature).squeeze(0))\n",
        "\n",
        "      #print(model(feature).shape) # return [32,1]\n",
        "      predictions = model(feature).squeeze(1) #  remove the dimension with length one..\n",
        "      #print(predictions.shape) #[64]\n",
        "#            \n",
        "      #calculate training accuracy\n",
        "\n",
        "      trainPrediction.append(predictions.cpu().detach().numpy().reshape(1,-1))\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "      loss = loss_fn(predictions, target)\n",
        "\n",
        "\n",
        "\n",
        "            # calculate the gradient of each parameter\n",
        "      loss.backward()\n",
        "            # update the parameters using the gradients and optimizer algorithm\n",
        "      optimizer.step()\n",
        "      batch_loss = loss.item() #?\n",
        "      #print(f'| Epoch: {epoch:02} | Batch: {i: 04} | Train Loss: {batch_loss:.3f}')\n",
        "      ##############################\n",
        "      epoch_loss.append(batch_loss)\n",
        "      ##############################  \n",
        "\n",
        "    #calculation of train accuracy per epoch\n",
        "\n",
        "\n",
        "    #####    \n",
        "\n",
        "\n",
        "\n",
        "    trainPrediction=np.array(trainPrediction).reshape(-1,1)\n",
        "\n",
        "    realValues=np.array(train_label_tensor.cpu().detach().numpy().reshape(-1,1))[0:trainPrediction.shape[0]]\n",
        "\n",
        "\n",
        "    #print(trainPrediction[:2])\n",
        "    #print(trainPrediction.shape)\n",
        "    #print(realValues.shape)\n",
        "\n",
        "    acc_train,f1_train=accuracy_train(trainPrediction,realValues)\n",
        "    #print( train_sent_tensor.shape)\n",
        "    train_sent_tensor=train_sent_tensor.to(device)\n",
        "    valid_sent_tensor=valid_sent_tensor.to(device)\n",
        "\n",
        "    #prediction_train = model(train_sent_tensor)   #hata burada\n",
        "    prediction_val = model(valid_sent_tensor).squeeze(1)\n",
        "\n",
        "   # train_acc = accuracy(prediction_train, train_label_tensor)\n",
        "    valid_acc, valid_f1score, valid_matrix  = accuracy2(prediction_val.to(device), valid_label_tensor.to(device))\n",
        "\n",
        "    if epoch == epochs:\n",
        "\n",
        "      print( clReport(prediction_val.to(device), valid_label_tensor.to(device)  ))\n",
        "\n",
        "#         print(predict_val.shape)\n",
        "    #print(f'Epoch: {epoch: 03} | Train accuracy: {train_acc * 100: .2f}% | Valid acc: {valid_acc * 100: .2f}%')\n",
        "\n",
        "\n",
        "    print(f'Epoch: {epoch: 03} Train Loss: {np.mean(epoch_loss): .2f} Train Acc:{acc_train*100: .2f}% Train F1: {f1_train*100: .2f}%  Valid acc: {valid_acc * 100: .2f}% Valid f1:{valid_f1score*100: .2f}%')\n",
        "    print('Valid confusion matrix: ')\n",
        "    print(valid_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmYb1Dvul0k0",
        "colab_type": "text"
      },
      "source": [
        "# Bidrectional LSTM\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_uXnAy5mIdm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LSTM, self).__init__()\n",
        "    VOCAB_SIZE=18122\n",
        "    EMBEDDING_DIM=300\n",
        "    HIDDEN_DIM=8\n",
        "    OUTPUT_DIM=1\n",
        "    max_len=25000\n",
        "    num_classes=1\n",
        "    self.embedding = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM)\n",
        "        # hidden layer\n",
        "    self.lstm = nn.LSTM(EMBEDDING_DIM,HIDDEN_DIM,1,bidirectional=True)\n",
        "    self.hidden_dim=HIDDEN_DIM\n",
        "    self.out = nn.Linear(HIDDEN_DIM*2*2, num_classes)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    embedded = self.embedding(x)\n",
        "    states, hidden = self.lstm(embedded.permute([1, 0, 2]))\n",
        "    encoding = torch.cat([states[0],states[-1]], dim=1)\n",
        "    out = self.out(encoding)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5D3gtaympWQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e9086179-e300-48e4-b5d3-e7aade159167"
      },
      "source": [
        "XTrainTens=torch.LongTensor(X_train_initial)\n",
        "XtestTens=torch.LongTensor(X_test_initial)\n",
        "ytestTens=torch.FloatTensor( y_test)\n",
        "y_trainTens=torch.FloatTensor(y_train)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#Non Static\n",
        "lstm_notstatic=LSTM()\n",
        "train(lstm_notstatic,50,0.0001,embedding_matrix,True, XTrainTens, y_trainTens, XtestTens, ytestTens,device)\n",
        "#Static\n",
        "lstm_static=LSTM()\n",
        "\n",
        "train(lstm_static,50,0.0001,embedding_matrix,False, XTrainTens, y_trainTens, XtestTens, ytestTens,device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(13216,)\n",
            "Epoch:  01 Train Loss:  0.65 Train Acc: 64.47% Train F1:  44.39%  Valid acc:  71.98% Valid f1: 43.79%\n",
            "Valid confusion matrix: \n",
            "[[614   6]\n",
            " [235   5]]\n",
            "(13216,)\n",
            "Epoch:  02 Train Loss:  0.63 Train Acc: 66.71% Train F1:  40.06%  Valid acc:  72.33% Valid f1: 42.78%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [238   2]]\n",
            "(13216,)\n",
            "Epoch:  03 Train Loss:  0.60 Train Acc: 67.74% Train F1:  43.99%  Valid acc:  73.26% Valid f1: 47.89%\n",
            "Valid confusion matrix: \n",
            "[[615   5]\n",
            " [225  15]]\n",
            "(13216,)\n",
            "Epoch:  04 Train Loss:  0.54 Train Acc: 73.79% Train F1:  64.80%  Valid acc:  73.37% Valid f1: 52.90%\n",
            "Valid confusion matrix: \n",
            "[[599  21]\n",
            " [208  32]]\n",
            "(13216,)\n",
            "Epoch:  05 Train Loss:  0.49 Train Acc: 78.94% Train F1:  74.59%  Valid acc:  74.42% Valid f1: 59.68%\n",
            "Valid confusion matrix: \n",
            "[[580  40]\n",
            " [180  60]]\n",
            "(13216,)\n",
            "Epoch:  06 Train Loss:  0.45 Train Acc: 81.79% Train F1:  78.32%  Valid acc:  75.81% Valid f1: 65.13%\n",
            "Valid confusion matrix: \n",
            "[[564  56]\n",
            " [152  88]]\n",
            "(13216,)\n",
            "Epoch:  07 Train Loss:  0.41 Train Acc: 84.56% Train F1:  82.13%  Valid acc:  76.74% Valid f1: 64.43%\n",
            "Valid confusion matrix: \n",
            "[[583  37]\n",
            " [163  77]]\n",
            "(13216,)\n",
            "Epoch:  08 Train Loss:  0.38 Train Acc: 86.65% Train F1:  84.64%  Valid acc:  76.40% Valid f1: 65.78%\n",
            "Valid confusion matrix: \n",
            "[[568  52]\n",
            " [151  89]]\n",
            "(13216,)\n",
            "Epoch:  09 Train Loss:  0.35 Train Acc: 87.95% Train F1:  86.21%  Valid acc:  75.70% Valid f1: 65.66%\n",
            "Valid confusion matrix: \n",
            "[[558  62]\n",
            " [147  93]]\n",
            "(13216,)\n",
            "Epoch:  10 Train Loss:  0.32 Train Acc: 89.16% Train F1:  87.62%  Valid acc:  76.63% Valid f1: 66.85%\n",
            "Valid confusion matrix: \n",
            "[[563  57]\n",
            " [144  96]]\n",
            "(13216,)\n",
            "Epoch:  11 Train Loss:  0.30 Train Acc: 90.35% Train F1:  89.01%  Valid acc:  76.40% Valid f1: 67.55%\n",
            "Valid confusion matrix: \n",
            "[[553  67]\n",
            " [136 104]]\n",
            "(13216,)\n",
            "Epoch:  12 Train Loss:  0.28 Train Acc: 91.09% Train F1:  89.85%  Valid acc:  76.63% Valid f1: 67.65%\n",
            "Valid confusion matrix: \n",
            "[[556  64]\n",
            " [137 103]]\n",
            "(13216,)\n",
            "Epoch:  13 Train Loss:  0.26 Train Acc: 91.81% Train F1:  90.72%  Valid acc:  77.21% Valid f1: 68.40%\n",
            "Valid confusion matrix: \n",
            "[[559  61]\n",
            " [135 105]]\n",
            "(13216,)\n",
            "Epoch:  14 Train Loss:  0.24 Train Acc: 92.68% Train F1:  91.70%  Valid acc:  77.09% Valid f1: 69.02%\n",
            "Valid confusion matrix: \n",
            "[[551  69]\n",
            " [128 112]]\n",
            "(13216,)\n",
            "Epoch:  15 Train Loss:  0.22 Train Acc: 93.31% Train F1:  92.43%  Valid acc:  77.21% Valid f1: 69.61%\n",
            "Valid confusion matrix: \n",
            "[[547  73]\n",
            " [123 117]]\n",
            "(13216,)\n",
            "Epoch:  16 Train Loss:  0.21 Train Acc: 93.76% Train F1:  92.94%  Valid acc:  77.67% Valid f1: 69.46%\n",
            "Valid confusion matrix: \n",
            "[[557  63]\n",
            " [129 111]]\n",
            "(13216,)\n",
            "Epoch:  17 Train Loss:  0.20 Train Acc: 94.22% Train F1:  93.47%  Valid acc:  76.98% Valid f1: 69.01%\n",
            "Valid confusion matrix: \n",
            "[[549  71]\n",
            " [127 113]]\n",
            "(13216,)\n",
            "Epoch:  18 Train Loss:  0.19 Train Acc: 94.80% Train F1:  94.12%  Valid acc:  77.21% Valid f1: 69.42%\n",
            "Valid confusion matrix: \n",
            "[[549  71]\n",
            " [125 115]]\n",
            "(13216,)\n",
            "Epoch:  19 Train Loss:  0.18 Train Acc: 95.28% Train F1:  94.66%  Valid acc:  77.33% Valid f1: 69.53%\n",
            "Valid confusion matrix: \n",
            "[[550  70]\n",
            " [125 115]]\n",
            "(13216,)\n",
            "Epoch:  20 Train Loss:  0.17 Train Acc: 95.50% Train F1:  94.92%  Valid acc:  77.44% Valid f1: 69.83%\n",
            "Valid confusion matrix: \n",
            "[[549  71]\n",
            " [123 117]]\n",
            "(13216,)\n",
            "Epoch:  21 Train Loss:  0.16 Train Acc: 95.75% Train F1:  95.20%  Valid acc:  78.49% Valid f1: 70.72%\n",
            "Valid confusion matrix: \n",
            "[[559  61]\n",
            " [124 116]]\n",
            "(13216,)\n",
            "Epoch:  22 Train Loss:  0.15 Train Acc: 96.12% Train F1:  95.62%  Valid acc:  77.79% Valid f1: 70.52%\n",
            "Valid confusion matrix: \n",
            "[[548  72]\n",
            " [119 121]]\n",
            "(13216,)\n",
            "Epoch:  23 Train Loss:  0.14 Train Acc: 96.35% Train F1:  95.89%  Valid acc:  77.91% Valid f1: 70.45%\n",
            "Valid confusion matrix: \n",
            "[[551  69]\n",
            " [121 119]]\n",
            "(13216,)\n",
            "Epoch:  24 Train Loss:  0.14 Train Acc: 96.63% Train F1:  96.20%  Valid acc:  77.67% Valid f1: 70.51%\n",
            "Valid confusion matrix: \n",
            "[[546  74]\n",
            " [118 122]]\n",
            "(13216,)\n",
            "Epoch:  25 Train Loss:  0.13 Train Acc: 96.82% Train F1:  96.42%  Valid acc:  78.14% Valid f1: 70.94%\n",
            "Valid confusion matrix: \n",
            "[[550  70]\n",
            " [118 122]]\n",
            "(13216,)\n",
            "Epoch:  26 Train Loss:  0.13 Train Acc: 96.93% Train F1:  96.53%  Valid acc:  77.91% Valid f1: 71.07%\n",
            "Valid confusion matrix: \n",
            "[[544  76]\n",
            " [114 126]]\n",
            "(13216,)\n",
            "Epoch:  27 Train Loss:  0.12 Train Acc: 97.08% Train F1:  96.71%  Valid acc:  77.79% Valid f1: 71.05%\n",
            "Valid confusion matrix: \n",
            "[[542  78]\n",
            " [113 127]]\n",
            "(13216,)\n",
            "Epoch:  28 Train Loss:  0.11 Train Acc: 97.30% Train F1:  96.95%  Valid acc:  77.91% Valid f1: 71.16%\n",
            "Valid confusion matrix: \n",
            "[[543  77]\n",
            " [113 127]]\n",
            "(13216,)\n",
            "Epoch:  29 Train Loss:  0.11 Train Acc: 97.48% Train F1:  97.16%  Valid acc:  78.14% Valid f1: 71.63%\n",
            "Valid confusion matrix: \n",
            "[[542  78]\n",
            " [110 130]]\n",
            "(13216,)\n",
            "Epoch:  30 Train Loss:  0.10 Train Acc: 97.56% Train F1:  97.25%  Valid acc:  77.56% Valid f1: 70.75%\n",
            "Valid confusion matrix: \n",
            "[[541  79]\n",
            " [114 126]]\n",
            "(13216,)\n",
            "Epoch:  31 Train Loss:  0.10 Train Acc: 97.62% Train F1:  97.31%  Valid acc:  78.95% Valid f1: 71.98%\n",
            "Valid confusion matrix: \n",
            "[[554  66]\n",
            " [115 125]]\n",
            "(13216,)\n",
            "Epoch:  32 Train Loss:  0.09 Train Acc: 97.81% Train F1:  97.53%  Valid acc:  78.49% Valid f1: 71.88%\n",
            "Valid confusion matrix: \n",
            "[[546  74]\n",
            " [111 129]]\n",
            "(13216,)\n",
            "Epoch:  33 Train Loss:  0.09 Train Acc: 97.84% Train F1:  97.56%  Valid acc:  78.72% Valid f1: 71.76%\n",
            "Valid confusion matrix: \n",
            "[[552  68]\n",
            " [115 125]]\n",
            "(13216,)\n",
            "Epoch:  34 Train Loss:  0.09 Train Acc: 97.91% Train F1:  97.64%  Valid acc:  77.67% Valid f1: 71.43%\n",
            "Valid confusion matrix: \n",
            "[[535  85]\n",
            " [107 133]]\n",
            "(13216,)\n",
            "Epoch:  35 Train Loss:  0.09 Train Acc: 98.03% Train F1:  97.77%  Valid acc:  77.21% Valid f1: 71.00%\n",
            "Valid confusion matrix: \n",
            "[[531  89]\n",
            " [107 133]]\n",
            "(13216,)\n",
            "Epoch:  36 Train Loss:  0.08 Train Acc: 98.22% Train F1:  97.99%  Valid acc:  79.53% Valid f1: 72.19%\n",
            "Valid confusion matrix: \n",
            "[[563  57]\n",
            " [119 121]]\n",
            "(13216,)\n",
            "Epoch:  37 Train Loss:  0.08 Train Acc: 98.28% Train F1:  98.06%  Valid acc:  76.05% Valid f1: 70.39%\n",
            "Valid confusion matrix: \n",
            "[[515 105]\n",
            " [101 139]]\n",
            "(13216,)\n",
            "Epoch:  38 Train Loss:  0.08 Train Acc: 98.17% Train F1:  97.94%  Valid acc:  77.21% Valid f1: 70.92%\n",
            "Valid confusion matrix: \n",
            "[[532  88]\n",
            " [108 132]]\n",
            "(13216,)\n",
            "Epoch:  39 Train Loss:  0.07 Train Acc: 98.38% Train F1:  98.17%  Valid acc:  77.91% Valid f1: 71.33%\n",
            "Valid confusion matrix: \n",
            "[[541  79]\n",
            " [111 129]]\n",
            "(13216,)\n",
            "Epoch:  40 Train Loss:  0.07 Train Acc: 98.46% Train F1:  98.27%  Valid acc:  77.33% Valid f1: 70.78%\n",
            "Valid confusion matrix: \n",
            "[[536  84]\n",
            " [111 129]]\n",
            "(13216,)\n",
            "Epoch:  41 Train Loss:  0.06 Train Acc: 98.54% Train F1:  98.35%  Valid acc:  77.21% Valid f1: 70.33%\n",
            "Valid confusion matrix: \n",
            "[[539  81]\n",
            " [115 125]]\n",
            "(13216,)\n",
            "Epoch:  42 Train Loss:  0.06 Train Acc: 98.63% Train F1:  98.45%  Valid acc:  78.60% Valid f1: 71.20%\n",
            "Valid confusion matrix: \n",
            "[[556  64]\n",
            " [120 120]]\n",
            "(13216,)\n",
            "Epoch:  43 Train Loss:  0.06 Train Acc: 98.67% Train F1:  98.50%  Valid acc:  75.93% Valid f1: 69.82%\n",
            "Valid confusion matrix: \n",
            "[[520 100]\n",
            " [107 133]]\n",
            "(13216,)\n",
            "Epoch:  44 Train Loss:  0.06 Train Acc: 98.62% Train F1:  98.45%  Valid acc:  78.02% Valid f1: 70.65%\n",
            "Valid confusion matrix: \n",
            "[[551  69]\n",
            " [120 120]]\n",
            "(13216,)\n",
            "Epoch:  45 Train Loss:  0.06 Train Acc: 98.73% Train F1:  98.57%  Valid acc:  77.67% Valid f1: 70.51%\n",
            "Valid confusion matrix: \n",
            "[[546  74]\n",
            " [118 122]]\n",
            "(13216,)\n",
            "Epoch:  46 Train Loss:  0.05 Train Acc: 98.79% Train F1:  98.64%  Valid acc:  77.21% Valid f1: 70.42%\n",
            "Valid confusion matrix: \n",
            "[[538  82]\n",
            " [114 126]]\n",
            "(13216,)\n",
            "Epoch:  47 Train Loss:  0.05 Train Acc: 98.90% Train F1:  98.75%  Valid acc:  77.21% Valid f1: 69.80%\n",
            "Valid confusion matrix: \n",
            "[[545  75]\n",
            " [121 119]]\n",
            "(13216,)\n",
            "Epoch:  48 Train Loss:  0.05 Train Acc: 98.86% Train F1:  98.71%  Valid acc:  76.28% Valid f1: 69.39%\n",
            "Valid confusion matrix: \n",
            "[[532  88]\n",
            " [116 124]]\n",
            "(13216,)\n",
            "Epoch:  49 Train Loss:  0.05 Train Acc: 98.90% Train F1:  98.75%  Valid acc:  77.44% Valid f1: 69.64%\n",
            "Valid confusion matrix: \n",
            "[[551  69]\n",
            " [125 115]]\n",
            "(13216,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.88      0.84       620\n",
            "         1.0       0.60      0.49      0.54       240\n",
            "\n",
            "    accuracy                           0.77       860\n",
            "   macro avg       0.71      0.68      0.69       860\n",
            "weighted avg       0.76      0.77      0.76       860\n",
            "\n",
            "Epoch:  50 Train Loss:  0.05 Train Acc: 98.82% Train F1:  98.67%  Valid acc:  76.74% Valid f1: 69.18%\n",
            "Valid confusion matrix: \n",
            "[[543  77]\n",
            " [123 117]]\n",
            "(13216,)\n",
            "Epoch:  01 Train Loss:  0.65 Train Acc: 64.13% Train F1:  45.15%  Valid acc:  70.23% Valid f1: 43.42%\n",
            "Valid confusion matrix: \n",
            "[[598  22]\n",
            " [234   6]]\n",
            "(13216,)\n",
            "Epoch:  02 Train Loss:  0.63 Train Acc: 66.73% Train F1:  40.09%  Valid acc:  72.33% Valid f1: 43.17%\n",
            "Valid confusion matrix: \n",
            "[[619   1]\n",
            " [237   3]]\n",
            "(13216,)\n",
            "Epoch:  03 Train Loss:  0.57 Train Acc: 69.29% Train F1:  52.99%  Valid acc:  74.77% Valid f1: 54.87%\n",
            "Valid confusion matrix: \n",
            "[[607  13]\n",
            " [204  36]]\n",
            "(13216,)\n",
            "Epoch:  04 Train Loss:  0.52 Train Acc: 76.25% Train F1:  70.27%  Valid acc:  76.51% Valid f1: 60.84%\n",
            "Valid confusion matrix: \n",
            "[[601  19]\n",
            " [183  57]]\n",
            "(13216,)\n",
            "Epoch:  05 Train Loss:  0.47 Train Acc: 80.71% Train F1:  77.01%  Valid acc:  78.49% Valid f1: 66.17%\n",
            "Valid confusion matrix: \n",
            "[[597  23]\n",
            " [162  78]]\n",
            "(13216,)\n",
            "Epoch:  06 Train Loss:  0.44 Train Acc: 83.42% Train F1:  80.62%  Valid acc:  78.72% Valid f1: 68.18%\n",
            "Valid confusion matrix: \n",
            "[[586  34]\n",
            " [149  91]]\n",
            "(13216,)\n",
            "Epoch:  07 Train Loss:  0.40 Train Acc: 85.45% Train F1:  83.18%  Valid acc:  79.19% Valid f1: 69.00%\n",
            "Valid confusion matrix: \n",
            "[[587  33]\n",
            " [146  94]]\n",
            "(13216,)\n",
            "Epoch:  08 Train Loss:  0.37 Train Acc: 86.97% Train F1:  85.00%  Valid acc:  79.07% Valid f1: 69.48%\n",
            "Valid confusion matrix: \n",
            "[[581  39]\n",
            " [141  99]]\n",
            "(13216,)\n",
            "Epoch:  09 Train Loss:  0.35 Train Acc: 88.30% Train F1:  86.61%  Valid acc:  79.65% Valid f1: 70.16%\n",
            "Valid confusion matrix: \n",
            "[[585  35]\n",
            " [140 100]]\n",
            "(13216,)\n",
            "Epoch:  10 Train Loss:  0.33 Train Acc: 89.23% Train F1:  87.72%  Valid acc:  79.19% Valid f1: 69.94%\n",
            "Valid confusion matrix: \n",
            "[[579  41]\n",
            " [138 102]]\n",
            "(13216,)\n",
            "Epoch:  11 Train Loss:  0.30 Train Acc: 90.46% Train F1:  89.13%  Valid acc:  79.19% Valid f1: 70.38%\n",
            "Valid confusion matrix: \n",
            "[[575  45]\n",
            " [134 106]]\n",
            "(13216,)\n",
            "Epoch:  12 Train Loss:  0.29 Train Acc: 91.09% Train F1:  89.89%  Valid acc:  79.77% Valid f1: 71.56%\n",
            "Valid confusion matrix: \n",
            "[[574  46]\n",
            " [128 112]]\n",
            "(13216,)\n",
            "Epoch:  13 Train Loss:  0.27 Train Acc: 91.61% Train F1:  90.49%  Valid acc:  79.88% Valid f1: 71.05%\n",
            "Valid confusion matrix: \n",
            "[[581  39]\n",
            " [134 106]]\n",
            "(13216,)\n",
            "Epoch:  14 Train Loss:  0.28 Train Acc: 91.18% Train F1:  89.86%  Valid acc:  79.30% Valid f1: 71.11%\n",
            "Valid confusion matrix: \n",
            "[[570  50]\n",
            " [128 112]]\n",
            "(13216,)\n",
            "Epoch:  15 Train Loss:  0.25 Train Acc: 92.52% Train F1:  91.51%  Valid acc:  79.07% Valid f1: 71.27%\n",
            "Valid confusion matrix: \n",
            "[[564  56]\n",
            " [124 116]]\n",
            "(13216,)\n",
            "Epoch:  16 Train Loss:  0.24 Train Acc: 93.32% Train F1:  92.40%  Valid acc:  79.53% Valid f1: 72.01%\n",
            "Valid confusion matrix: \n",
            "[[565  55]\n",
            " [121 119]]\n",
            "(13216,)\n",
            "Epoch:  17 Train Loss:  0.22 Train Acc: 93.80% Train F1:  92.97%  Valid acc:  80.12% Valid f1: 72.48%\n",
            "Valid confusion matrix: \n",
            "[[571  49]\n",
            " [122 118]]\n",
            "(13216,)\n",
            "Epoch:  18 Train Loss:  0.21 Train Acc: 94.21% Train F1:  93.43%  Valid acc:  79.77% Valid f1: 71.76%\n",
            "Valid confusion matrix: \n",
            "[[572  48]\n",
            " [126 114]]\n",
            "(13216,)\n",
            "Epoch:  19 Train Loss:  0.20 Train Acc: 94.57% Train F1:  93.84%  Valid acc:  79.30% Valid f1: 71.50%\n",
            "Valid confusion matrix: \n",
            "[[566  54]\n",
            " [124 116]]\n",
            "(13216,)\n",
            "Epoch:  20 Train Loss:  0.19 Train Acc: 94.86% Train F1:  94.18%  Valid acc:  79.30% Valid f1: 71.50%\n",
            "Valid confusion matrix: \n",
            "[[566  54]\n",
            " [124 116]]\n",
            "(13216,)\n",
            "Epoch:  21 Train Loss:  0.19 Train Acc: 95.13% Train F1:  94.47%  Valid acc:  79.53% Valid f1: 71.03%\n",
            "Valid confusion matrix: \n",
            "[[575  45]\n",
            " [131 109]]\n",
            "(13216,)\n",
            "Epoch:  22 Train Loss:  0.18 Train Acc: 95.27% Train F1:  94.65%  Valid acc:  79.53% Valid f1: 71.82%\n",
            "Valid confusion matrix: \n",
            "[[567  53]\n",
            " [123 117]]\n",
            "(13216,)\n",
            "Epoch:  23 Train Loss:  0.17 Train Acc: 95.58% Train F1:  95.00%  Valid acc:  79.07% Valid f1: 71.65%\n",
            "Valid confusion matrix: \n",
            "[[560  60]\n",
            " [120 120]]\n",
            "(13216,)\n",
            "Epoch:  24 Train Loss:  0.17 Train Acc: 95.71% Train F1:  95.15%  Valid acc:  79.77% Valid f1: 72.32%\n",
            "Valid confusion matrix: \n",
            "[[566  54]\n",
            " [120 120]]\n",
            "(13216,)\n",
            "Epoch:  25 Train Loss:  0.16 Train Acc: 96.09% Train F1:  95.57%  Valid acc:  79.53% Valid f1: 71.91%\n",
            "Valid confusion matrix: \n",
            "[[566  54]\n",
            " [122 118]]\n",
            "(13216,)\n",
            "Epoch:  26 Train Loss:  0.15 Train Acc: 96.19% Train F1:  95.70%  Valid acc:  78.72% Valid f1: 71.13%\n",
            "Valid confusion matrix: \n",
            "[[559  61]\n",
            " [122 118]]\n",
            "(13216,)\n",
            "Epoch:  27 Train Loss:  0.15 Train Acc: 96.09% Train F1:  95.57%  Valid acc:  78.72% Valid f1: 71.22%\n",
            "Valid confusion matrix: \n",
            "[[558  62]\n",
            " [121 119]]\n",
            "(13216,)\n",
            "Epoch:  28 Train Loss:  0.14 Train Acc: 96.43% Train F1:  95.95%  Valid acc:  77.91% Valid f1: 71.16%\n",
            "Valid confusion matrix: \n",
            "[[543  77]\n",
            " [113 127]]\n",
            "(13216,)\n",
            "Epoch:  29 Train Loss:  0.14 Train Acc: 96.57% Train F1:  96.12%  Valid acc:  78.60% Valid f1: 71.02%\n",
            "Valid confusion matrix: \n",
            "[[558  62]\n",
            " [122 118]]\n",
            "(13216,)\n",
            "Epoch:  30 Train Loss:  0.14 Train Acc: 96.44% Train F1:  95.99%  Valid acc:  78.26% Valid f1: 70.69%\n",
            "Valid confusion matrix: \n",
            "[[555  65]\n",
            " [122 118]]\n",
            "(13216,)\n",
            "Epoch:  31 Train Loss:  0.13 Train Acc: 96.75% Train F1:  96.33%  Valid acc:  78.84% Valid f1: 71.43%\n",
            "Valid confusion matrix: \n",
            "[[558  62]\n",
            " [120 120]]\n",
            "(13216,)\n",
            "Epoch:  32 Train Loss:  0.13 Train Acc: 96.98% Train F1:  96.58%  Valid acc:  78.72% Valid f1: 71.22%\n",
            "Valid confusion matrix: \n",
            "[[558  62]\n",
            " [121 119]]\n",
            "(13216,)\n",
            "Epoch:  33 Train Loss:  0.12 Train Acc: 97.22% Train F1:  96.86%  Valid acc:  78.72% Valid f1: 71.04%\n",
            "Valid confusion matrix: \n",
            "[[560  60]\n",
            " [123 117]]\n",
            "(13216,)\n",
            "Epoch:  34 Train Loss:  0.12 Train Acc: 97.16% Train F1:  96.79%  Valid acc:  78.02% Valid f1: 70.65%\n",
            "Valid confusion matrix: \n",
            "[[551  69]\n",
            " [120 120]]\n",
            "(13216,)\n",
            "Epoch:  35 Train Loss:  0.12 Train Acc: 97.05% Train F1:  96.66%  Valid acc:  77.67% Valid f1: 70.94%\n",
            "Valid confusion matrix: \n",
            "[[541  79]\n",
            " [113 127]]\n",
            "(13216,)\n",
            "Epoch:  36 Train Loss:  0.12 Train Acc: 97.09% Train F1:  96.71%  Valid acc:  78.60% Valid f1: 71.20%\n",
            "Valid confusion matrix: \n",
            "[[556  64]\n",
            " [120 120]]\n",
            "(13216,)\n",
            "Epoch:  37 Train Loss:  0.11 Train Acc: 97.44% Train F1:  97.11%  Valid acc:  78.02% Valid f1: 70.56%\n",
            "Valid confusion matrix: \n",
            "[[552  68]\n",
            " [121 119]]\n",
            "(13216,)\n",
            "Epoch:  38 Train Loss:  0.11 Train Acc: 97.53% Train F1:  97.20%  Valid acc:  78.49% Valid f1: 70.53%\n",
            "Valid confusion matrix: \n",
            "[[561  59]\n",
            " [126 114]]\n",
            "(13216,)\n",
            "Epoch:  39 Train Loss:  0.11 Train Acc: 97.38% Train F1:  97.04%  Valid acc:  78.72% Valid f1: 71.13%\n",
            "Valid confusion matrix: \n",
            "[[559  61]\n",
            " [122 118]]\n",
            "(13216,)\n",
            "Epoch:  40 Train Loss:  0.11 Train Acc: 97.34% Train F1:  97.00%  Valid acc:  78.84% Valid f1: 71.52%\n",
            "Valid confusion matrix: \n",
            "[[557  63]\n",
            " [119 121]]\n",
            "(13216,)\n",
            "Epoch:  41 Train Loss:  0.10 Train Acc: 97.67% Train F1:  97.36%  Valid acc:  78.26% Valid f1: 71.05%\n",
            "Valid confusion matrix: \n",
            "[[551  69]\n",
            " [118 122]]\n",
            "(13216,)\n",
            "Epoch:  42 Train Loss:  0.10 Train Acc: 97.87% Train F1:  97.59%  Valid acc:  77.91% Valid f1: 70.72%\n",
            "Valid confusion matrix: \n",
            "[[548  72]\n",
            " [118 122]]\n",
            "(13216,)\n",
            "Epoch:  43 Train Loss:  0.10 Train Acc: 97.89% Train F1:  97.61%  Valid acc:  76.98% Valid f1: 69.94%\n",
            "Valid confusion matrix: \n",
            "[[539  81]\n",
            " [117 123]]\n",
            "(13216,)\n",
            "Epoch:  44 Train Loss:  0.09 Train Acc: 97.92% Train F1:  97.65%  Valid acc:  78.02% Valid f1: 70.47%\n",
            "Valid confusion matrix: \n",
            "[[553  67]\n",
            " [122 118]]\n",
            "(13216,)\n",
            "Epoch:  45 Train Loss:  0.09 Train Acc: 97.96% Train F1:  97.69%  Valid acc:  77.56% Valid f1: 70.31%\n",
            "Valid confusion matrix: \n",
            "[[546  74]\n",
            " [119 121]]\n",
            "(13216,)\n",
            "Epoch:  46 Train Loss:  0.09 Train Acc: 97.98% Train F1:  97.72%  Valid acc:  77.91% Valid f1: 70.81%\n",
            "Valid confusion matrix: \n",
            "[[547  73]\n",
            " [117 123]]\n",
            "(13216,)\n",
            "Epoch:  47 Train Loss:  0.09 Train Acc: 97.96% Train F1:  97.69%  Valid acc:  78.02% Valid f1: 70.56%\n",
            "Valid confusion matrix: \n",
            "[[552  68]\n",
            " [121 119]]\n",
            "(13216,)\n",
            "Epoch:  48 Train Loss:  0.09 Train Acc: 97.83% Train F1:  97.55%  Valid acc:  78.72% Valid f1: 71.13%\n",
            "Valid confusion matrix: \n",
            "[[559  61]\n",
            " [122 118]]\n",
            "(13216,)\n",
            "Epoch:  49 Train Loss:  0.09 Train Acc: 98.13% Train F1:  97.89%  Valid acc:  77.91% Valid f1: 70.63%\n",
            "Valid confusion matrix: \n",
            "[[549  71]\n",
            " [119 121]]\n",
            "(13216,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.90      0.86       620\n",
            "         1.0       0.65      0.47      0.55       240\n",
            "\n",
            "    accuracy                           0.78       860\n",
            "   macro avg       0.73      0.69      0.70       860\n",
            "weighted avg       0.77      0.78      0.77       860\n",
            "\n",
            "Epoch:  50 Train Loss:  0.08 Train Acc: 98.21% Train F1:  97.97%  Valid acc:  78.26% Valid f1: 70.31%\n",
            "Valid confusion matrix: \n",
            "[[559  61]\n",
            " [126 114]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKuYA5E2v8Fd",
        "colab_type": "text"
      },
      "source": [
        "# CNN with Multiple Filter sizes\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IQk8SVswMHF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class CNN(nn.Module):\n",
        "    \"\"\"\n",
        "    DPCNN for sentences classification.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        VOCAB_SIZE=18122\n",
        "        EMBEDDING_DIM=300\n",
        "        HIDDEN_LAYER=64\n",
        "        OUTPUT_DIM=1\n",
        "        MAX_LEN=18122\n",
        "        OUTPUT_CLASSES=1\n",
        "        MAX_SENT_LEN=50\n",
        "        window_size=1\n",
        "\n",
        "        \n",
        "        self.EMBEDDING_DIM=EMBEDDING_DIM\n",
        "        self.MAX_SENT_LEN=MAX_SENT_LEN\n",
        "        self.embedding = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM)\n",
        "      \n",
        "       # self.embedding.weight = nn.Parameter(embedding)\n",
        "       # self.embedding.weight.requires_grad = update_embedding\n",
        "        self.dropout = nn.Dropout(p=0.1) #  dropout layer\n",
        "\n",
        "        #KERNEL_SIZE=3\n",
        "\n",
        "        KERNEL_SIZES = [3,1]\n",
        "        self.NUM_KERNEL = 32\n",
        "        self.KERNEL_COUNT=len(KERNEL_SIZES)\n",
        "        #self.conv_region_embedding = nn.Conv2d(1, self.num_filter, (filter_size, EMBEDDING_DIM), stride=2)\n",
        "        #self.conv = nn.Conv1d(in_channels=1, out_channels=64,  kernel_size=(KERNEL_SIZE, EMBEDDING_DIM), stride=1) #conv for 1 kernel size\n",
        "        \n",
        "        self.convolution=nn.ModuleList([nn.Conv2d(1,32, (K, EMBEDDING_DIM)) for K in KERNEL_SIZES])\n",
        "        \n",
        "        #self.maxpool = nn.MaxPool1d(kernel_size=5)\n",
        "        self.fully_connected = nn.Linear(self.NUM_KERNEL*self.KERNEL_COUNT, OUTPUT_CLASSES)\n",
        "        self.rel = nn.ReLU()\n",
        "        self.sig = nn.Sigmoid()\n",
        "        self.soft=nn.Softmax()\n",
        "\n",
        "    def forward(self, x, lengths=None):\n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convolution] \n",
        "        #x = self.conv(x) # x becomes feature map\n",
        "        #x=x.squeeze(3)\n",
        "        #x= self.rel(x)\n",
        "\n",
        "        \n",
        "\n",
        "        #pooled = F.max_pool1d(feature_maps, feature_maps.shape[2])\n",
        "        x = [F.max_pool1d(i, i.shape[2]).squeeze(2) for i in x]  \n",
        "        \n",
        "        #print('IAMHERE2')\n",
        "      # x = x.squeeze(2)\n",
        "        \n",
        "        x = torch.cat(x, 1)\n",
        "        \n",
        "        x = self.dropout(x)\n",
        "\n",
        "        #print('here')\n",
        "        output = self.sig(self.fully_connected(x))\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCBdG5EPrFB0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "806204d2-7114-40a4-feda-824b2e5e1c10"
      },
      "source": [
        "XTrainTens=torch.LongTensor(X_train_initial)\n",
        "XtestTens=torch.LongTensor(X_test_initial)\n",
        "ytestTens=torch.FloatTensor( y_test)\n",
        "y_trainTens=torch.FloatTensor(y_train)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "cnn_notstatic=CNN()\n",
        "#Non Static\n",
        "train(cnn_notstatic,50,0.001,embedding_matrix,True,XTrainTens, y_trainTens, XtestTens, ytestTens,device)\n",
        "#Static\n",
        "cnn_static=CNN()\n",
        "\n",
        "train(cnn_static,50,0.001,embedding_matrix,False,XTrainTens, y_trainTens, XtestTens, ytestTens,device)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(13216,)\n",
            "Epoch:  01 Train Loss:  0.70 Train Acc: 33.24% Train F1:  24.95%  Valid acc:  27.91% Valid f1: 21.82%\n",
            "Valid confusion matrix: \n",
            "[[  0 620]\n",
            " [  0 240]]\n",
            "(13216,)\n",
            "Epoch:  02 Train Loss:  0.69 Train Acc: 33.26% Train F1:  24.96%  Valid acc:  27.79% Valid f1: 21.75%\n",
            "Valid confusion matrix: \n",
            "[[  0 620]\n",
            " [  1 239]]\n",
            "(13216,)\n",
            "Epoch:  03 Train Loss:  0.69 Train Acc: 33.24% Train F1:  24.95%  Valid acc:  28.60% Valid f1: 22.90%\n",
            "Valid confusion matrix: \n",
            "[[  6 614]\n",
            " [  0 240]]\n",
            "(13216,)\n",
            "Epoch:  04 Train Loss:  0.69 Train Acc: 33.24% Train F1:  24.95%  Valid acc:  29.42% Valid f1: 24.33%\n",
            "Valid confusion matrix: \n",
            "[[ 15 605]\n",
            " [  2 238]]\n",
            "(13216,)\n",
            "Epoch:  05 Train Loss:  0.69 Train Acc: 33.22% Train F1:  24.93%  Valid acc:  30.23% Valid f1: 25.90%\n",
            "Valid confusion matrix: \n",
            "[[ 26 594]\n",
            " [  6 234]]\n",
            "(13216,)\n",
            "Epoch:  06 Train Loss:  0.69 Train Acc: 33.25% Train F1:  24.95%  Valid acc:  27.91% Valid f1: 21.82%\n",
            "Valid confusion matrix: \n",
            "[[  0 620]\n",
            " [  0 240]]\n",
            "(13216,)\n",
            "Epoch:  07 Train Loss:  0.66 Train Acc: 33.22% Train F1:  24.93%  Valid acc:  32.44% Valid f1: 28.70%\n",
            "Valid confusion matrix: \n",
            "[[ 41 579]\n",
            " [  2 238]]\n",
            "(13216,)\n",
            "Epoch:  08 Train Loss:  0.65 Train Acc: 33.23% Train F1:  24.94%  Valid acc:  28.84% Valid f1: 23.36%\n",
            "Valid confusion matrix: \n",
            "[[  9 611]\n",
            " [  1 239]]\n",
            "(13216,)\n",
            "Epoch:  09 Train Loss:  0.63 Train Acc: 33.27% Train F1:  24.96%  Valid acc:  31.05% Valid f1: 26.63%\n",
            "Valid confusion matrix: \n",
            "[[ 28 592]\n",
            " [  1 239]]\n",
            "(13216,)\n",
            "Epoch:  10 Train Loss:  0.62 Train Acc: 33.24% Train F1:  24.95%  Valid acc:  37.44% Valid f1: 35.48%\n",
            "Valid confusion matrix: \n",
            "[[ 86 534]\n",
            " [  4 236]]\n",
            "(13216,)\n",
            "Epoch:  11 Train Loss:  0.61 Train Acc: 33.24% Train F1:  24.95%  Valid acc:  31.86% Valid f1: 28.13%\n",
            "Valid confusion matrix: \n",
            "[[ 39 581]\n",
            " [  5 235]]\n",
            "(13216,)\n",
            "Epoch:  12 Train Loss:  0.61 Train Acc: 33.25% Train F1:  24.95%  Valid acc:  33.49% Valid f1: 30.58%\n",
            "Valid confusion matrix: \n",
            "[[ 56 564]\n",
            " [  8 232]]\n",
            "(13216,)\n",
            "Epoch:  13 Train Loss:  0.60 Train Acc: 33.22% Train F1:  24.93%  Valid acc:  36.51% Valid f1: 34.78%\n",
            "Valid confusion matrix: \n",
            "[[ 87 533]\n",
            " [ 13 227]]\n",
            "(13216,)\n",
            "Epoch:  14 Train Loss:  0.60 Train Acc: 33.30% Train F1:  25.03%  Valid acc:  38.72% Valid f1: 37.44%\n",
            "Valid confusion matrix: \n",
            "[[105 515]\n",
            " [ 12 228]]\n",
            "(13216,)\n",
            "Epoch:  15 Train Loss:  0.59 Train Acc: 33.26% Train F1:  25.02%  Valid acc:  44.77% Valid f1: 44.49%\n",
            "Valid confusion matrix: \n",
            "[[162 458]\n",
            " [ 17 223]]\n",
            "(13216,)\n",
            "Epoch:  16 Train Loss:  0.59 Train Acc: 33.32% Train F1:  25.11%  Valid acc:  40.35% Valid f1: 39.41%\n",
            "Valid confusion matrix: \n",
            "[[120 500]\n",
            " [ 13 227]]\n",
            "(13216,)\n",
            "Epoch:  17 Train Loss:  0.59 Train Acc: 33.35% Train F1:  25.12%  Valid acc:  41.05% Valid f1: 40.44%\n",
            "Valid confusion matrix: \n",
            "[[133 487]\n",
            " [ 20 220]]\n",
            "(13216,)\n",
            "Epoch:  18 Train Loss:  0.59 Train Acc: 33.29% Train F1:  25.09%  Valid acc:  45.70% Valid f1: 45.61%\n",
            "Valid confusion matrix: \n",
            "[[179 441]\n",
            " [ 26 214]]\n",
            "(13216,)\n",
            "Epoch:  19 Train Loss:  0.59 Train Acc: 33.46% Train F1:  25.35%  Valid acc:  46.05% Valid f1: 45.92%\n",
            "Valid confusion matrix: \n",
            "[[177 443]\n",
            " [ 21 219]]\n",
            "(13216,)\n",
            "Epoch:  20 Train Loss:  0.59 Train Acc: 33.63% Train F1:  25.62%  Valid acc:  50.81% Valid f1: 50.81%\n",
            "Valid confusion matrix: \n",
            "[[224 396]\n",
            " [ 27 213]]\n",
            "(13216,)\n",
            "Epoch:  21 Train Loss:  0.59 Train Acc: 33.72% Train F1:  25.82%  Valid acc:  53.02% Valid f1: 52.91%\n",
            "Valid confusion matrix: \n",
            "[[249 371]\n",
            " [ 33 207]]\n",
            "(13216,)\n",
            "Epoch:  22 Train Loss:  0.58 Train Acc: 33.94% Train F1:  26.17%  Valid acc:  52.09% Valid f1: 52.08%\n",
            "Valid confusion matrix: \n",
            "[[232 388]\n",
            " [ 24 216]]\n",
            "(13216,)\n",
            "Epoch:  23 Train Loss:  0.58 Train Acc: 34.11% Train F1:  26.46%  Valid acc:  54.53% Valid f1: 54.32%\n",
            "Valid confusion matrix: \n",
            "[[264 356]\n",
            " [ 35 205]]\n",
            "(13216,)\n",
            "Epoch:  24 Train Loss:  0.58 Train Acc: 34.48% Train F1:  27.08%  Valid acc:  50.81% Valid f1: 50.81%\n",
            "Valid confusion matrix: \n",
            "[[222 398]\n",
            " [ 25 215]]\n",
            "(13216,)\n",
            "Epoch:  25 Train Loss:  0.58 Train Acc: 34.60% Train F1:  27.27%  Valid acc:  54.42% Valid f1: 54.25%\n",
            "Valid confusion matrix: \n",
            "[[260 360]\n",
            " [ 32 208]]\n",
            "(13216,)\n",
            "Epoch:  26 Train Loss:  0.58 Train Acc: 34.81% Train F1:  27.64%  Valid acc:  58.02% Valid f1: 57.41%\n",
            "Valid confusion matrix: \n",
            "[[301 319]\n",
            " [ 42 198]]\n",
            "(13216,)\n",
            "Epoch:  27 Train Loss:  0.58 Train Acc: 35.95% Train F1:  29.50%  Valid acc:  58.37% Valid f1: 57.87%\n",
            "Valid confusion matrix: \n",
            "[[298 322]\n",
            " [ 36 204]]\n",
            "(13216,)\n",
            "Epoch:  28 Train Loss:  0.58 Train Acc: 36.32% Train F1:  30.14%  Valid acc:  60.23% Valid f1: 59.52%\n",
            "Valid confusion matrix: \n",
            "[[316 304]\n",
            " [ 38 202]]\n",
            "(13216,)\n",
            "Epoch:  29 Train Loss:  0.58 Train Acc: 36.52% Train F1:  30.36%  Valid acc:  57.56% Valid f1: 57.10%\n",
            "Valid confusion matrix: \n",
            "[[292 328]\n",
            " [ 37 203]]\n",
            "(13216,)\n",
            "Epoch:  30 Train Loss:  0.58 Train Acc: 36.80% Train F1:  30.82%  Valid acc:  63.60% Valid f1: 62.14%\n",
            "Valid confusion matrix: \n",
            "[[358 262]\n",
            " [ 51 189]]\n",
            "(13216,)\n",
            "Epoch:  31 Train Loss:  0.58 Train Acc: 38.26% Train F1:  33.09%  Valid acc:  58.84% Valid f1: 58.23%\n",
            "Valid confusion matrix: \n",
            "[[305 315]\n",
            " [ 39 201]]\n",
            "(13216,)\n",
            "Epoch:  32 Train Loss:  0.58 Train Acc: 38.45% Train F1:  33.41%  Valid acc:  58.95% Valid f1: 58.28%\n",
            "Valid confusion matrix: \n",
            "[[308 312]\n",
            " [ 41 199]]\n",
            "(13216,)\n",
            "Epoch:  33 Train Loss:  0.58 Train Acc: 39.27% Train F1:  34.61%  Valid acc:  64.30% Valid f1: 62.69%\n",
            "Valid confusion matrix: \n",
            "[[366 254]\n",
            " [ 53 187]]\n",
            "(13216,)\n",
            "Epoch:  34 Train Loss:  0.58 Train Acc: 38.98% Train F1:  34.24%  Valid acc:  60.12% Valid f1: 59.23%\n",
            "Valid confusion matrix: \n",
            "[[322 298]\n",
            " [ 45 195]]\n",
            "(13216,)\n",
            "Epoch:  35 Train Loss:  0.58 Train Acc: 39.97% Train F1:  35.63%  Valid acc:  63.26% Valid f1: 61.72%\n",
            "Valid confusion matrix: \n",
            "[[358 262]\n",
            " [ 54 186]]\n",
            "(13216,)\n",
            "Epoch:  36 Train Loss:  0.58 Train Acc: 41.43% Train F1:  37.70%  Valid acc:  60.93% Valid f1: 60.13%\n",
            "Valid confusion matrix: \n",
            "[[323 297]\n",
            " [ 39 201]]\n",
            "(13216,)\n",
            "Epoch:  37 Train Loss:  0.58 Train Acc: 41.55% Train F1:  37.92%  Valid acc:  63.72% Valid f1: 62.06%\n",
            "Valid confusion matrix: \n",
            "[[364 256]\n",
            " [ 56 184]]\n",
            "(13216,)\n",
            "Epoch:  38 Train Loss:  0.58 Train Acc: 42.58% Train F1:  39.35%  Valid acc:  61.98% Valid f1: 60.73%\n",
            "Valid confusion matrix: \n",
            "[[343 277]\n",
            " [ 50 190]]\n",
            "(13216,)\n",
            "Epoch:  39 Train Loss:  0.58 Train Acc: 43.13% Train F1:  40.09%  Valid acc:  63.14% Valid f1: 61.66%\n",
            "Valid confusion matrix: \n",
            "[[356 264]\n",
            " [ 53 187]]\n",
            "(13216,)\n",
            "Epoch:  40 Train Loss:  0.58 Train Acc: 44.70% Train F1:  42.18%  Valid acc:  66.05% Valid f1: 63.62%\n",
            "Valid confusion matrix: \n",
            "[[395 225]\n",
            " [ 67 173]]\n",
            "(13216,)\n",
            "Epoch:  41 Train Loss:  0.58 Train Acc: 44.17% Train F1:  41.49%  Valid acc:  64.42% Valid f1: 62.71%\n",
            "Valid confusion matrix: \n",
            "[[369 251]\n",
            " [ 55 185]]\n",
            "(13216,)\n",
            "Epoch:  42 Train Loss:  0.58 Train Acc: 45.13% Train F1:  42.76%  Valid acc:  61.16% Valid f1: 60.07%\n",
            "Valid confusion matrix: \n",
            "[[334 286]\n",
            " [ 48 192]]\n",
            "(13216,)\n",
            "Epoch:  43 Train Loss:  0.58 Train Acc: 46.80% Train F1:  44.89%  Valid acc:  64.07% Valid f1: 62.63%\n",
            "Valid confusion matrix: \n",
            "[[360 260]\n",
            " [ 49 191]]\n",
            "(13216,)\n",
            "Epoch:  44 Train Loss:  0.58 Train Acc: 47.12% Train F1:  45.30%  Valid acc:  61.28% Valid f1: 60.08%\n",
            "Valid confusion matrix: \n",
            "[[338 282]\n",
            " [ 51 189]]\n",
            "(13216,)\n",
            "Epoch:  45 Train Loss:  0.58 Train Acc: 47.67% Train F1:  45.99%  Valid acc:  65.58% Valid f1: 63.65%\n",
            "Valid confusion matrix: \n",
            "[[381 239]\n",
            " [ 57 183]]\n",
            "(13216,)\n",
            "Epoch:  46 Train Loss:  0.58 Train Acc: 48.59% Train F1:  47.09%  Valid acc:  64.19% Valid f1: 62.51%\n",
            "Valid confusion matrix: \n",
            "[[367 253]\n",
            " [ 55 185]]\n",
            "(13216,)\n",
            "Epoch:  47 Train Loss:  0.58 Train Acc: 47.47% Train F1:  45.73%  Valid acc:  66.28% Valid f1: 64.27%\n",
            "Valid confusion matrix: \n",
            "[[387 233]\n",
            " [ 57 183]]\n",
            "(13216,)\n",
            "Epoch:  48 Train Loss:  0.58 Train Acc: 47.70% Train F1:  46.01%  Valid acc:  69.07% Valid f1: 66.64%\n",
            "Valid confusion matrix: \n",
            "[[413 207]\n",
            " [ 59 181]]\n",
            "(13216,)\n",
            "Epoch:  49 Train Loss:  0.58 Train Acc: 49.61% Train F1:  48.36%  Valid acc:  65.47% Valid f1: 63.75%\n",
            "Valid confusion matrix: \n",
            "[[375 245]\n",
            " [ 52 188]]\n",
            "(13216,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.66      0.75       620\n",
            "         1.0       0.45      0.72      0.56       240\n",
            "\n",
            "    accuracy                           0.68       860\n",
            "   macro avg       0.66      0.69      0.65       860\n",
            "weighted avg       0.75      0.68      0.69       860\n",
            "\n",
            "Epoch:  50 Train Loss:  0.58 Train Acc: 49.24% Train F1:  47.90%  Valid acc:  67.79% Valid f1: 65.14%\n",
            "Valid confusion matrix: \n",
            "[[410 210]\n",
            " [ 67 173]]\n",
            "(13216,)\n",
            "Epoch:  01 Train Loss:  0.69 Train Acc: 39.15% Train F1:  37.28%  Valid acc:  67.79% Valid f1: 48.19%\n",
            "Valid confusion matrix: \n",
            "[[556  64]\n",
            " [213  27]]\n",
            "(13216,)\n",
            "Epoch:  02 Train Loss:  0.69 Train Acc: 39.61% Train F1:  38.11%  Valid acc:  67.79% Valid f1: 47.70%\n",
            "Valid confusion matrix: \n",
            "[[558  62]\n",
            " [215  25]]\n",
            "(13216,)\n",
            "Epoch:  03 Train Loss:  0.69 Train Acc: 38.20% Train F1:  35.58%  Valid acc:  66.28% Valid f1: 51.74%\n",
            "Valid confusion matrix: \n",
            "[[521  99]\n",
            " [191  49]]\n",
            "(13216,)\n",
            "Epoch:  04 Train Loss:  0.69 Train Acc: 51.95% Train F1:  49.46%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  05 Train Loss:  0.69 Train Acc: 66.78% Train F1:  40.04%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  06 Train Loss:  0.69 Train Acc: 66.75% Train F1:  40.03%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  07 Train Loss:  0.69 Train Acc: 66.78% Train F1:  40.04%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  08 Train Loss:  0.69 Train Acc: 66.77% Train F1:  40.04%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  09 Train Loss:  0.69 Train Acc: 66.74% Train F1:  40.05%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  10 Train Loss:  0.69 Train Acc: 66.76% Train F1:  40.03%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  11 Train Loss:  0.69 Train Acc: 66.76% Train F1:  40.03%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  12 Train Loss:  0.69 Train Acc: 66.75% Train F1:  40.03%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  13 Train Loss:  0.69 Train Acc: 66.78% Train F1:  40.04%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  14 Train Loss:  0.69 Train Acc: 66.74% Train F1:  40.03%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  15 Train Loss:  0.69 Train Acc: 66.79% Train F1:  40.04%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  16 Train Loss:  0.69 Train Acc: 66.77% Train F1:  40.04%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  17 Train Loss:  0.69 Train Acc: 66.75% Train F1:  40.03%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  18 Train Loss:  0.69 Train Acc: 66.80% Train F1:  40.05%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  19 Train Loss:  0.69 Train Acc: 66.78% Train F1:  40.06%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  20 Train Loss:  0.69 Train Acc: 66.74% Train F1:  40.03%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  21 Train Loss:  0.69 Train Acc: 66.77% Train F1:  40.04%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  22 Train Loss:  0.69 Train Acc: 66.78% Train F1:  40.06%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  23 Train Loss:  0.69 Train Acc: 66.76% Train F1:  40.03%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  24 Train Loss:  0.69 Train Acc: 66.74% Train F1:  40.03%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  25 Train Loss:  0.69 Train Acc: 66.76% Train F1:  40.03%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  26 Train Loss:  0.69 Train Acc: 66.77% Train F1:  40.04%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  27 Train Loss:  0.69 Train Acc: 66.79% Train F1:  40.04%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  28 Train Loss:  0.69 Train Acc: 66.81% Train F1:  40.05%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  29 Train Loss:  0.69 Train Acc: 66.74% Train F1:  40.03%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  30 Train Loss:  0.69 Train Acc: 66.74% Train F1:  40.03%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  31 Train Loss:  0.69 Train Acc: 66.74% Train F1:  40.05%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  32 Train Loss:  0.69 Train Acc: 66.80% Train F1:  40.05%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  33 Train Loss:  0.69 Train Acc: 66.77% Train F1:  40.04%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  34 Train Loss:  0.69 Train Acc: 66.81% Train F1:  40.05%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  35 Train Loss:  0.69 Train Acc: 66.76% Train F1:  40.03%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  36 Train Loss:  0.69 Train Acc: 66.75% Train F1:  40.03%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  37 Train Loss:  0.69 Train Acc: 66.80% Train F1:  40.05%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  38 Train Loss:  0.69 Train Acc: 66.78% Train F1:  40.06%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  39 Train Loss:  0.69 Train Acc: 66.78% Train F1:  40.04%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  40 Train Loss:  0.69 Train Acc: 66.75% Train F1:  40.03%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  41 Train Loss:  0.69 Train Acc: 66.78% Train F1:  40.04%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  42 Train Loss:  0.69 Train Acc: 66.78% Train F1:  40.04%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  43 Train Loss:  0.69 Train Acc: 66.76% Train F1:  40.03%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  44 Train Loss:  0.69 Train Acc: 66.78% Train F1:  40.04%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  45 Train Loss:  0.69 Train Acc: 66.78% Train F1:  40.04%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  46 Train Loss:  0.69 Train Acc: 66.74% Train F1:  40.03%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  47 Train Loss:  0.69 Train Acc: 66.76% Train F1:  40.03%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  48 Train Loss:  0.69 Train Acc: 66.75% Train F1:  40.03%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "Epoch:  49 Train Loss:  0.69 Train Acc: 66.79% Train F1:  40.04%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n",
            "(13216,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.72      1.00      0.84       620\n",
            "         1.0       0.00      0.00      0.00       240\n",
            "\n",
            "    accuracy                           0.72       860\n",
            "   macro avg       0.36      0.50      0.42       860\n",
            "weighted avg       0.52      0.72      0.60       860\n",
            "\n",
            "Epoch:  50 Train Loss:  0.69 Train Acc: 66.74% Train F1:  40.03%  Valid acc:  72.09% Valid f1: 41.89%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [240   0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntnUta6WwRvE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI6V8Knaw_2I",
        "colab_type": "text"
      },
      "source": [
        "# Deep Cnn\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTxnXykMxEHT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DPCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    DPCNN for sentences classification.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(DPCNN, self).__init__()\n",
        "\n",
        "        VOCAB_SIZE=18122\n",
        "        EMBEDDING_DIM=300\n",
        "        HIDDEN_LAYER=64\n",
        "        OUTPUT_DIM=1\n",
        "        MAX_LEN=18122\n",
        "        OUTPUT_CLASSES=1\n",
        "        MAX_SENT_LEN=50\n",
        "        window_size=1\n",
        "\n",
        "        \n",
        "        self.EMBEDDING_DIM=EMBEDDING_DIM\n",
        "        self.MAX_SENT_LEN=MAX_SENT_LEN\n",
        "        self.embedding = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM)\n",
        "      \n",
        "       # self.embedding.weight = nn.Parameter(embedding)\n",
        "       # self.embedding.weight.requires_grad = update_embedding\n",
        "        self.dropout = nn.Dropout(p=0.1) #  dropout layer\n",
        "\n",
        "\n",
        "        KERNEL_SIZES = [1,3]\n",
        "        self.NUM_KERNEL = 32\n",
        "        self.KERNEL_COUNT=len(KERNEL_SIZES)\n",
        "        #self.conv_region_embedding = nn.Conv2d(1, self.num_filter, (filter_size, EMBEDDING_DIM), stride=2)\n",
        "        self.conv1 = nn.Conv1d(in_channels=300, out_channels=64,  kernel_size=(KERNEL_SIZES[1]), stride=1 ) #conv for 1 kernel size\n",
        "        \n",
        "\n",
        "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=32,  kernel_size=(KERNEL_SIZES[0]), stride=1 ) #conv for 1 kernel size\n",
        "\n",
        "       \n",
        "        \n",
        "\n",
        "        #self.maxpool = nn.MaxPool1d(kernel_size=5)\n",
        "        self.fully_connected = nn.Linear(32, OUTPUT_CLASSES)\n",
        "        self.rel = nn.ReLU()\n",
        "        self.sig = nn.Sigmoid()\n",
        "        self.soft=nn.Softmax()\n",
        "\n",
        "    def forward(self, x, lengths=None):\n",
        "        x = self.embedding(x)\n",
        "        #print(x.shape)\n",
        "        #x = x.unsqueeze(1)\n",
        "        x = self.dropout(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        \n",
        "\n",
        "        #x=x.squeeze(1) # After embedding, i must remove 1st dimension every time ! \n",
        "        #Now x becomes 32,50,300\n",
        "        #print(x.shape) # torch.Size([32, 1, 50, 300])\n",
        "        x = self.rel(self.conv1(x))\n",
        "        #print('conv_end')\n",
        "        #print(x.shape)  # ([32, 32, 48, 1]) if kernel size =3\n",
        "                         # ([32, 32, 50, 1])  if kernel size = 1 no padding.\n",
        "\n",
        "\n",
        "        #x=x.squeeze(3)\n",
        "        \n",
        "        # x becomes ([32, 32, 50])  if kernel size = 1 no padding.\n",
        "        x = F.max_pool1d(x, x.shape[2]) \n",
        "\n",
        "        # x becomes ([32, 32, 1])  \n",
        "        #print('pooling end')\n",
        "        #print(x.shape)\n",
        "\n",
        "        \n",
        "        x = self.rel(self.conv2(x))\n",
        "        #print('here')\n",
        "#        x = self.rel(self.conv2(x)).squeeze(3) \n",
        "\n",
        "        x = F.max_pool1d(x, x.shape[2]) \n",
        "\n",
        "        #x = x.squeeze(2)\n",
        "\n",
        "\n",
        "        x# = F.max_pool1d(x, x.shape[2]) \n",
        "\n",
        "        #x = x.squeeze(2)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        x=x.squeeze(2)\n",
        "        output = self.sig(self.fully_connected(x))\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRc0E17UxHhC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f69c392e-da65-4a83-fe35-60095bbd3bb6"
      },
      "source": [
        "XTrainTens=torch.LongTensor(X_train_initial)\n",
        "XtestTens=torch.LongTensor(X_test_initial)\n",
        "ytestTens=torch.FloatTensor( y_test)\n",
        "y_trainTens=torch.FloatTensor(y_train)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "deepCNN_notstatic=DPCNN()\n",
        "#Non Static\n",
        "train(deepCNN_notstatic,100,0.001,embedding_matrix,True,XTrainTens, y_trainTens, XtestTens, ytestTens,device)\n",
        "#Static\n",
        "deepCNN_static=DPCNN()\n",
        "\n",
        "train(deepCNN_static,100,0.001,embedding_matrix,False,XTrainTens, y_trainTens, XtestTens, ytestTens,device)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(13216,)\n",
            "Epoch:  01 Train Loss:  0.70 Train Acc: 33.24% Train F1:  24.95%  Valid acc:  27.79% Valid f1: 22.18%\n",
            "Valid confusion matrix: \n",
            "[[  4 616]\n",
            " [  5 235]]\n",
            "(13216,)\n",
            "Epoch:  02 Train Loss:  0.69 Train Acc: 33.26% Train F1:  24.96%  Valid acc:  29.77% Valid f1: 26.24%\n",
            "Valid confusion matrix: \n",
            "[[ 34 586]\n",
            " [ 18 222]]\n",
            "(13216,)\n",
            "Epoch:  03 Train Loss:  0.69 Train Acc: 33.24% Train F1:  24.95%  Valid acc:  33.84% Valid f1: 31.62%\n",
            "Valid confusion matrix: \n",
            "[[ 68 552]\n",
            " [ 17 223]]\n",
            "(13216,)\n",
            "Epoch:  04 Train Loss:  0.69 Train Acc: 33.24% Train F1:  24.95%  Valid acc:  33.37% Valid f1: 31.07%\n",
            "Valid confusion matrix: \n",
            "[[ 65 555]\n",
            " [ 18 222]]\n",
            "(13216,)\n",
            "Epoch:  05 Train Loss:  0.69 Train Acc: 33.22% Train F1:  24.93%  Valid acc:  32.67% Valid f1: 29.77%\n",
            "Valid confusion matrix: \n",
            "[[ 53 567]\n",
            " [ 12 228]]\n",
            "(13216,)\n",
            "Epoch:  06 Train Loss:  0.69 Train Acc: 33.25% Train F1:  24.95%  Valid acc:  28.14% Valid f1: 22.18%\n",
            "Valid confusion matrix: \n",
            "[[  2 618]\n",
            " [  0 240]]\n",
            "(13216,)\n",
            "Epoch:  07 Train Loss:  0.66 Train Acc: 33.22% Train F1:  24.93%  Valid acc:  40.23% Valid f1: 39.13%\n",
            "Valid confusion matrix: \n",
            "[[115 505]\n",
            " [  9 231]]\n",
            "(13216,)\n",
            "Epoch:  08 Train Loss:  0.64 Train Acc: 33.23% Train F1:  24.94%  Valid acc:  60.93% Valid f1: 60.18%\n",
            "Valid confusion matrix: \n",
            "[[321 299]\n",
            " [ 37 203]]\n",
            "(13216,)\n",
            "Epoch:  09 Train Loss:  0.63 Train Acc: 33.34% Train F1:  25.08%  Valid acc:  69.19% Valid f1: 67.09%\n",
            "Valid confusion matrix: \n",
            "[[406 214]\n",
            " [ 51 189]]\n",
            "(13216,)\n",
            "Epoch:  10 Train Loss:  0.62 Train Acc: 33.72% Train F1:  25.84%  Valid acc:  61.63% Valid f1: 60.84%\n",
            "Valid confusion matrix: \n",
            "[[326 294]\n",
            " [ 36 204]]\n",
            "(13216,)\n",
            "Epoch:  11 Train Loss:  0.62 Train Acc: 34.33% Train F1:  26.88%  Valid acc:  61.98% Valid f1: 61.26%\n",
            "Valid confusion matrix: \n",
            "[[325 295]\n",
            " [ 32 208]]\n",
            "(13216,)\n",
            "Epoch:  12 Train Loss:  0.61 Train Acc: 35.94% Train F1:  29.60%  Valid acc:  61.40% Valid f1: 60.87%\n",
            "Valid confusion matrix: \n",
            "[[314 306]\n",
            " [ 26 214]]\n",
            "(13216,)\n",
            "Epoch:  13 Train Loss:  0.61 Train Acc: 36.59% Train F1:  30.64%  Valid acc:  68.95% Valid f1: 67.38%\n",
            "Valid confusion matrix: \n",
            "[[391 229]\n",
            " [ 38 202]]\n",
            "(13216,)\n",
            "Epoch:  14 Train Loss:  0.61 Train Acc: 40.42% Train F1:  36.52%  Valid acc:  72.67% Valid f1: 69.66%\n",
            "Valid confusion matrix: \n",
            "[[448 172]\n",
            " [ 63 177]]\n",
            "(13216,)\n",
            "Epoch:  15 Train Loss:  0.61 Train Acc: 41.68% Train F1:  38.42%  Valid acc:  71.63% Valid f1: 69.01%\n",
            "Valid confusion matrix: \n",
            "[[433 187]\n",
            " [ 57 183]]\n",
            "(13216,)\n",
            "Epoch:  16 Train Loss:  0.60 Train Acc: 43.64% Train F1:  41.08%  Valid acc:  72.33% Valid f1: 69.68%\n",
            "Valid confusion matrix: \n",
            "[[438 182]\n",
            " [ 56 184]]\n",
            "(13216,)\n",
            "Epoch:  17 Train Loss:  0.60 Train Acc: 48.40% Train F1:  47.21%  Valid acc:  72.33% Valid f1: 69.45%\n",
            "Valid confusion matrix: \n",
            "[[443 177]\n",
            " [ 61 179]]\n",
            "(13216,)\n",
            "Epoch:  18 Train Loss:  0.60 Train Acc: 50.42% Train F1:  49.62%  Valid acc:  74.19% Valid f1: 71.22%\n",
            "Valid confusion matrix: \n",
            "[[457 163]\n",
            " [ 59 181]]\n",
            "(13216,)\n",
            "Epoch:  19 Train Loss:  0.60 Train Acc: 51.10% Train F1:  50.39%  Valid acc:  75.12% Valid f1: 71.67%\n",
            "Valid confusion matrix: \n",
            "[[473 147]\n",
            " [ 67 173]]\n",
            "(13216,)\n",
            "Epoch:  20 Train Loss:  0.60 Train Acc: 54.01% Train F1:  53.70%  Valid acc:  74.19% Valid f1: 70.77%\n",
            "Valid confusion matrix: \n",
            "[[466 154]\n",
            " [ 68 172]]\n",
            "(13216,)\n",
            "Epoch:  21 Train Loss:  0.60 Train Acc: 58.25% Train F1:  58.20%  Valid acc:  77.67% Valid f1: 73.60%\n",
            "Valid confusion matrix: \n",
            "[[503 117]\n",
            " [ 75 165]]\n",
            "(13216,)\n",
            "Epoch:  22 Train Loss:  0.60 Train Acc: 61.80% Train F1:  61.80%  Valid acc:  75.23% Valid f1: 72.03%\n",
            "Valid confusion matrix: \n",
            "[[469 151]\n",
            " [ 62 178]]\n",
            "(13216,)\n",
            "Epoch:  23 Train Loss:  0.60 Train Acc: 64.01% Train F1:  63.97%  Valid acc:  75.81% Valid f1: 71.81%\n",
            "Valid confusion matrix: \n",
            "[[488 132]\n",
            " [ 76 164]]\n",
            "(13216,)\n",
            "Epoch:  24 Train Loss:  0.60 Train Acc: 66.60% Train F1:  66.46%  Valid acc:  77.67% Valid f1: 73.12%\n",
            "Valid confusion matrix: \n",
            "[[511 109]\n",
            " [ 83 157]]\n",
            "(13216,)\n",
            "Epoch:  25 Train Loss:  0.60 Train Acc: 68.02% Train F1:  67.82%  Valid acc:  75.81% Valid f1: 72.51%\n",
            "Valid confusion matrix: \n",
            "[[475 145]\n",
            " [ 63 177]]\n",
            "(13216,)\n",
            "Epoch:  26 Train Loss:  0.60 Train Acc: 72.28% Train F1:  71.82%  Valid acc:  79.19% Valid f1: 74.49%\n",
            "Valid confusion matrix: \n",
            "[[525  95]\n",
            " [ 84 156]]\n",
            "(13216,)\n",
            "Epoch:  27 Train Loss:  0.60 Train Acc: 70.57% Train F1:  70.21%  Valid acc:  75.47% Valid f1: 71.72%\n",
            "Valid confusion matrix: \n",
            "[[481 139]\n",
            " [ 72 168]]\n",
            "(13216,)\n",
            "Epoch:  28 Train Loss:  0.60 Train Acc: 70.92% Train F1:  70.50%  Valid acc:  71.74% Valid f1: 69.16%\n",
            "Valid confusion matrix: \n",
            "[[433 187]\n",
            " [ 56 184]]\n",
            "(13216,)\n",
            "Epoch:  29 Train Loss:  0.60 Train Acc: 71.55% Train F1:  71.14%  Valid acc:  80.70% Valid f1: 75.50%\n",
            "Valid confusion matrix: \n",
            "[[545  75]\n",
            " [ 91 149]]\n",
            "(13216,)\n",
            "Epoch:  30 Train Loss:  0.60 Train Acc: 73.14% Train F1:  72.63%  Valid acc:  78.14% Valid f1: 73.98%\n",
            "Valid confusion matrix: \n",
            "[[508 112]\n",
            " [ 76 164]]\n",
            "(13216,)\n",
            "Epoch:  31 Train Loss:  0.60 Train Acc: 74.77% Train F1:  74.15%  Valid acc:  75.81% Valid f1: 72.20%\n",
            "Valid confusion matrix: \n",
            "[[481 139]\n",
            " [ 69 171]]\n",
            "(13216,)\n",
            "Epoch:  32 Train Loss:  0.60 Train Acc: 75.70% Train F1:  75.01%  Valid acc:  75.93% Valid f1: 72.31%\n",
            "Valid confusion matrix: \n",
            "[[482 138]\n",
            " [ 69 171]]\n",
            "(13216,)\n",
            "Epoch:  33 Train Loss:  0.59 Train Acc: 78.98% Train F1:  78.09%  Valid acc:  79.07% Valid f1: 73.29%\n",
            "Valid confusion matrix: \n",
            "[[540  80]\n",
            " [100 140]]\n",
            "(13216,)\n",
            "Epoch:  34 Train Loss:  0.60 Train Acc: 78.14% Train F1:  77.27%  Valid acc:  78.37% Valid f1: 73.90%\n",
            "Valid confusion matrix: \n",
            "[[515 105]\n",
            " [ 81 159]]\n",
            "(13216,)\n",
            "Epoch:  35 Train Loss:  0.59 Train Acc: 79.68% Train F1:  78.76%  Valid acc:  78.60% Valid f1: 74.00%\n",
            "Valid confusion matrix: \n",
            "[[519 101]\n",
            " [ 83 157]]\n",
            "(13216,)\n",
            "Epoch:  36 Train Loss:  0.59 Train Acc: 79.44% Train F1:  78.50%  Valid acc:  79.42% Valid f1: 74.65%\n",
            "Valid confusion matrix: \n",
            "[[528  92]\n",
            " [ 85 155]]\n",
            "(13216,)\n",
            "Epoch:  37 Train Loss:  0.59 Train Acc: 80.03% Train F1:  79.06%  Valid acc:  79.53% Valid f1: 74.70%\n",
            "Valid confusion matrix: \n",
            "[[530  90]\n",
            " [ 86 154]]\n",
            "(13216,)\n",
            "Epoch:  38 Train Loss:  0.59 Train Acc: 78.84% Train F1:  77.96%  Valid acc:  77.21% Valid f1: 73.16%\n",
            "Valid confusion matrix: \n",
            "[[499 121]\n",
            " [ 75 165]]\n",
            "(13216,)\n",
            "Epoch:  39 Train Loss:  0.59 Train Acc: 81.32% Train F1:  80.28%  Valid acc:  78.02% Valid f1: 74.09%\n",
            "Valid confusion matrix: \n",
            "[[503 117]\n",
            " [ 72 168]]\n",
            "(13216,)\n",
            "Epoch:  40 Train Loss:  0.59 Train Acc: 82.92% Train F1:  81.82%  Valid acc:  80.35% Valid f1: 75.23%\n",
            "Valid confusion matrix: \n",
            "[[541  79]\n",
            " [ 90 150]]\n",
            "(13216,)\n",
            "Epoch:  41 Train Loss:  0.59 Train Acc: 86.14% Train F1:  84.95%  Valid acc:  80.12% Valid f1: 75.32%\n",
            "Valid confusion matrix: \n",
            "[[534  86]\n",
            " [ 85 155]]\n",
            "(13216,)\n",
            "Epoch:  42 Train Loss:  0.59 Train Acc: 86.34% Train F1:  85.13%  Valid acc:  77.21% Valid f1: 72.99%\n",
            "Valid confusion matrix: \n",
            "[[502 118]\n",
            " [ 78 162]]\n",
            "(13216,)\n",
            "Epoch:  43 Train Loss:  0.59 Train Acc: 86.71% Train F1:  85.51%  Valid acc:  77.91% Valid f1: 73.46%\n",
            "Valid confusion matrix: \n",
            "[[511 109]\n",
            " [ 81 159]]\n",
            "(13216,)\n",
            "Epoch:  44 Train Loss:  0.59 Train Acc: 86.09% Train F1:  84.88%  Valid acc:  78.60% Valid f1: 73.81%\n",
            "Valid confusion matrix: \n",
            "[[522  98]\n",
            " [ 86 154]]\n",
            "(13216,)\n",
            "Epoch:  45 Train Loss:  0.59 Train Acc: 86.06% Train F1:  84.86%  Valid acc:  78.60% Valid f1: 73.28%\n",
            "Valid confusion matrix: \n",
            "[[530  90]\n",
            " [ 94 146]]\n",
            "(13216,)\n",
            "Epoch:  46 Train Loss:  0.59 Train Acc: 84.93% Train F1:  83.78%  Valid acc:  77.67% Valid f1: 73.24%\n",
            "Valid confusion matrix: \n",
            "[[509 111]\n",
            " [ 81 159]]\n",
            "(13216,)\n",
            "Epoch:  47 Train Loss:  0.59 Train Acc: 85.99% Train F1:  84.79%  Valid acc:  78.49% Valid f1: 73.70%\n",
            "Valid confusion matrix: \n",
            "[[521  99]\n",
            " [ 86 154]]\n",
            "(13216,)\n",
            "Epoch:  48 Train Loss:  0.59 Train Acc: 86.58% Train F1:  85.37%  Valid acc:  79.77% Valid f1: 75.17%\n",
            "Valid confusion matrix: \n",
            "[[528  92]\n",
            " [ 82 158]]\n",
            "(13216,)\n",
            "Epoch:  49 Train Loss:  0.59 Train Acc: 87.95% Train F1:  86.75%  Valid acc:  78.84% Valid f1: 73.64%\n",
            "Valid confusion matrix: \n",
            "[[530  90]\n",
            " [ 92 148]]\n",
            "(13216,)\n",
            "Epoch:  50 Train Loss:  0.59 Train Acc: 87.14% Train F1:  85.96%  Valid acc:  80.58% Valid f1: 75.39%\n",
            "Valid confusion matrix: \n",
            "[[544  76]\n",
            " [ 91 149]]\n",
            "(13216,)\n",
            "Epoch:  51 Train Loss:  0.59 Train Acc: 89.57% Train F1:  88.39%  Valid acc:  79.77% Valid f1: 74.99%\n",
            "Valid confusion matrix: \n",
            "[[531  89]\n",
            " [ 85 155]]\n",
            "(13216,)\n",
            "Epoch:  52 Train Loss:  0.59 Train Acc: 89.44% Train F1:  88.26%  Valid acc:  79.42% Valid f1: 75.25%\n",
            "Valid confusion matrix: \n",
            "[[518 102]\n",
            " [ 75 165]]\n",
            "(13216,)\n",
            "Epoch:  53 Train Loss:  0.59 Train Acc: 89.24% Train F1:  88.06%  Valid acc:  78.37% Valid f1: 74.02%\n",
            "Valid confusion matrix: \n",
            "[[513 107]\n",
            " [ 79 161]]\n",
            "(13216,)\n",
            "Epoch:  54 Train Loss:  0.59 Train Acc: 89.67% Train F1:  88.48%  Valid acc:  81.63% Valid f1: 75.58%\n",
            "Valid confusion matrix: \n",
            "[[565  55]\n",
            " [103 137]]\n",
            "(13216,)\n",
            "Epoch:  55 Train Loss:  0.59 Train Acc: 90.35% Train F1:  89.19%  Valid acc:  77.44% Valid f1: 72.90%\n",
            "Valid confusion matrix: \n",
            "[[509 111]\n",
            " [ 83 157]]\n",
            "(13216,)\n",
            "Epoch:  56 Train Loss:  0.59 Train Acc: 90.19% Train F1:  89.02%  Valid acc:  80.58% Valid f1: 74.76%\n",
            "Valid confusion matrix: \n",
            "[[553  67]\n",
            " [100 140]]\n",
            "(13216,)\n",
            "Epoch:  57 Train Loss:  0.59 Train Acc: 91.50% Train F1:  90.39%  Valid acc:  80.23% Valid f1: 74.12%\n",
            "Valid confusion matrix: \n",
            "[[554  66]\n",
            " [104 136]]\n",
            "(13216,)\n",
            "Epoch:  58 Train Loss:  0.59 Train Acc: 90.87% Train F1:  89.74%  Valid acc:  80.58% Valid f1: 75.25%\n",
            "Valid confusion matrix: \n",
            "[[546  74]\n",
            " [ 93 147]]\n",
            "(13216,)\n",
            "Epoch:  59 Train Loss:  0.59 Train Acc: 89.96% Train F1:  88.80%  Valid acc:  80.81% Valid f1: 75.42%\n",
            "Valid confusion matrix: \n",
            "[[549  71]\n",
            " [ 94 146]]\n",
            "(13216,)\n",
            "Epoch:  60 Train Loss:  0.59 Train Acc: 90.42% Train F1:  89.27%  Valid acc:  80.23% Valid f1: 73.88%\n",
            "Valid confusion matrix: \n",
            "[[557  63]\n",
            " [107 133]]\n",
            "(13216,)\n",
            "Epoch:  61 Train Loss:  0.59 Train Acc: 90.23% Train F1:  89.06%  Valid acc:  80.58% Valid f1: 73.39%\n",
            "Valid confusion matrix: \n",
            "[[570  50]\n",
            " [117 123]]\n",
            "(13216,)\n",
            "Epoch:  62 Train Loss:  0.59 Train Acc: 90.78% Train F1:  89.64%  Valid acc:  80.23% Valid f1: 74.98%\n",
            "Valid confusion matrix: \n",
            "[[542  78]\n",
            " [ 92 148]]\n",
            "(13216,)\n",
            "Epoch:  63 Train Loss:  0.59 Train Acc: 91.68% Train F1:  90.58%  Valid acc:  79.88% Valid f1: 74.97%\n",
            "Valid confusion matrix: \n",
            "[[534  86]\n",
            " [ 87 153]]\n",
            "(13216,)\n",
            "Epoch:  64 Train Loss:  0.59 Train Acc: 91.73% Train F1:  90.64%  Valid acc:  80.35% Valid f1: 74.61%\n",
            "Valid confusion matrix: \n",
            "[[550  70]\n",
            " [ 99 141]]\n",
            "(13216,)\n",
            "Epoch:  65 Train Loss:  0.59 Train Acc: 92.42% Train F1:  91.37%  Valid acc:  80.00% Valid f1: 74.75%\n",
            "Valid confusion matrix: \n",
            "[[540  80]\n",
            " [ 92 148]]\n",
            "(13216,)\n",
            "Epoch:  66 Train Loss:  0.59 Train Acc: 90.53% Train F1:  89.39%  Valid acc:  77.33% Valid f1: 72.48%\n",
            "Valid confusion matrix: \n",
            "[[513 107]\n",
            " [ 88 152]]\n",
            "(13216,)\n",
            "Epoch:  67 Train Loss:  0.59 Train Acc: 91.95% Train F1:  90.88%  Valid acc:  80.00% Valid f1: 74.34%\n",
            "Valid confusion matrix: \n",
            "[[546  74]\n",
            " [ 98 142]]\n",
            "(13216,)\n",
            "Epoch:  68 Train Loss:  0.59 Train Acc: 91.98% Train F1:  90.90%  Valid acc:  78.26% Valid f1: 73.79%\n",
            "Valid confusion matrix: \n",
            "[[514 106]\n",
            " [ 81 159]]\n",
            "(13216,)\n",
            "Epoch:  69 Train Loss:  0.59 Train Acc: 92.77% Train F1:  91.75%  Valid acc:  81.63% Valid f1: 76.22%\n",
            "Valid confusion matrix: \n",
            "[[556  64]\n",
            " [ 94 146]]\n",
            "(13216,)\n",
            "Epoch:  70 Train Loss:  0.59 Train Acc: 91.90% Train F1:  90.82%  Valid acc:  81.05% Valid f1: 74.85%\n",
            "Valid confusion matrix: \n",
            "[[562  58]\n",
            " [105 135]]\n",
            "(13216,)\n",
            "Epoch:  71 Train Loss:  0.59 Train Acc: 91.65% Train F1:  90.58%  Valid acc:  79.88% Valid f1: 74.44%\n",
            "Valid confusion matrix: \n",
            "[[542  78]\n",
            " [ 95 145]]\n",
            "(13216,)\n",
            "Epoch:  72 Train Loss:  0.59 Train Acc: 92.52% Train F1:  91.48%  Valid acc:  80.70% Valid f1: 74.95%\n",
            "Valid confusion matrix: \n",
            "[[553  67]\n",
            " [ 99 141]]\n",
            "(13216,)\n",
            "Epoch:  73 Train Loss:  0.59 Train Acc: 93.23% Train F1:  92.24%  Valid acc:  81.05% Valid f1: 75.07%\n",
            "Valid confusion matrix: \n",
            "[[559  61]\n",
            " [102 138]]\n",
            "(13216,)\n",
            "Epoch:  74 Train Loss:  0.59 Train Acc: 92.89% Train F1:  91.87%  Valid acc:  80.00% Valid f1: 74.55%\n",
            "Valid confusion matrix: \n",
            "[[543  77]\n",
            " [ 95 145]]\n",
            "(13216,)\n",
            "Epoch:  75 Train Loss:  0.59 Train Acc: 92.61% Train F1:  91.58%  Valid acc:  81.74% Valid f1: 76.13%\n",
            "Valid confusion matrix: \n",
            "[[560  60]\n",
            " [ 97 143]]\n",
            "(13216,)\n",
            "Epoch:  76 Train Loss:  0.59 Train Acc: 92.89% Train F1:  91.89%  Valid acc:  79.77% Valid f1: 74.39%\n",
            "Valid confusion matrix: \n",
            "[[540  80]\n",
            " [ 94 146]]\n",
            "(13216,)\n",
            "Epoch:  77 Train Loss:  0.59 Train Acc: 92.74% Train F1:  91.73%  Valid acc:  79.65% Valid f1: 74.87%\n",
            "Valid confusion matrix: \n",
            "[[530  90]\n",
            " [ 85 155]]\n",
            "(13216,)\n",
            "Epoch:  78 Train Loss:  0.59 Train Acc: 92.99% Train F1:  91.98%  Valid acc:  79.77% Valid f1: 74.53%\n",
            "Valid confusion matrix: \n",
            "[[538  82]\n",
            " [ 92 148]]\n",
            "(13216,)\n",
            "Epoch:  79 Train Loss:  0.59 Train Acc: 93.28% Train F1:  92.30%  Valid acc:  80.47% Valid f1: 72.54%\n",
            "Valid confusion matrix: \n",
            "[[577  43]\n",
            " [125 115]]\n",
            "(13216,)\n",
            "Epoch:  80 Train Loss:  0.59 Train Acc: 92.85% Train F1:  91.84%  Valid acc:  80.93% Valid f1: 75.46%\n",
            "Valid confusion matrix: \n",
            "[[551  69]\n",
            " [ 95 145]]\n",
            "(13216,)\n",
            "Epoch:  81 Train Loss:  0.59 Train Acc: 92.67% Train F1:  91.65%  Valid acc:  80.47% Valid f1: 74.57%\n",
            "Valid confusion matrix: \n",
            "[[553  67]\n",
            " [101 139]]\n",
            "(13216,)\n",
            "Epoch:  82 Train Loss:  0.59 Train Acc: 93.42% Train F1:  92.46%  Valid acc:  79.30% Valid f1: 74.15%\n",
            "Valid confusion matrix: \n",
            "[[533  87]\n",
            " [ 91 149]]\n",
            "(13216,)\n",
            "Epoch:  83 Train Loss:  0.59 Train Acc: 92.30% Train F1:  91.25%  Valid acc:  81.05% Valid f1: 74.85%\n",
            "Valid confusion matrix: \n",
            "[[562  58]\n",
            " [105 135]]\n",
            "(13216,)\n",
            "Epoch:  84 Train Loss:  0.59 Train Acc: 93.13% Train F1:  92.14%  Valid acc:  80.70% Valid f1: 75.23%\n",
            "Valid confusion matrix: \n",
            "[[549  71]\n",
            " [ 95 145]]\n",
            "(13216,)\n",
            "Epoch:  85 Train Loss:  0.59 Train Acc: 92.48% Train F1:  91.44%  Valid acc:  79.65% Valid f1: 74.55%\n",
            "Valid confusion matrix: \n",
            "[[535  85]\n",
            " [ 90 150]]\n",
            "(13216,)\n",
            "Epoch:  86 Train Loss:  0.59 Train Acc: 93.30% Train F1:  92.33%  Valid acc:  80.58% Valid f1: 75.05%\n",
            "Valid confusion matrix: \n",
            "[[549  71]\n",
            " [ 96 144]]\n",
            "(13216,)\n",
            "Epoch:  87 Train Loss:  0.59 Train Acc: 93.39% Train F1:  92.42%  Valid acc:  81.16% Valid f1: 75.62%\n",
            "Valid confusion matrix: \n",
            "[[554  66]\n",
            " [ 96 144]]\n",
            "(13216,)\n",
            "Epoch:  88 Train Loss:  0.59 Train Acc: 93.08% Train F1:  92.08%  Valid acc:  80.93% Valid f1: 75.93%\n",
            "Valid confusion matrix: \n",
            "[[544  76]\n",
            " [ 88 152]]\n",
            "(13216,)\n",
            "Epoch:  89 Train Loss:  0.59 Train Acc: 92.35% Train F1:  91.31%  Valid acc:  81.40% Valid f1: 75.78%\n",
            "Valid confusion matrix: \n",
            "[[557  63]\n",
            " [ 97 143]]\n",
            "(13216,)\n",
            "Epoch:  90 Train Loss:  0.59 Train Acc: 93.01% Train F1:  92.01%  Valid acc:  80.93% Valid f1: 75.11%\n",
            "Valid confusion matrix: \n",
            "[[556  64]\n",
            " [100 140]]\n",
            "(13216,)\n",
            "Epoch:  91 Train Loss:  0.59 Train Acc: 93.80% Train F1:  92.87%  Valid acc:  80.93% Valid f1: 75.99%\n",
            "Valid confusion matrix: \n",
            "[[543  77]\n",
            " [ 87 153]]\n",
            "(13216,)\n",
            "Epoch:  92 Train Loss:  0.59 Train Acc: 93.12% Train F1:  92.13%  Valid acc:  79.42% Valid f1: 74.46%\n",
            "Valid confusion matrix: \n",
            "[[531  89]\n",
            " [ 88 152]]\n",
            "(13216,)\n",
            "Epoch:  93 Train Loss:  0.59 Train Acc: 93.11% Train F1:  92.11%  Valid acc:  80.00% Valid f1: 75.08%\n",
            "Valid confusion matrix: \n",
            "[[535  85]\n",
            " [ 87 153]]\n",
            "(13216,)\n",
            "Epoch:  94 Train Loss:  0.59 Train Acc: 93.18% Train F1:  92.20%  Valid acc:  81.40% Valid f1: 76.06%\n",
            "Valid confusion matrix: \n",
            "[[553  67]\n",
            " [ 93 147]]\n",
            "(13216,)\n",
            "Epoch:  95 Train Loss:  0.59 Train Acc: 93.85% Train F1:  92.92%  Valid acc:  80.81% Valid f1: 75.55%\n",
            "Valid confusion matrix: \n",
            "[[547  73]\n",
            " [ 92 148]]\n",
            "(13216,)\n",
            "Epoch:  96 Train Loss:  0.59 Train Acc: 93.77% Train F1:  92.83%  Valid acc:  80.35% Valid f1: 75.16%\n",
            "Valid confusion matrix: \n",
            "[[542  78]\n",
            " [ 91 149]]\n",
            "(13216,)\n",
            "Epoch:  97 Train Loss:  0.59 Train Acc: 93.05% Train F1:  92.07%  Valid acc:  80.81% Valid f1: 75.28%\n",
            "Valid confusion matrix: \n",
            "[[551  69]\n",
            " [ 96 144]]\n",
            "(13216,)\n",
            "Epoch:  98 Train Loss:  0.59 Train Acc: 93.73% Train F1:  92.79%  Valid acc:  80.58% Valid f1: 73.99%\n",
            "Valid confusion matrix: \n",
            "[[563  57]\n",
            " [110 130]]\n",
            "(13216,)\n",
            "Epoch:  99 Train Loss:  0.59 Train Acc: 93.77% Train F1:  92.83%  Valid acc:  80.70% Valid f1: 74.65%\n",
            "Valid confusion matrix: \n",
            "[[557  63]\n",
            " [103 137]]\n",
            "(13216,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.85      0.89      0.87       620\n",
            "         1.0       0.68      0.60      0.64       240\n",
            "\n",
            "    accuracy                           0.81       860\n",
            "   macro avg       0.77      0.74      0.75       860\n",
            "weighted avg       0.80      0.81      0.81       860\n",
            "\n",
            "Epoch:  100 Train Loss:  0.59 Train Acc: 93.86% Train F1:  92.93%  Valid acc:  80.93% Valid f1: 75.32%\n",
            "Valid confusion matrix: \n",
            "[[553  67]\n",
            " [ 97 143]]\n",
            "(13216,)\n",
            "Epoch:  01 Train Loss:  0.70 Train Acc: 33.24% Train F1:  24.95%  Valid acc:  30.00% Valid f1: 25.83%\n",
            "Valid confusion matrix: \n",
            "[[ 27 593]\n",
            " [  9 231]]\n",
            "(13216,)\n",
            "Epoch:  02 Train Loss:  0.69 Train Acc: 33.26% Train F1:  24.96%  Valid acc:  35.47% Valid f1: 34.07%\n",
            "Valid confusion matrix: \n",
            "[[ 90 530]\n",
            " [ 25 215]]\n",
            "(13216,)\n",
            "Epoch:  03 Train Loss:  0.69 Train Acc: 33.24% Train F1:  24.95%  Valid acc:  36.05% Valid f1: 34.69%\n",
            "Valid confusion matrix: \n",
            "[[ 93 527]\n",
            " [ 23 217]]\n",
            "(13216,)\n",
            "Epoch:  04 Train Loss:  0.69 Train Acc: 33.24% Train F1:  24.95%  Valid acc:  36.16% Valid f1: 35.31%\n",
            "Valid confusion matrix: \n",
            "[[106 514]\n",
            " [ 35 205]]\n",
            "(13216,)\n",
            "Epoch:  05 Train Loss:  0.69 Train Acc: 33.22% Train F1:  24.93%  Valid acc:  34.65% Valid f1: 32.87%\n",
            "Valid confusion matrix: \n",
            "[[ 79 541]\n",
            " [ 21 219]]\n",
            "(13216,)\n",
            "Epoch:  06 Train Loss:  0.69 Train Acc: 35.26% Train F1:  29.92%  Valid acc:  72.79% Valid f1: 49.51%\n",
            "Valid confusion matrix: \n",
            "[[605  15]\n",
            " [219  21]]\n",
            "(13216,)\n",
            "Epoch:  07 Train Loss:  0.68 Train Acc: 41.64% Train F1:  40.74%  Valid acc:  39.30% Valid f1: 38.18%\n",
            "Valid confusion matrix: \n",
            "[[111 509]\n",
            " [ 13 227]]\n",
            "(13216,)\n",
            "Epoch:  08 Train Loss:  0.65 Train Acc: 33.52% Train F1:  25.50%  Valid acc:  60.23% Valid f1: 59.57%\n",
            "Valid confusion matrix: \n",
            "[[314 306]\n",
            " [ 36 204]]\n",
            "(13216,)\n",
            "Epoch:  09 Train Loss:  0.64 Train Acc: 33.73% Train F1:  25.80%  Valid acc:  67.44% Valid f1: 65.73%\n",
            "Valid confusion matrix: \n",
            "[[386 234]\n",
            " [ 46 194]]\n",
            "(13216,)\n",
            "Epoch:  10 Train Loss:  0.64 Train Acc: 34.15% Train F1:  26.54%  Valid acc:  67.09% Valid f1: 65.60%\n",
            "Valid confusion matrix: \n",
            "[[378 242]\n",
            " [ 41 199]]\n",
            "(13216,)\n",
            "Epoch:  11 Train Loss:  0.63 Train Acc: 34.26% Train F1:  26.77%  Valid acc:  61.63% Valid f1: 60.87%\n",
            "Valid confusion matrix: \n",
            "[[325 295]\n",
            " [ 35 205]]\n",
            "(13216,)\n",
            "Epoch:  12 Train Loss:  0.62 Train Acc: 35.10% Train F1:  28.36%  Valid acc:  61.63% Valid f1: 60.81%\n",
            "Valid confusion matrix: \n",
            "[[327 293]\n",
            " [ 37 203]]\n",
            "(13216,)\n",
            "Epoch:  13 Train Loss:  0.62 Train Acc: 35.13% Train F1:  28.29%  Valid acc:  65.00% Valid f1: 64.00%\n",
            "Valid confusion matrix: \n",
            "[[351 269]\n",
            " [ 32 208]]\n",
            "(13216,)\n",
            "Epoch:  14 Train Loss:  0.61 Train Acc: 34.74% Train F1:  27.61%  Valid acc:  72.33% Valid f1: 69.54%\n",
            "Valid confusion matrix: \n",
            "[[441 179]\n",
            " [ 59 181]]\n",
            "(13216,)\n",
            "Epoch:  15 Train Loss:  0.61 Train Acc: 35.14% Train F1:  28.41%  Valid acc:  61.74% Valid f1: 61.02%\n",
            "Valid confusion matrix: \n",
            "[[324 296]\n",
            " [ 33 207]]\n",
            "(13216,)\n",
            "Epoch:  16 Train Loss:  0.61 Train Acc: 35.60% Train F1:  29.12%  Valid acc:  71.98% Valid f1: 69.67%\n",
            "Valid confusion matrix: \n",
            "[[428 192]\n",
            " [ 49 191]]\n",
            "(13216,)\n",
            "Epoch:  17 Train Loss:  0.61 Train Acc: 37.73% Train F1:  32.52%  Valid acc:  76.86% Valid f1: 73.48%\n",
            "Valid confusion matrix: \n",
            "[[484 136]\n",
            " [ 63 177]]\n",
            "(13216,)\n",
            "Epoch:  18 Train Loss:  0.61 Train Acc: 39.10% Train F1:  34.72%  Valid acc:  68.26% Valid f1: 66.42%\n",
            "Valid confusion matrix: \n",
            "[[394 226]\n",
            " [ 47 193]]\n",
            "(13216,)\n",
            "Epoch:  19 Train Loss:  0.60 Train Acc: 40.66% Train F1:  37.04%  Valid acc:  77.44% Valid f1: 74.17%\n",
            "Valid confusion matrix: \n",
            "[[486 134]\n",
            " [ 60 180]]\n",
            "(13216,)\n",
            "Epoch:  20 Train Loss:  0.60 Train Acc: 41.52% Train F1:  38.19%  Valid acc:  72.09% Valid f1: 69.74%\n",
            "Valid confusion matrix: \n",
            "[[430 190]\n",
            " [ 50 190]]\n",
            "(13216,)\n",
            "Epoch:  21 Train Loss:  0.60 Train Acc: 42.93% Train F1:  40.16%  Valid acc:  72.33% Valid f1: 70.39%\n",
            "Valid confusion matrix: \n",
            "[[421 199]\n",
            " [ 39 201]]\n",
            "(13216,)\n",
            "Epoch:  22 Train Loss:  0.60 Train Acc: 44.51% Train F1:  42.32%  Valid acc:  71.28% Valid f1: 69.29%\n",
            "Valid confusion matrix: \n",
            "[[416 204]\n",
            " [ 43 197]]\n",
            "(13216,)\n",
            "Epoch:  23 Train Loss:  0.60 Train Acc: 46.62% Train F1:  45.05%  Valid acc:  75.23% Valid f1: 72.18%\n",
            "Valid confusion matrix: \n",
            "[[466 154]\n",
            " [ 59 181]]\n",
            "(13216,)\n",
            "Epoch:  24 Train Loss:  0.60 Train Acc: 49.20% Train F1:  48.20%  Valid acc:  76.51% Valid f1: 73.06%\n",
            "Valid confusion matrix: \n",
            "[[483 137]\n",
            " [ 65 175]]\n",
            "(13216,)\n",
            "Epoch:  25 Train Loss:  0.60 Train Acc: 52.33% Train F1:  51.82%  Valid acc:  74.30% Valid f1: 71.78%\n",
            "Valid confusion matrix: \n",
            "[[448 172]\n",
            " [ 49 191]]\n",
            "(13216,)\n",
            "Epoch:  26 Train Loss:  0.60 Train Acc: 56.08% Train F1:  55.93%  Valid acc:  79.65% Valid f1: 75.69%\n",
            "Valid confusion matrix: \n",
            "[[516 104]\n",
            " [ 71 169]]\n",
            "(13216,)\n",
            "Epoch:  27 Train Loss:  0.60 Train Acc: 57.98% Train F1:  57.92%  Valid acc:  77.79% Valid f1: 74.69%\n",
            "Valid confusion matrix: \n",
            "[[485 135]\n",
            " [ 56 184]]\n",
            "(13216,)\n",
            "Epoch:  28 Train Loss:  0.60 Train Acc: 61.06% Train F1:  61.06%  Valid acc:  76.16% Valid f1: 72.78%\n",
            "Valid confusion matrix: \n",
            "[[479 141]\n",
            " [ 64 176]]\n",
            "(13216,)\n",
            "Epoch:  29 Train Loss:  0.60 Train Acc: 61.07% Train F1:  61.07%  Valid acc:  77.21% Valid f1: 73.91%\n",
            "Valid confusion matrix: \n",
            "[[485 135]\n",
            " [ 61 179]]\n",
            "(13216,)\n",
            "Epoch:  30 Train Loss:  0.60 Train Acc: 63.05% Train F1:  63.03%  Valid acc:  77.67% Valid f1: 74.58%\n",
            "Valid confusion matrix: \n",
            "[[484 136]\n",
            " [ 56 184]]\n",
            "(13216,)\n",
            "Epoch:  31 Train Loss:  0.60 Train Acc: 65.34% Train F1:  65.26%  Valid acc:  78.72% Valid f1: 75.38%\n",
            "Valid confusion matrix: \n",
            "[[497 123]\n",
            " [ 60 180]]\n",
            "(13216,)\n",
            "Epoch:  32 Train Loss:  0.60 Train Acc: 64.24% Train F1:  64.19%  Valid acc:  78.14% Valid f1: 74.73%\n",
            "Valid confusion matrix: \n",
            "[[494 126]\n",
            " [ 62 178]]\n",
            "(13216,)\n",
            "Epoch:  33 Train Loss:  0.60 Train Acc: 68.95% Train F1:  68.69%  Valid acc:  79.77% Valid f1: 75.05%\n",
            "Valid confusion matrix: \n",
            "[[530  90]\n",
            " [ 84 156]]\n",
            "(13216,)\n",
            "Epoch:  34 Train Loss:  0.60 Train Acc: 70.27% Train F1:  69.92%  Valid acc:  79.30% Valid f1: 75.52%\n",
            "Valid confusion matrix: \n",
            "[[510 110]\n",
            " [ 68 172]]\n",
            "(13216,)\n",
            "Epoch:  35 Train Loss:  0.60 Train Acc: 71.25% Train F1:  70.85%  Valid acc:  79.53% Valid f1: 75.36%\n",
            "Valid confusion matrix: \n",
            "[[519 101]\n",
            " [ 75 165]]\n",
            "(13216,)\n",
            "Epoch:  36 Train Loss:  0.60 Train Acc: 71.72% Train F1:  71.29%  Valid acc:  79.30% Valid f1: 75.73%\n",
            "Valid confusion matrix: \n",
            "[[506 114]\n",
            " [ 64 176]]\n",
            "(13216,)\n",
            "Epoch:  37 Train Loss:  0.60 Train Acc: 72.25% Train F1:  71.75%  Valid acc:  81.86% Valid f1: 77.63%\n",
            "Valid confusion matrix: \n",
            "[[539  81]\n",
            " [ 75 165]]\n",
            "(13216,)\n",
            "Epoch:  38 Train Loss:  0.60 Train Acc: 75.88% Train F1:  75.14%  Valid acc:  80.35% Valid f1: 75.97%\n",
            "Valid confusion matrix: \n",
            "[[529  91]\n",
            " [ 78 162]]\n",
            "(13216,)\n",
            "Epoch:  39 Train Loss:  0.60 Train Acc: 75.59% Train F1:  74.88%  Valid acc:  80.12% Valid f1: 75.63%\n",
            "Valid confusion matrix: \n",
            "[[529  91]\n",
            " [ 80 160]]\n",
            "(13216,)\n",
            "Epoch:  40 Train Loss:  0.60 Train Acc: 77.54% Train F1:  76.70%  Valid acc:  78.95% Valid f1: 75.55%\n",
            "Valid confusion matrix: \n",
            "[[500 120]\n",
            " [ 61 179]]\n",
            "(13216,)\n",
            "Epoch:  41 Train Loss:  0.60 Train Acc: 77.19% Train F1:  76.37%  Valid acc:  78.26% Valid f1: 74.47%\n",
            "Valid confusion matrix: \n",
            "[[502 118]\n",
            " [ 69 171]]\n",
            "(13216,)\n",
            "Epoch:  42 Train Loss:  0.60 Train Acc: 77.00% Train F1:  76.22%  Valid acc:  80.81% Valid f1: 76.77%\n",
            "Valid confusion matrix: \n",
            "[[527  93]\n",
            " [ 72 168]]\n",
            "(13216,)\n",
            "Epoch:  43 Train Loss:  0.60 Train Acc: 80.04% Train F1:  79.03%  Valid acc:  80.81% Valid f1: 76.19%\n",
            "Valid confusion matrix: \n",
            "[[537  83]\n",
            " [ 82 158]]\n",
            "(13216,)\n",
            "Epoch:  44 Train Loss:  0.60 Train Acc: 79.84% Train F1:  78.84%  Valid acc:  80.58% Valid f1: 76.14%\n",
            "Valid confusion matrix: \n",
            "[[532  88]\n",
            " [ 79 161]]\n",
            "(13216,)\n",
            "Epoch:  45 Train Loss:  0.60 Train Acc: 80.32% Train F1:  79.30%  Valid acc:  80.12% Valid f1: 76.20%\n",
            "Valid confusion matrix: \n",
            "[[519 101]\n",
            " [ 70 170]]\n",
            "(13216,)\n",
            "Epoch:  46 Train Loss:  0.60 Train Acc: 81.32% Train F1:  80.24%  Valid acc:  78.95% Valid f1: 74.92%\n",
            "Valid confusion matrix: \n",
            "[[512 108]\n",
            " [ 73 167]]\n",
            "(13216,)\n",
            "Epoch:  47 Train Loss:  0.60 Train Acc: 83.73% Train F1:  82.56%  Valid acc:  82.44% Valid f1: 77.31%\n",
            "Valid confusion matrix: \n",
            "[[559  61]\n",
            " [ 90 150]]\n",
            "(13216,)\n",
            "Epoch:  48 Train Loss:  0.60 Train Acc: 84.24% Train F1:  83.04%  Valid acc:  79.65% Valid f1: 75.47%\n",
            "Valid confusion matrix: \n",
            "[[520 100]\n",
            " [ 75 165]]\n",
            "(13216,)\n",
            "Epoch:  49 Train Loss:  0.60 Train Acc: 83.36% Train F1:  82.19%  Valid acc:  80.23% Valid f1: 75.74%\n",
            "Valid confusion matrix: \n",
            "[[530  90]\n",
            " [ 80 160]]\n",
            "(13216,)\n",
            "Epoch:  50 Train Loss:  0.59 Train Acc: 82.82% Train F1:  81.68%  Valid acc:  79.88% Valid f1: 75.92%\n",
            "Valid confusion matrix: \n",
            "[[518 102]\n",
            " [ 71 169]]\n",
            "(13216,)\n",
            "Epoch:  51 Train Loss:  0.60 Train Acc: 84.28% Train F1:  83.09%  Valid acc:  81.16% Valid f1: 77.05%\n",
            "Valid confusion matrix: \n",
            "[[531  89]\n",
            " [ 73 167]]\n",
            "(13216,)\n",
            "Epoch:  52 Train Loss:  0.60 Train Acc: 83.71% Train F1:  82.55%  Valid acc:  79.30% Valid f1: 75.57%\n",
            "Valid confusion matrix: \n",
            "[[509 111]\n",
            " [ 67 173]]\n",
            "(13216,)\n",
            "Epoch:  53 Train Loss:  0.60 Train Acc: 82.95% Train F1:  81.83%  Valid acc:  79.07% Valid f1: 75.08%\n",
            "Valid confusion matrix: \n",
            "[[512 108]\n",
            " [ 72 168]]\n",
            "(13216,)\n",
            "Epoch:  54 Train Loss:  0.59 Train Acc: 83.86% Train F1:  82.69%  Valid acc:  81.40% Valid f1: 76.45%\n",
            "Valid confusion matrix: \n",
            "[[547  73]\n",
            " [ 87 153]]\n",
            "(13216,)\n",
            "Epoch:  55 Train Loss:  0.59 Train Acc: 85.08% Train F1:  83.88%  Valid acc:  80.70% Valid f1: 76.65%\n",
            "Valid confusion matrix: \n",
            "[[526  94]\n",
            " [ 72 168]]\n",
            "(13216,)\n",
            "Epoch:  56 Train Loss:  0.59 Train Acc: 84.65% Train F1:  83.46%  Valid acc:  80.81% Valid f1: 76.43%\n",
            "Valid confusion matrix: \n",
            "[[533  87]\n",
            " [ 78 162]]\n",
            "(13216,)\n",
            "Epoch:  57 Train Loss:  0.59 Train Acc: 85.71% Train F1:  84.49%  Valid acc:  80.58% Valid f1: 76.37%\n",
            "Valid confusion matrix: \n",
            "[[528  92]\n",
            " [ 75 165]]\n",
            "(13216,)\n",
            "Epoch:  58 Train Loss:  0.59 Train Acc: 85.10% Train F1:  83.91%  Valid acc:  79.88% Valid f1: 75.92%\n",
            "Valid confusion matrix: \n",
            "[[518 102]\n",
            " [ 71 169]]\n",
            "(13216,)\n",
            "Epoch:  59 Train Loss:  0.59 Train Acc: 87.11% Train F1:  85.88%  Valid acc:  80.58% Valid f1: 76.26%\n",
            "Valid confusion matrix: \n",
            "[[530  90]\n",
            " [ 77 163]]\n",
            "(13216,)\n",
            "Epoch:  60 Train Loss:  0.59 Train Acc: 87.73% Train F1:  86.49%  Valid acc:  80.00% Valid f1: 75.52%\n",
            "Valid confusion matrix: \n",
            "[[528  92]\n",
            " [ 80 160]]\n",
            "(13216,)\n",
            "Epoch:  61 Train Loss:  0.59 Train Acc: 87.57% Train F1:  86.33%  Valid acc:  80.00% Valid f1: 76.03%\n",
            "Valid confusion matrix: \n",
            "[[519 101]\n",
            " [ 71 169]]\n",
            "(13216,)\n",
            "Epoch:  62 Train Loss:  0.59 Train Acc: 88.13% Train F1:  86.90%  Valid acc:  82.67% Valid f1: 77.74%\n",
            "Valid confusion matrix: \n",
            "[[558  62]\n",
            " [ 87 153]]\n",
            "(13216,)\n",
            "Epoch:  63 Train Loss:  0.59 Train Acc: 88.14% Train F1:  86.89%  Valid acc:  83.49% Valid f1: 77.34%\n",
            "Valid confusion matrix: \n",
            "[[583  37]\n",
            " [105 135]]\n",
            "(13216,)\n",
            "Epoch:  64 Train Loss:  0.59 Train Acc: 87.92% Train F1:  86.68%  Valid acc:  82.21% Valid f1: 78.14%\n",
            "Valid confusion matrix: \n",
            "[[539  81]\n",
            " [ 72 168]]\n",
            "(13216,)\n",
            "Epoch:  65 Train Loss:  0.59 Train Acc: 87.83% Train F1:  86.61%  Valid acc:  82.09% Valid f1: 77.63%\n",
            "Valid confusion matrix: \n",
            "[[545  75]\n",
            " [ 79 161]]\n",
            "(13216,)\n",
            "Epoch:  66 Train Loss:  0.59 Train Acc: 89.75% Train F1:  88.54%  Valid acc:  82.33% Valid f1: 77.69%\n",
            "Valid confusion matrix: \n",
            "[[550  70]\n",
            " [ 82 158]]\n",
            "(13216,)\n",
            "Epoch:  67 Train Loss:  0.59 Train Acc: 88.70% Train F1:  87.48%  Valid acc:  81.16% Valid f1: 76.41%\n",
            "Valid confusion matrix: \n",
            "[[542  78]\n",
            " [ 84 156]]\n",
            "(13216,)\n",
            "Epoch:  68 Train Loss:  0.59 Train Acc: 89.67% Train F1:  88.47%  Valid acc:  82.33% Valid f1: 77.87%\n",
            "Valid confusion matrix: \n",
            "[[547  73]\n",
            " [ 79 161]]\n",
            "(13216,)\n",
            "Epoch:  69 Train Loss:  0.59 Train Acc: 88.81% Train F1:  87.59%  Valid acc:  82.21% Valid f1: 77.45%\n",
            "Valid confusion matrix: \n",
            "[[551  69]\n",
            " [ 84 156]]\n",
            "(13216,)\n",
            "Epoch:  70 Train Loss:  0.59 Train Acc: 90.94% Train F1:  89.75%  Valid acc:  82.33% Valid f1: 77.45%\n",
            "Valid confusion matrix: \n",
            "[[554  66]\n",
            " [ 86 154]]\n",
            "(13216,)\n",
            "Epoch:  71 Train Loss:  0.59 Train Acc: 89.66% Train F1:  88.46%  Valid acc:  81.74% Valid f1: 77.34%\n",
            "Valid confusion matrix: \n",
            "[[541  79]\n",
            " [ 78 162]]\n",
            "(13216,)\n",
            "Epoch:  72 Train Loss:  0.59 Train Acc: 89.63% Train F1:  88.42%  Valid acc:  82.79% Valid f1: 78.28%\n",
            "Valid confusion matrix: \n",
            "[[552  68]\n",
            " [ 80 160]]\n",
            "(13216,)\n",
            "Epoch:  73 Train Loss:  0.59 Train Acc: 88.36% Train F1:  87.13%  Valid acc:  82.91% Valid f1: 77.65%\n",
            "Valid confusion matrix: \n",
            "[[565  55]\n",
            " [ 92 148]]\n",
            "(13216,)\n",
            "Epoch:  74 Train Loss:  0.59 Train Acc: 91.58% Train F1:  90.44%  Valid acc:  82.09% Valid f1: 76.55%\n",
            "Valid confusion matrix: \n",
            "[[562  58]\n",
            " [ 96 144]]\n",
            "(13216,)\n",
            "Epoch:  75 Train Loss:  0.59 Train Acc: 89.79% Train F1:  88.59%  Valid acc:  81.86% Valid f1: 75.02%\n",
            "Valid confusion matrix: \n",
            "[[577  43]\n",
            " [113 127]]\n",
            "(13216,)\n",
            "Epoch:  76 Train Loss:  0.59 Train Acc: 89.76% Train F1:  88.55%  Valid acc:  81.98% Valid f1: 76.51%\n",
            "Valid confusion matrix: \n",
            "[[560  60]\n",
            " [ 95 145]]\n",
            "(13216,)\n",
            "Epoch:  77 Train Loss:  0.59 Train Acc: 90.50% Train F1:  89.31%  Valid acc:  82.33% Valid f1: 76.29%\n",
            "Valid confusion matrix: \n",
            "[[571  49]\n",
            " [103 137]]\n",
            "(13216,)\n",
            "Epoch:  78 Train Loss:  0.59 Train Acc: 91.15% Train F1:  89.99%  Valid acc:  81.51% Valid f1: 76.94%\n",
            "Valid confusion matrix: \n",
            "[[542  78]\n",
            " [ 81 159]]\n",
            "(13216,)\n",
            "Epoch:  79 Train Loss:  0.59 Train Acc: 91.64% Train F1:  90.51%  Valid acc:  82.21% Valid f1: 76.39%\n",
            "Valid confusion matrix: \n",
            "[[567  53]\n",
            " [100 140]]\n",
            "(13216,)\n",
            "Epoch:  80 Train Loss:  0.59 Train Acc: 91.32% Train F1:  90.18%  Valid acc:  81.05% Valid f1: 76.04%\n",
            "Valid confusion matrix: \n",
            "[[545  75]\n",
            " [ 88 152]]\n",
            "(13216,)\n",
            "Epoch:  81 Train Loss:  0.59 Train Acc: 91.80% Train F1:  90.67%  Valid acc:  81.16% Valid f1: 75.83%\n",
            "Valid confusion matrix: \n",
            "[[551  69]\n",
            " [ 93 147]]\n",
            "(13216,)\n",
            "Epoch:  82 Train Loss:  0.59 Train Acc: 91.98% Train F1:  90.87%  Valid acc:  82.21% Valid f1: 76.88%\n",
            "Valid confusion matrix: \n",
            "[[560  60]\n",
            " [ 93 147]]\n",
            "(13216,)\n",
            "Epoch:  83 Train Loss:  0.59 Train Acc: 91.61% Train F1:  90.48%  Valid acc:  80.81% Valid f1: 75.75%\n",
            "Valid confusion matrix: \n",
            "[[544  76]\n",
            " [ 89 151]]\n",
            "(13216,)\n",
            "Epoch:  84 Train Loss:  0.59 Train Acc: 91.99% Train F1:  90.89%  Valid acc:  79.42% Valid f1: 75.42%\n",
            "Valid confusion matrix: \n",
            "[[515 105]\n",
            " [ 72 168]]\n",
            "(13216,)\n",
            "Epoch:  85 Train Loss:  0.59 Train Acc: 91.90% Train F1:  90.78%  Valid acc:  80.23% Valid f1: 76.09%\n",
            "Valid confusion matrix: \n",
            "[[524  96]\n",
            " [ 74 166]]\n",
            "(13216,)\n",
            "Epoch:  86 Train Loss:  0.59 Train Acc: 92.49% Train F1:  91.41%  Valid acc:  81.51% Valid f1: 75.68%\n",
            "Valid confusion matrix: \n",
            "[[561  59]\n",
            " [100 140]]\n",
            "(13216,)\n",
            "Epoch:  87 Train Loss:  0.59 Train Acc: 91.81% Train F1:  90.70%  Valid acc:  80.70% Valid f1: 73.94%\n",
            "Valid confusion matrix: \n",
            "[[566  54]\n",
            " [112 128]]\n",
            "(13216,)\n",
            "Epoch:  88 Train Loss:  0.59 Train Acc: 92.05% Train F1:  90.95%  Valid acc:  81.51% Valid f1: 76.63%\n",
            "Valid confusion matrix: \n",
            "[[547  73]\n",
            " [ 86 154]]\n",
            "(13216,)\n",
            "Epoch:  89 Train Loss:  0.59 Train Acc: 92.66% Train F1:  91.60%  Valid acc:  81.86% Valid f1: 77.10%\n",
            "Valid confusion matrix: \n",
            "[[548  72]\n",
            " [ 84 156]]\n",
            "(13216,)\n",
            "Epoch:  90 Train Loss:  0.59 Train Acc: 91.51% Train F1:  90.38%  Valid acc:  81.16% Valid f1: 76.77%\n",
            "Valid confusion matrix: \n",
            "[[536  84]\n",
            " [ 78 162]]\n",
            "(13216,)\n",
            "Epoch:  91 Train Loss:  0.59 Train Acc: 91.62% Train F1:  90.49%  Valid acc:  82.21% Valid f1: 76.39%\n",
            "Valid confusion matrix: \n",
            "[[567  53]\n",
            " [100 140]]\n",
            "(13216,)\n",
            "Epoch:  92 Train Loss:  0.59 Train Acc: 92.60% Train F1:  91.54%  Valid acc:  81.74% Valid f1: 76.48%\n",
            "Valid confusion matrix: \n",
            "[[555  65]\n",
            " [ 92 148]]\n",
            "(13216,)\n",
            "Epoch:  93 Train Loss:  0.59 Train Acc: 93.06% Train F1:  92.02%  Valid acc:  82.33% Valid f1: 77.45%\n",
            "Valid confusion matrix: \n",
            "[[554  66]\n",
            " [ 86 154]]\n",
            "(13216,)\n",
            "Epoch:  94 Train Loss:  0.59 Train Acc: 91.87% Train F1:  90.77%  Valid acc:  82.09% Valid f1: 77.09%\n",
            "Valid confusion matrix: \n",
            "[[554  66]\n",
            " [ 88 152]]\n",
            "(13216,)\n",
            "Epoch:  95 Train Loss:  0.59 Train Acc: 92.86% Train F1:  91.82%  Valid acc:  81.86% Valid f1: 75.10%\n",
            "Valid confusion matrix: \n",
            "[[576  44]\n",
            " [112 128]]\n",
            "(13216,)\n",
            "Epoch:  96 Train Loss:  0.59 Train Acc: 93.59% Train F1:  92.60%  Valid acc:  82.79% Valid f1: 77.66%\n",
            "Valid confusion matrix: \n",
            "[[562  58]\n",
            " [ 90 150]]\n",
            "(13216,)\n",
            "Epoch:  97 Train Loss:  0.59 Train Acc: 93.00% Train F1:  91.97%  Valid acc:  81.74% Valid f1: 76.99%\n",
            "Valid confusion matrix: \n",
            "[[547  73]\n",
            " [ 84 156]]\n",
            "(13216,)\n",
            "Epoch:  98 Train Loss:  0.59 Train Acc: 92.87% Train F1:  91.83%  Valid acc:  82.09% Valid f1: 76.34%\n",
            "Valid confusion matrix: \n",
            "[[565  55]\n",
            " [ 99 141]]\n",
            "(13216,)\n",
            "Epoch:  99 Train Loss:  0.59 Train Acc: 93.33% Train F1:  92.33%  Valid acc:  82.21% Valid f1: 76.94%\n",
            "Valid confusion matrix: \n",
            "[[559  61]\n",
            " [ 92 148]]\n",
            "(13216,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.85      0.90      0.88       620\n",
            "         1.0       0.70      0.60      0.65       240\n",
            "\n",
            "    accuracy                           0.82       860\n",
            "   macro avg       0.78      0.75      0.76       860\n",
            "weighted avg       0.81      0.82      0.81       860\n",
            "\n",
            "Epoch:  100 Train Loss:  0.59 Train Acc: 93.02% Train F1:  91.98%  Valid acc:  81.74% Valid f1: 76.27%\n",
            "Valid confusion matrix: \n",
            "[[558  62]\n",
            " [ 95 145]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EY1cWiSLxNmQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIoCIJUiyC_r",
        "colab_type": "text"
      },
      "source": [
        "# GRU\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ugfNKheyFMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GRU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(GRU, self).__init__()\n",
        "    epochs=10\n",
        "    VOCAB_SIZE=18122\n",
        "    EMBEDDING_DIM=300\n",
        "    HIDDEN_DIM=8\n",
        "    OUTPUT_DIM=1\n",
        "    max_len=25000\n",
        "    num_classes=1\n",
        "    self.embedding = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM)\n",
        "        # hidden layer\n",
        "    self.GRU = nn.GRU(EMBEDDING_DIM,HIDDEN_DIM,1,bidirectional=True)\n",
        "    self.hidden_dim=HIDDEN_DIM\n",
        "    self.out = nn.Linear(HIDDEN_DIM*2*2, num_classes)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    embedded = self.embedding(x)\n",
        "    states, hidden = self.GRU(embedded.permute([1, 0, 2]))\n",
        "    encoding = torch.cat([states[0],states[-1]], dim=1)\n",
        "    out = self.out(encoding)\n",
        "    return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qq6kpSmyKb0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0e795451-5b23-4fcb-d0b4-c663bf189c92"
      },
      "source": [
        "XTrainTens=torch.LongTensor(X_train_initial)\n",
        "XtestTens=torch.LongTensor(X_test_initial)\n",
        "ytestTens=torch.FloatTensor( y_test)\n",
        "y_trainTens=torch.FloatTensor(y_train)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#Not Static\n",
        "gru_notstatic=GRU()\n",
        "\n",
        "train(gru_notstatic,50,0.0001,embedding_matrix,True, XTrainTens, y_trainTens, XtestTens, ytestTens,device)\n",
        "#Static\n",
        "gru_static=GRU()\n",
        "\n",
        "train(gru_static,50,0.0001,embedding_matrix,False, XTrainTens, y_trainTens, XtestTens, ytestTens,device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(13216,)\n",
            "Epoch:  01 Train Loss:  0.64 Train Acc: 65.94% Train F1:  40.57%  Valid acc:  70.70% Valid f1: 43.97%\n",
            "Valid confusion matrix: \n",
            "[[601  19]\n",
            " [233   7]]\n",
            "(13216,)\n",
            "Epoch:  02 Train Loss:  0.64 Train Acc: 66.63% Train F1:  40.20%  Valid acc:  71.86% Valid f1: 42.99%\n",
            "Valid confusion matrix: \n",
            "[[615   5]\n",
            " [237   3]]\n",
            "(13216,)\n",
            "Epoch:  03 Train Loss:  0.62 Train Acc: 66.73% Train F1:  40.18%  Valid acc:  71.98% Valid f1: 42.25%\n",
            "Valid confusion matrix: \n",
            "[[618   2]\n",
            " [239   1]]\n",
            "(13216,)\n",
            "Epoch:  04 Train Loss:  0.59 Train Acc: 67.90% Train F1:  44.20%  Valid acc:  73.37% Valid f1: 46.94%\n",
            "Valid confusion matrix: \n",
            "[[619   1]\n",
            " [228  12]]\n",
            "(13216,)\n",
            "Epoch:  05 Train Loss:  0.53 Train Acc: 74.36% Train F1:  62.81%  Valid acc:  76.16% Valid f1: 57.83%\n",
            "Valid confusion matrix: \n",
            "[[611   9]\n",
            " [196  44]]\n",
            "(13216,)\n",
            "Epoch:  06 Train Loss:  0.47 Train Acc: 80.37% Train F1:  75.35%  Valid acc:  79.77% Valid f1: 69.05%\n",
            "Valid confusion matrix: \n",
            "[[596  24]\n",
            " [150  90]]\n",
            "(13216,)\n",
            "Epoch:  07 Train Loss:  0.41 Train Acc: 83.66% Train F1:  80.73%  Valid acc:  80.70% Valid f1: 71.86%\n",
            "Valid confusion matrix: \n",
            "[[588  32]\n",
            " [134 106]]\n",
            "(13216,)\n",
            "Epoch:  08 Train Loss:  0.37 Train Acc: 85.67% Train F1:  83.43%  Valid acc:  81.28% Valid f1: 74.09%\n",
            "Valid confusion matrix: \n",
            "[[576  44]\n",
            " [117 123]]\n",
            "(13216,)\n",
            "Epoch:  09 Train Loss:  0.34 Train Acc: 87.55% Train F1:  85.78%  Valid acc:  81.51% Valid f1: 75.54%\n",
            "Valid confusion matrix: \n",
            "[[563  57]\n",
            " [102 138]]\n",
            "(13216,)\n",
            "Epoch:  10 Train Loss:  0.31 Train Acc: 89.05% Train F1:  87.60%  Valid acc:  81.05% Valid f1: 73.94%\n",
            "Valid confusion matrix: \n",
            "[[573  47]\n",
            " [116 124]]\n",
            "(13216,)\n",
            "Epoch:  11 Train Loss:  0.28 Train Acc: 90.66% Train F1:  89.47%  Valid acc:  81.28% Valid f1: 74.60%\n",
            "Valid confusion matrix: \n",
            "[[570  50]\n",
            " [111 129]]\n",
            "(13216,)\n",
            "Epoch:  12 Train Loss:  0.25 Train Acc: 91.85% Train F1:  90.83%  Valid acc:  81.05% Valid f1: 74.28%\n",
            "Valid confusion matrix: \n",
            "[[569  51]\n",
            " [112 128]]\n",
            "(13216,)\n",
            "Epoch:  13 Train Loss:  0.23 Train Acc: 92.83% Train F1:  91.92%  Valid acc:  81.16% Valid f1: 74.40%\n",
            "Valid confusion matrix: \n",
            "[[570  50]\n",
            " [112 128]]\n",
            "(13216,)\n",
            "Epoch:  14 Train Loss:  0.21 Train Acc: 93.83% Train F1:  93.06%  Valid acc:  80.47% Valid f1: 73.71%\n",
            "Valid confusion matrix: \n",
            "[[564  56]\n",
            " [112 128]]\n",
            "(13216,)\n",
            "Epoch:  15 Train Loss:  0.19 Train Acc: 94.51% Train F1:  93.82%  Valid acc:  79.88% Valid f1: 72.88%\n",
            "Valid confusion matrix: \n",
            "[[562  58]\n",
            " [115 125]]\n",
            "(13216,)\n",
            "Epoch:  16 Train Loss:  0.17 Train Acc: 95.13% Train F1:  94.52%  Valid acc:  79.65% Valid f1: 72.91%\n",
            "Valid confusion matrix: \n",
            "[[557  63]\n",
            " [112 128]]\n",
            "(13216,)\n",
            "Epoch:  17 Train Loss:  0.15 Train Acc: 95.70% Train F1:  95.15%  Valid acc:  79.19% Valid f1: 72.12%\n",
            "Valid confusion matrix: \n",
            "[[557  63]\n",
            " [116 124]]\n",
            "(13216,)\n",
            "Epoch:  18 Train Loss:  0.14 Train Acc: 96.19% Train F1:  95.70%  Valid acc:  79.53% Valid f1: 72.80%\n",
            "Valid confusion matrix: \n",
            "[[556  64]\n",
            " [112 128]]\n",
            "(13216,)\n",
            "Epoch:  19 Train Loss:  0.13 Train Acc: 96.61% Train F1:  96.18%  Valid acc:  79.42% Valid f1: 72.34%\n",
            "Valid confusion matrix: \n",
            "[[559  61]\n",
            " [116 124]]\n",
            "(13216,)\n",
            "Epoch:  20 Train Loss:  0.12 Train Acc: 96.90% Train F1:  96.50%  Valid acc:  79.07% Valid f1: 72.18%\n",
            "Valid confusion matrix: \n",
            "[[554  66]\n",
            " [114 126]]\n",
            "(13216,)\n",
            "Epoch:  21 Train Loss:  0.11 Train Acc: 97.25% Train F1:  96.89%  Valid acc:  78.37% Valid f1: 71.60%\n",
            "Valid confusion matrix: \n",
            "[[547  73]\n",
            " [113 127]]\n",
            "(13216,)\n",
            "Epoch:  22 Train Loss:  0.10 Train Acc: 97.53% Train F1:  97.22%  Valid acc:  77.21% Valid f1: 70.51%\n",
            "Valid confusion matrix: \n",
            "[[537  83]\n",
            " [113 127]]\n",
            "(13216,)\n",
            "Epoch:  23 Train Loss:  0.09 Train Acc: 97.68% Train F1:  97.39%  Valid acc:  77.44% Valid f1: 70.55%\n",
            "Valid confusion matrix: \n",
            "[[541  79]\n",
            " [115 125]]\n",
            "(13216,)\n",
            "Epoch:  24 Train Loss:  0.08 Train Acc: 97.87% Train F1:  97.60%  Valid acc:  77.33% Valid f1: 70.36%\n",
            "Valid confusion matrix: \n",
            "[[541  79]\n",
            " [116 124]]\n",
            "(13216,)\n",
            "Epoch:  25 Train Loss:  0.07 Train Acc: 98.15% Train F1:  97.91%  Valid acc:  76.74% Valid f1: 69.82%\n",
            "Valid confusion matrix: \n",
            "[[536  84]\n",
            " [116 124]]\n",
            "(13216,)\n",
            "Epoch:  26 Train Loss:  0.07 Train Acc: 98.30% Train F1:  98.08%  Valid acc:  76.51% Valid f1: 69.43%\n",
            "Valid confusion matrix: \n",
            "[[536  84]\n",
            " [118 122]]\n",
            "(13216,)\n",
            "Epoch:  27 Train Loss:  0.06 Train Acc: 98.43% Train F1:  98.23%  Valid acc:  76.40% Valid f1: 69.58%\n",
            "Valid confusion matrix: \n",
            "[[532  88]\n",
            " [115 125]]\n",
            "(13216,)\n",
            "Epoch:  28 Train Loss:  0.06 Train Acc: 98.56% Train F1:  98.38%  Valid acc:  76.74% Valid f1: 69.55%\n",
            "Valid confusion matrix: \n",
            "[[539  81]\n",
            " [119 121]]\n",
            "(13216,)\n",
            "Epoch:  29 Train Loss:  0.05 Train Acc: 98.74% Train F1:  98.58%  Valid acc:  76.28% Valid f1: 69.21%\n",
            "Valid confusion matrix: \n",
            "[[534  86]\n",
            " [118 122]]\n",
            "(13216,)\n",
            "Epoch:  30 Train Loss:  0.05 Train Acc: 98.83% Train F1:  98.69%  Valid acc:  75.93% Valid f1: 68.89%\n",
            "Valid confusion matrix: \n",
            "[[531  89]\n",
            " [118 122]]\n",
            "(13216,)\n",
            "Epoch:  31 Train Loss:  0.05 Train Acc: 98.93% Train F1:  98.79%  Valid acc:  76.05% Valid f1: 68.82%\n",
            "Valid confusion matrix: \n",
            "[[534  86]\n",
            " [120 120]]\n",
            "(13216,)\n",
            "Epoch:  32 Train Loss:  0.04 Train Acc: 99.00% Train F1:  98.87%  Valid acc:  75.93% Valid f1: 68.89%\n",
            "Valid confusion matrix: \n",
            "[[531  89]\n",
            " [118 122]]\n",
            "(13216,)\n",
            "Epoch:  33 Train Loss:  0.04 Train Acc: 99.07% Train F1:  98.95%  Valid acc:  75.58% Valid f1: 68.12%\n",
            "Valid confusion matrix: \n",
            "[[533  87]\n",
            " [123 117]]\n",
            "(13216,)\n",
            "Epoch:  34 Train Loss:  0.04 Train Acc: 99.11% Train F1:  99.00%  Valid acc:  75.70% Valid f1: 68.59%\n",
            "Valid confusion matrix: \n",
            "[[530  90]\n",
            " [119 121]]\n",
            "(13216,)\n",
            "Epoch:  35 Train Loss:  0.03 Train Acc: 99.21% Train F1:  99.10%  Valid acc:  75.00% Valid f1: 67.41%\n",
            "Valid confusion matrix: \n",
            "[[530  90]\n",
            " [125 115]]\n",
            "(13216,)\n",
            "Epoch:  36 Train Loss:  0.03 Train Acc: 99.20% Train F1:  99.10%  Valid acc:  75.47% Valid f1: 67.34%\n",
            "Valid confusion matrix: \n",
            "[[539  81]\n",
            " [130 110]]\n",
            "(13216,)\n",
            "Epoch:  37 Train Loss:  0.03 Train Acc: 99.27% Train F1:  99.18%  Valid acc:  74.88% Valid f1: 67.31%\n",
            "Valid confusion matrix: \n",
            "[[529  91]\n",
            " [125 115]]\n",
            "(13216,)\n",
            "Epoch:  38 Train Loss:  0.03 Train Acc: 99.30% Train F1:  99.21%  Valid acc:  74.77% Valid f1: 67.39%\n",
            "Valid confusion matrix: \n",
            "[[526  94]\n",
            " [123 117]]\n",
            "(13216,)\n",
            "Epoch:  39 Train Loss:  0.03 Train Acc: 99.39% Train F1:  99.31%  Valid acc:  74.65% Valid f1: 67.38%\n",
            "Valid confusion matrix: \n",
            "[[524  96]\n",
            " [122 118]]\n",
            "(13216,)\n",
            "Epoch:  40 Train Loss:  0.03 Train Acc: 99.36% Train F1:  99.28%  Valid acc:  75.00% Valid f1: 67.60%\n",
            "Valid confusion matrix: \n",
            "[[528  92]\n",
            " [123 117]]\n",
            "(13216,)\n",
            "Epoch:  41 Train Loss:  0.03 Train Acc: 99.41% Train F1:  99.33%  Valid acc:  74.88% Valid f1: 67.50%\n",
            "Valid confusion matrix: \n",
            "[[527  93]\n",
            " [123 117]]\n",
            "(13216,)\n",
            "Epoch:  42 Train Loss:  0.03 Train Acc: 99.34% Train F1:  99.26%  Valid acc:  74.53% Valid f1: 67.46%\n",
            "Valid confusion matrix: \n",
            "[[521  99]\n",
            " [120 120]]\n",
            "(13216,)\n",
            "Epoch:  43 Train Loss:  0.02 Train Acc: 99.48% Train F1:  99.41%  Valid acc:  75.00% Valid f1: 67.60%\n",
            "Valid confusion matrix: \n",
            "[[528  92]\n",
            " [123 117]]\n",
            "(13216,)\n",
            "Epoch:  44 Train Loss:  0.02 Train Acc: 99.52% Train F1:  99.46%  Valid acc:  74.19% Valid f1: 67.50%\n",
            "Valid confusion matrix: \n",
            "[[514 106]\n",
            " [116 124]]\n",
            "(13216,)\n",
            "Epoch:  45 Train Loss:  0.02 Train Acc: 99.48% Train F1:  99.41%  Valid acc:  74.19% Valid f1: 67.42%\n",
            "Valid confusion matrix: \n",
            "[[515 105]\n",
            " [117 123]]\n",
            "(13216,)\n",
            "Epoch:  46 Train Loss:  0.02 Train Acc: 99.55% Train F1:  99.50%  Valid acc:  73.95% Valid f1: 67.21%\n",
            "Valid confusion matrix: \n",
            "[[513 107]\n",
            " [117 123]]\n",
            "(13216,)\n",
            "Epoch:  47 Train Loss:  0.02 Train Acc: 99.55% Train F1:  99.49%  Valid acc:  74.30% Valid f1: 67.16%\n",
            "Valid confusion matrix: \n",
            "[[520 100]\n",
            " [121 119]]\n",
            "(13216,)\n",
            "Epoch:  48 Train Loss:  0.02 Train Acc: 99.58% Train F1:  99.52%  Valid acc:  74.42% Valid f1: 66.89%\n",
            "Valid confusion matrix: \n",
            "[[525  95]\n",
            " [125 115]]\n",
            "(13216,)\n",
            "Epoch:  49 Train Loss:  0.02 Train Acc: 99.61% Train F1:  99.56%  Valid acc:  73.37% Valid f1: 66.25%\n",
            "Valid confusion matrix: \n",
            "[[513 107]\n",
            " [122 118]]\n",
            "(13216,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.85      0.82       620\n",
            "         1.0       0.53      0.45      0.49       240\n",
            "\n",
            "    accuracy                           0.74       860\n",
            "   macro avg       0.67      0.65      0.66       860\n",
            "weighted avg       0.73      0.74      0.73       860\n",
            "\n",
            "Epoch:  50 Train Loss:  0.02 Train Acc: 99.61% Train F1:  99.56%  Valid acc:  73.72% Valid f1: 65.69%\n",
            "Valid confusion matrix: \n",
            "[[525  95]\n",
            " [131 109]]\n",
            "(13216,)\n",
            "Epoch:  01 Train Loss:  0.64 Train Acc: 66.46% Train F1:  40.50%  Valid acc:  72.09% Valid f1: 42.69%\n",
            "Valid confusion matrix: \n",
            "[[618   2]\n",
            " [238   2]]\n",
            "(13216,)\n",
            "Epoch:  02 Train Loss:  0.63 Train Acc: 66.74% Train F1:  40.14%  Valid acc:  72.21% Valid f1: 42.34%\n",
            "Valid confusion matrix: \n",
            "[[620   0]\n",
            " [239   1]]\n",
            "(13216,)\n",
            "Epoch:  03 Train Loss:  0.60 Train Acc: 67.12% Train F1:  41.35%  Valid acc:  72.67% Valid f1: 44.45%\n",
            "Valid confusion matrix: \n",
            "[[619   1]\n",
            " [234   6]]\n",
            "(13216,)\n",
            "Epoch:  04 Train Loss:  0.53 Train Acc: 74.20% Train F1:  62.52%  Valid acc:  76.05% Valid f1: 58.84%\n",
            "Valid confusion matrix: \n",
            "[[605  15]\n",
            " [191  49]]\n",
            "(13216,)\n",
            "Epoch:  05 Train Loss:  0.46 Train Acc: 80.91% Train F1:  77.07%  Valid acc:  79.19% Valid f1: 68.75%\n",
            "Valid confusion matrix: \n",
            "[[589  31]\n",
            " [148  92]]\n",
            "(13216,)\n",
            "Epoch:  06 Train Loss:  0.41 Train Acc: 83.57% Train F1:  81.00%  Valid acc:  79.53% Valid f1: 70.82%\n",
            "Valid confusion matrix: \n",
            "[[577  43]\n",
            " [133 107]]\n",
            "(13216,)\n",
            "Epoch:  07 Train Loss:  0.37 Train Acc: 85.84% Train F1:  83.86%  Valid acc:  79.42% Valid f1: 70.81%\n",
            "Valid confusion matrix: \n",
            "[[575  45]\n",
            " [132 108]]\n",
            "(13216,)\n",
            "Epoch:  08 Train Loss:  0.34 Train Acc: 87.92% Train F1:  86.28%  Valid acc:  79.88% Valid f1: 71.87%\n",
            "Valid confusion matrix: \n",
            "[[573  47]\n",
            " [126 114]]\n",
            "(13216,)\n",
            "Epoch:  09 Train Loss:  0.31 Train Acc: 89.49% Train F1:  88.11%  Valid acc:  80.00% Valid f1: 72.27%\n",
            "Valid confusion matrix: \n",
            "[[571  49]\n",
            " [123 117]]\n",
            "(13216,)\n",
            "Epoch:  10 Train Loss:  0.28 Train Acc: 90.76% Train F1:  89.57%  Valid acc:  80.00% Valid f1: 72.18%\n",
            "Valid confusion matrix: \n",
            "[[572  48]\n",
            " [124 116]]\n",
            "(13216,)\n",
            "Epoch:  11 Train Loss:  0.26 Train Acc: 91.73% Train F1:  90.68%  Valid acc:  79.88% Valid f1: 72.97%\n",
            "Valid confusion matrix: \n",
            "[[561  59]\n",
            " [114 126]]\n",
            "(13216,)\n",
            "Epoch:  12 Train Loss:  0.23 Train Acc: 92.65% Train F1:  91.72%  Valid acc:  80.12% Valid f1: 72.76%\n",
            "Valid confusion matrix: \n",
            "[[568  52]\n",
            " [119 121]]\n",
            "(13216,)\n",
            "Epoch:  13 Train Loss:  0.21 Train Acc: 93.59% Train F1:  92.78%  Valid acc:  80.35% Valid f1: 72.99%\n",
            "Valid confusion matrix: \n",
            "[[570  50]\n",
            " [119 121]]\n",
            "(13216,)\n",
            "Epoch:  14 Train Loss:  0.20 Train Acc: 94.33% Train F1:  93.60%  Valid acc:  79.65% Valid f1: 72.57%\n",
            "Valid confusion matrix: \n",
            "[[561  59]\n",
            " [116 124]]\n",
            "(13216,)\n",
            "Epoch:  15 Train Loss:  0.18 Train Acc: 94.85% Train F1:  94.19%  Valid acc:  79.53% Valid f1: 72.63%\n",
            "Valid confusion matrix: \n",
            "[[558  62]\n",
            " [114 126]]\n",
            "(13216,)\n",
            "Epoch:  16 Train Loss:  0.17 Train Acc: 95.33% Train F1:  94.74%  Valid acc:  78.60% Valid f1: 71.82%\n",
            "Valid confusion matrix: \n",
            "[[549  71]\n",
            " [113 127]]\n",
            "(13216,)\n",
            "Epoch:  17 Train Loss:  0.16 Train Acc: 95.82% Train F1:  95.28%  Valid acc:  79.07% Valid f1: 72.18%\n",
            "Valid confusion matrix: \n",
            "[[554  66]\n",
            " [114 126]]\n",
            "(13216,)\n",
            "Epoch:  18 Train Loss:  0.14 Train Acc: 96.15% Train F1:  95.65%  Valid acc:  78.72% Valid f1: 71.22%\n",
            "Valid confusion matrix: \n",
            "[[558  62]\n",
            " [121 119]]\n",
            "(13216,)\n",
            "Epoch:  19 Train Loss:  0.13 Train Acc: 96.60% Train F1:  96.16%  Valid acc:  79.30% Valid f1: 72.14%\n",
            "Valid confusion matrix: \n",
            "[[559  61]\n",
            " [117 123]]\n",
            "(13216,)\n",
            "Epoch:  20 Train Loss:  0.12 Train Acc: 96.78% Train F1:  96.37%  Valid acc:  79.19% Valid f1: 72.20%\n",
            "Valid confusion matrix: \n",
            "[[556  64]\n",
            " [115 125]]\n",
            "(13216,)\n",
            "Epoch:  21 Train Loss:  0.12 Train Acc: 96.93% Train F1:  96.53%  Valid acc:  77.91% Valid f1: 71.89%\n",
            "Valid confusion matrix: \n",
            "[[534  86]\n",
            " [104 136]]\n",
            "(13216,)\n",
            "Epoch:  22 Train Loss:  0.11 Train Acc: 97.30% Train F1:  96.95%  Valid acc:  77.09% Valid f1: 70.57%\n",
            "Valid confusion matrix: \n",
            "[[534  86]\n",
            " [111 129]]\n",
            "(13216,)\n",
            "Epoch:  23 Train Loss:  0.10 Train Acc: 97.47% Train F1:  97.15%  Valid acc:  77.67% Valid f1: 70.68%\n",
            "Valid confusion matrix: \n",
            "[[544  76]\n",
            " [116 124]]\n",
            "(13216,)\n",
            "Epoch:  24 Train Loss:  0.09 Train Acc: 97.73% Train F1:  97.44%  Valid acc:  78.02% Valid f1: 71.10%\n",
            "Valid confusion matrix: \n",
            "[[546  74]\n",
            " [115 125]]\n",
            "(13216,)\n",
            "Epoch:  25 Train Loss:  0.09 Train Acc: 97.90% Train F1:  97.62%  Valid acc:  77.56% Valid f1: 70.83%\n",
            "Valid confusion matrix: \n",
            "[[540  80]\n",
            " [113 127]]\n",
            "(13216,)\n",
            "Epoch:  26 Train Loss:  0.08 Train Acc: 98.01% Train F1:  97.75%  Valid acc:  77.56% Valid f1: 70.57%\n",
            "Valid confusion matrix: \n",
            "[[543  77]\n",
            " [116 124]]\n",
            "(13216,)\n",
            "Epoch:  27 Train Loss:  0.08 Train Acc: 98.08% Train F1:  97.83%  Valid acc:  77.44% Valid f1: 70.46%\n",
            "Valid confusion matrix: \n",
            "[[542  78]\n",
            " [116 124]]\n",
            "(13216,)\n",
            "Epoch:  28 Train Loss:  0.08 Train Acc: 97.71% Train F1:  97.40%  Valid acc:  76.51% Valid f1: 69.43%\n",
            "Valid confusion matrix: \n",
            "[[536  84]\n",
            " [118 122]]\n",
            "(13216,)\n",
            "Epoch:  29 Train Loss:  0.07 Train Acc: 98.31% Train F1:  98.09%  Valid acc:  76.74% Valid f1: 69.37%\n",
            "Valid confusion matrix: \n",
            "[[541  79]\n",
            " [121 119]]\n",
            "(13216,)\n",
            "Epoch:  30 Train Loss:  0.06 Train Acc: 98.45% Train F1:  98.25%  Valid acc:  76.51% Valid f1: 69.34%\n",
            "Valid confusion matrix: \n",
            "[[537  83]\n",
            " [119 121]]\n",
            "(13216,)\n",
            "Epoch:  31 Train Loss:  0.06 Train Acc: 98.61% Train F1:  98.43%  Valid acc:  76.51% Valid f1: 69.15%\n",
            "Valid confusion matrix: \n",
            "[[539  81]\n",
            " [121 119]]\n",
            "(13216,)\n",
            "Epoch:  32 Train Loss:  0.06 Train Acc: 98.68% Train F1:  98.51%  Valid acc:  76.16% Valid f1: 68.93%\n",
            "Valid confusion matrix: \n",
            "[[535  85]\n",
            " [120 120]]\n",
            "(13216,)\n",
            "Epoch:  33 Train Loss:  0.06 Train Acc: 98.66% Train F1:  98.49%  Valid acc:  76.63% Valid f1: 69.17%\n",
            "Valid confusion matrix: \n",
            "[[541  79]\n",
            " [122 118]]\n",
            "(13216,)\n",
            "Epoch:  34 Train Loss:  0.05 Train Acc: 98.87% Train F1:  98.72%  Valid acc:  76.16% Valid f1: 69.11%\n",
            "Valid confusion matrix: \n",
            "[[533  87]\n",
            " [118 122]]\n",
            "(13216,)\n",
            "Epoch:  35 Train Loss:  0.05 Train Acc: 98.96% Train F1:  98.83%  Valid acc:  76.51% Valid f1: 69.15%\n",
            "Valid confusion matrix: \n",
            "[[539  81]\n",
            " [121 119]]\n",
            "(13216,)\n",
            "Epoch:  36 Train Loss:  0.04 Train Acc: 99.03% Train F1:  98.91%  Valid acc:  76.40% Valid f1: 69.23%\n",
            "Valid confusion matrix: \n",
            "[[536  84]\n",
            " [119 121]]\n",
            "(13216,)\n",
            "Epoch:  37 Train Loss:  0.04 Train Acc: 99.05% Train F1:  98.92%  Valid acc:  76.63% Valid f1: 69.17%\n",
            "Valid confusion matrix: \n",
            "[[541  79]\n",
            " [122 118]]\n",
            "(13216,)\n",
            "Epoch:  38 Train Loss:  0.04 Train Acc: 99.06% Train F1:  98.94%  Valid acc:  75.93% Valid f1: 69.07%\n",
            "Valid confusion matrix: \n",
            "[[529  91]\n",
            " [116 124]]\n",
            "(13216,)\n",
            "Epoch:  39 Train Loss:  0.04 Train Acc: 99.08% Train F1:  98.97%  Valid acc:  76.16% Valid f1: 68.56%\n",
            "Valid confusion matrix: \n",
            "[[539  81]\n",
            " [124 116]]\n",
            "(13216,)\n",
            "Epoch:  40 Train Loss:  0.04 Train Acc: 98.95% Train F1:  98.82%  Valid acc:  75.81% Valid f1: 69.47%\n",
            "Valid confusion matrix: \n",
            "[[522  98]\n",
            " [110 130]]\n",
            "(13216,)\n",
            "Epoch:  41 Train Loss:  0.04 Train Acc: 99.14% Train F1:  99.03%  Valid acc:  75.81% Valid f1: 68.61%\n",
            "Valid confusion matrix: \n",
            "[[532  88]\n",
            " [120 120]]\n",
            "(13216,)\n",
            "Epoch:  42 Train Loss:  0.03 Train Acc: 99.24% Train F1:  99.14%  Valid acc:  75.93% Valid f1: 68.62%\n",
            "Valid confusion matrix: \n",
            "[[534  86]\n",
            " [121 119]]\n",
            "(13216,)\n",
            "Epoch:  43 Train Loss:  0.03 Train Acc: 99.33% Train F1:  99.24%  Valid acc:  75.81% Valid f1: 68.43%\n",
            "Valid confusion matrix: \n",
            "[[534  86]\n",
            " [122 118]]\n",
            "(13216,)\n",
            "Epoch:  44 Train Loss:  0.03 Train Acc: 99.34% Train F1:  99.26%  Valid acc:  75.81% Valid f1: 68.88%\n",
            "Valid confusion matrix: \n",
            "[[529  91]\n",
            " [117 123]]\n",
            "(13216,)\n",
            "Epoch:  45 Train Loss:  0.03 Train Acc: 99.35% Train F1:  99.27%  Valid acc:  76.05% Valid f1: 69.09%\n",
            "Valid confusion matrix: \n",
            "[[531  89]\n",
            " [117 123]]\n",
            "(13216,)\n",
            "Epoch:  46 Train Loss:  0.03 Train Acc: 99.34% Train F1:  99.26%  Valid acc:  75.12% Valid f1: 68.42%\n",
            "Valid confusion matrix: \n",
            "[[521  99]\n",
            " [115 125]]\n",
            "(13216,)\n",
            "Epoch:  47 Train Loss:  0.03 Train Acc: 99.43% Train F1:  99.36%  Valid acc:  75.23% Valid f1: 67.99%\n",
            "Valid confusion matrix: \n",
            "[[528  92]\n",
            " [121 119]]\n",
            "(13216,)\n",
            "Epoch:  48 Train Loss:  0.02 Train Acc: 99.44% Train F1:  99.37%  Valid acc:  75.35% Valid f1: 68.19%\n",
            "Valid confusion matrix: \n",
            "[[528  92]\n",
            " [120 120]]\n",
            "(13216,)\n",
            "Epoch:  49 Train Loss:  0.02 Train Acc: 99.37% Train F1:  99.29%  Valid acc:  75.12% Valid f1: 67.80%\n",
            "Valid confusion matrix: \n",
            "[[528  92]\n",
            " [122 118]]\n",
            "(13216,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.81      0.85      0.83       620\n",
            "         1.0       0.56      0.49      0.52       240\n",
            "\n",
            "    accuracy                           0.75       860\n",
            "   macro avg       0.69      0.67      0.68       860\n",
            "weighted avg       0.74      0.75      0.74       860\n",
            "\n",
            "Epoch:  50 Train Loss:  0.02 Train Acc: 99.49% Train F1:  99.43%  Valid acc:  75.00% Valid f1: 67.69%\n",
            "Valid confusion matrix: \n",
            "[[527  93]\n",
            " [122 118]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A0tAl5Vy8_d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KySDEBsBzQ1S",
        "colab_type": "text"
      },
      "source": [
        "# LSTM + CNN\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1LqH-C3zSdR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    DPCNN for sentences classification.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(LSTMCNN, self).__init__()\n",
        "\n",
        "        VOCAB_SIZE=18122\n",
        "        EMBEDDING_DIM=300\n",
        "        HIDDEN_LAYER=64\n",
        "        OUTPUT_DIM=1\n",
        "        MAX_LEN=18122\n",
        "        OUTPUT_CLASSES=1\n",
        "        MAX_SENT_LEN=50\n",
        "        window_size=1\n",
        "        HIDDEN_DIM=8\n",
        "\n",
        "        \n",
        "        self.EMBEDDING_DIM=EMBEDDING_DIM\n",
        "        self.MAX_SENT_LEN=MAX_SENT_LEN\n",
        "        self.embedding = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM)\n",
        "      \n",
        "       # self.embedding.weight = nn.Parameter(embedding)\n",
        "       # self.embedding.weight.requires_grad = update_embedding\n",
        "        self.dropout = nn.Dropout(p=0.1) #  dropout layer\n",
        "\n",
        "\n",
        "        KERNEL_SIZES = [3,1]\n",
        "        self.NUM_KERNEL = 32\n",
        "        self.KERNEL_COUNT=len(KERNEL_SIZES)\n",
        "        #self.conv_region_embedding = nn.Conv2d(1, self.num_filter, (filter_size, EMBEDDING_DIM), stride=2)\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32,  kernel_size=(KERNEL_SIZES[0], EMBEDDING_DIM), stride=1 ) #conv for 1 kernel size\n",
        "        self.lstm = nn.LSTM(EMBEDDING_DIM,HIDDEN_DIM,1,bidirectional=True)\n",
        "\n",
        "\n",
        "        #self.conv2 = nn.Conv1d(in_channels=32, out_channels=32,  kernel_size=(KERNEL_SIZES[1], 1), stride=1 ) #conv for 1 kernel size\n",
        "\n",
        "       \n",
        "        \n",
        "\n",
        "        #self.maxpool = nn.MaxPool1d(kernel_size=5)\n",
        "        self.fully_connected = nn.Linear(HIDDEN_DIM*2*2, OUTPUT_CLASSES)\n",
        "        self.rel = nn.ReLU()\n",
        "        self.sig = nn.Sigmoid()\n",
        "        self.soft=nn.Softmax()\n",
        "\n",
        "    def forward(self, x, lengths=None):\n",
        "        x = self.embedding(x)\n",
        "        states, hidden = self.lstm(x)\n",
        "        encoding = torch.cat([states[0],states[-1]], dim=1)\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.rel(self.conv1(x))\n",
        "        x = x.squeeze(3) \n",
        "        x = F.max_pool1d(x, x.shape[2]) \n",
        "\n",
        "        x = x.squeeze(2)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "      \n",
        "\n",
        "        output = self.sig(self.fully_connected(x))\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvnE2QAzzWXP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a61503b8-de52-4f83-d989-cc24a3522a86"
      },
      "source": [
        "\n",
        "XTrainTens=torch.LongTensor(X_train_initial)\n",
        "XtestTens=torch.LongTensor(X_test_initial)\n",
        "ytestTens=torch.FloatTensor( y_test)\n",
        "y_trainTens=torch.FloatTensor(y_train)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "LSTMCNN_notstatic=LSTMCNN()\n",
        "\n",
        "#Not Static\n",
        "train(LSTMCNN_notstatic,50,0.001,embedding_matrix,True, XTrainTens, y_trainTens, XtestTens, ytestTens,device)\n",
        "#Static\n",
        "LSTMCNN_static=LSTMCNN()\n",
        "\n",
        "train(LSTMCNN_static,50,0.001,embedding_matrix,False, XTrainTens, y_trainTens, XtestTens, ytestTens,device)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(13216,)\n",
            "Epoch:  01 Train Loss:  0.70 Train Acc: 33.24% Train F1:  24.95%  Valid acc:  27.91% Valid f1: 21.82%\n",
            "Valid confusion matrix: \n",
            "[[  0 620]\n",
            " [  0 240]]\n",
            "(13216,)\n",
            "Epoch:  02 Train Loss:  0.69 Train Acc: 33.26% Train F1:  24.96%  Valid acc:  27.91% Valid f1: 21.82%\n",
            "Valid confusion matrix: \n",
            "[[  0 620]\n",
            " [  0 240]]\n",
            "(13216,)\n",
            "Epoch:  03 Train Loss:  0.69 Train Acc: 33.24% Train F1:  24.95%  Valid acc:  28.14% Valid f1: 22.39%\n",
            "Valid confusion matrix: \n",
            "[[  4 616]\n",
            " [  2 238]]\n",
            "(13216,)\n",
            "Epoch:  04 Train Loss:  0.69 Train Acc: 33.24% Train F1:  24.95%  Valid acc:  29.65% Valid f1: 24.58%\n",
            "Valid confusion matrix: \n",
            "[[ 16 604]\n",
            " [  1 239]]\n",
            "(13216,)\n",
            "Epoch:  05 Train Loss:  0.69 Train Acc: 33.22% Train F1:  24.93%  Valid acc:  28.84% Valid f1: 24.05%\n",
            "Valid confusion matrix: \n",
            "[[ 16 604]\n",
            " [  8 232]]\n",
            "(13216,)\n",
            "Epoch:  06 Train Loss:  0.69 Train Acc: 33.25% Train F1:  24.95%  Valid acc:  27.91% Valid f1: 21.82%\n",
            "Valid confusion matrix: \n",
            "[[  0 620]\n",
            " [  0 240]]\n",
            "(13216,)\n",
            "Epoch:  07 Train Loss:  0.66 Train Acc: 33.22% Train F1:  24.93%  Valid acc:  28.26% Valid f1: 22.36%\n",
            "Valid confusion matrix: \n",
            "[[  3 617]\n",
            " [  0 240]]\n",
            "(13216,)\n",
            "Epoch:  08 Train Loss:  0.64 Train Acc: 33.23% Train F1:  24.94%  Valid acc:  28.84% Valid f1: 23.56%\n",
            "Valid confusion matrix: \n",
            "[[ 11 609]\n",
            " [  3 237]]\n",
            "(13216,)\n",
            "Epoch:  09 Train Loss:  0.63 Train Acc: 33.27% Train F1:  24.96%  Valid acc:  29.30% Valid f1: 24.16%\n",
            "Valid confusion matrix: \n",
            "[[ 14 606]\n",
            " [  2 238]]\n",
            "(13216,)\n",
            "Epoch:  10 Train Loss:  0.62 Train Acc: 33.24% Train F1:  24.95%  Valid acc:  31.28% Valid f1: 27.05%\n",
            "Valid confusion matrix: \n",
            "[[ 31 589]\n",
            " [  2 238]]\n",
            "(13216,)\n",
            "Epoch:  11 Train Loss:  0.61 Train Acc: 33.24% Train F1:  24.95%  Valid acc:  32.91% Valid f1: 29.35%\n",
            "Valid confusion matrix: \n",
            "[[ 45 575]\n",
            " [  2 238]]\n",
            "(13216,)\n",
            "Epoch:  12 Train Loss:  0.60 Train Acc: 33.25% Train F1:  24.95%  Valid acc:  31.40% Valid f1: 27.22%\n",
            "Valid confusion matrix: \n",
            "[[ 32 588]\n",
            " [  2 238]]\n",
            "(13216,)\n",
            "Epoch:  13 Train Loss:  0.60 Train Acc: 33.22% Train F1:  24.93%  Valid acc:  35.81% Valid f1: 33.69%\n",
            "Valid confusion matrix: \n",
            "[[ 77 543]\n",
            " [  9 231]]\n",
            "(13216,)\n",
            "Epoch:  14 Train Loss:  0.59 Train Acc: 33.26% Train F1:  24.96%  Valid acc:  37.09% Valid f1: 35.30%\n",
            "Valid confusion matrix: \n",
            "[[ 88 532]\n",
            " [  9 231]]\n",
            "(13216,)\n",
            "Epoch:  15 Train Loss:  0.59 Train Acc: 33.21% Train F1:  24.93%  Valid acc:  41.74% Valid f1: 40.76%\n",
            "Valid confusion matrix: \n",
            "[[124 496]\n",
            " [  5 235]]\n",
            "(13216,)\n",
            "Epoch:  16 Train Loss:  0.59 Train Acc: 33.23% Train F1:  24.94%  Valid acc:  43.37% Valid f1: 42.87%\n",
            "Valid confusion matrix: \n",
            "[[146 474]\n",
            " [ 13 227]]\n",
            "(13216,)\n",
            "Epoch:  17 Train Loss:  0.59 Train Acc: 33.26% Train F1:  24.98%  Valid acc:  45.35% Valid f1: 45.12%\n",
            "Valid confusion matrix: \n",
            "[[167 453]\n",
            " [ 17 223]]\n",
            "(13216,)\n",
            "Epoch:  18 Train Loss:  0.59 Train Acc: 33.22% Train F1:  24.97%  Valid acc:  45.58% Valid f1: 45.40%\n",
            "Valid confusion matrix: \n",
            "[[171 449]\n",
            " [ 19 221]]\n",
            "(13216,)\n",
            "Epoch:  19 Train Loss:  0.59 Train Acc: 33.26% Train F1:  25.00%  Valid acc:  48.37% Valid f1: 48.36%\n",
            "Valid confusion matrix: \n",
            "[[201 419]\n",
            " [ 25 215]]\n",
            "(13216,)\n",
            "Epoch:  20 Train Loss:  0.58 Train Acc: 33.36% Train F1:  25.13%  Valid acc:  52.21% Valid f1: 52.19%\n",
            "Valid confusion matrix: \n",
            "[[233 387]\n",
            " [ 24 216]]\n",
            "(13216,)\n",
            "Epoch:  21 Train Loss:  0.58 Train Acc: 33.43% Train F1:  25.29%  Valid acc:  50.70% Valid f1: 50.70%\n",
            "Valid confusion matrix: \n",
            "[[215 405]\n",
            " [ 19 221]]\n",
            "(13216,)\n",
            "Epoch:  22 Train Loss:  0.58 Train Acc: 33.49% Train F1:  25.39%  Valid acc:  48.49% Valid f1: 48.47%\n",
            "Valid confusion matrix: \n",
            "[[201 419]\n",
            " [ 24 216]]\n",
            "(13216,)\n",
            "Epoch:  23 Train Loss:  0.58 Train Acc: 33.66% Train F1:  25.68%  Valid acc:  50.23% Valid f1: 50.23%\n",
            "Valid confusion matrix: \n",
            "[[220 400]\n",
            " [ 28 212]]\n",
            "(13216,)\n",
            "Epoch:  24 Train Loss:  0.58 Train Acc: 33.61% Train F1:  25.59%  Valid acc:  49.77% Valid f1: 49.77%\n",
            "Valid confusion matrix: \n",
            "[[216 404]\n",
            " [ 28 212]]\n",
            "(13216,)\n",
            "Epoch:  25 Train Loss:  0.58 Train Acc: 34.01% Train F1:  26.28%  Valid acc:  55.23% Valid f1: 54.91%\n",
            "Valid confusion matrix: \n",
            "[[274 346]\n",
            " [ 39 201]]\n",
            "(13216,)\n",
            "Epoch:  26 Train Loss:  0.58 Train Acc: 34.28% Train F1:  26.76%  Valid acc:  55.23% Valid f1: 55.06%\n",
            "Valid confusion matrix: \n",
            "[[264 356]\n",
            " [ 29 211]]\n",
            "(13216,)\n",
            "Epoch:  27 Train Loss:  0.58 Train Acc: 34.81% Train F1:  27.63%  Valid acc:  58.60% Valid f1: 58.04%\n",
            "Valid confusion matrix: \n",
            "[[302 318]\n",
            " [ 38 202]]\n",
            "(13216,)\n",
            "Epoch:  28 Train Loss:  0.58 Train Acc: 35.07% Train F1:  28.11%  Valid acc:  54.77% Valid f1: 54.47%\n",
            "Valid confusion matrix: \n",
            "[[270 350]\n",
            " [ 39 201]]\n",
            "(13216,)\n",
            "Epoch:  29 Train Loss:  0.58 Train Acc: 35.20% Train F1:  28.25%  Valid acc:  56.51% Valid f1: 56.11%\n",
            "Valid confusion matrix: \n",
            "[[284 336]\n",
            " [ 38 202]]\n",
            "(13216,)\n",
            "Epoch:  30 Train Loss:  0.58 Train Acc: 36.32% Train F1:  30.00%  Valid acc:  58.95% Valid f1: 58.23%\n",
            "Valid confusion matrix: \n",
            "[[310 310]\n",
            " [ 43 197]]\n",
            "(13216,)\n",
            "Epoch:  31 Train Loss:  0.58 Train Acc: 36.46% Train F1:  30.26%  Valid acc:  55.23% Valid f1: 54.83%\n",
            "Valid confusion matrix: \n",
            "[[278 342]\n",
            " [ 43 197]]\n",
            "(13216,)\n",
            "Epoch:  32 Train Loss:  0.58 Train Acc: 36.92% Train F1:  31.06%  Valid acc:  57.21% Valid f1: 56.60%\n",
            "Valid confusion matrix: \n",
            "[[297 323]\n",
            " [ 45 195]]\n",
            "(13216,)\n",
            "Epoch:  33 Train Loss:  0.58 Train Acc: 37.50% Train F1:  31.91%  Valid acc:  63.60% Valid f1: 62.07%\n",
            "Valid confusion matrix: \n",
            "[[360 260]\n",
            " [ 53 187]]\n",
            "(13216,)\n",
            "Epoch:  34 Train Loss:  0.58 Train Acc: 37.83% Train F1:  32.42%  Valid acc:  60.00% Valid f1: 59.03%\n",
            "Valid confusion matrix: \n",
            "[[324 296]\n",
            " [ 48 192]]\n",
            "(13216,)\n",
            "Epoch:  35 Train Loss:  0.58 Train Acc: 38.86% Train F1:  33.95%  Valid acc:  61.28% Valid f1: 60.27%\n",
            "Valid confusion matrix: \n",
            "[[332 288]\n",
            " [ 45 195]]\n",
            "(13216,)\n",
            "Epoch:  36 Train Loss:  0.58 Train Acc: 39.02% Train F1:  34.19%  Valid acc:  58.26% Valid f1: 57.60%\n",
            "Valid confusion matrix: \n",
            "[[304 316]\n",
            " [ 43 197]]\n",
            "(13216,)\n",
            "Epoch:  37 Train Loss:  0.58 Train Acc: 39.54% Train F1:  35.01%  Valid acc:  60.23% Valid f1: 59.42%\n",
            "Valid confusion matrix: \n",
            "[[320 300]\n",
            " [ 42 198]]\n",
            "(13216,)\n",
            "Epoch:  38 Train Loss:  0.58 Train Acc: 41.20% Train F1:  37.37%  Valid acc:  57.91% Valid f1: 57.33%\n",
            "Valid confusion matrix: \n",
            "[[299 321]\n",
            " [ 41 199]]\n",
            "(13216,)\n",
            "Epoch:  39 Train Loss:  0.58 Train Acc: 40.87% Train F1:  36.90%  Valid acc:  59.30% Valid f1: 58.44%\n",
            "Valid confusion matrix: \n",
            "[[317 303]\n",
            " [ 47 193]]\n",
            "(13216,)\n",
            "Epoch:  40 Train Loss:  0.58 Train Acc: 43.10% Train F1:  40.00%  Valid acc:  56.63% Valid f1: 56.09%\n",
            "Valid confusion matrix: \n",
            "[[291 329]\n",
            " [ 44 196]]\n",
            "(13216,)\n",
            "Epoch:  41 Train Loss:  0.58 Train Acc: 44.13% Train F1:  41.38%  Valid acc:  62.33% Valid f1: 61.01%\n",
            "Valid confusion matrix: \n",
            "[[347 273]\n",
            " [ 51 189]]\n",
            "(13216,)\n",
            "Epoch:  42 Train Loss:  0.58 Train Acc: 45.90% Train F1:  43.71%  Valid acc:  64.07% Valid f1: 62.33%\n",
            "Valid confusion matrix: \n",
            "[[368 252]\n",
            " [ 57 183]]\n",
            "(13216,)\n",
            "Epoch:  43 Train Loss:  0.58 Train Acc: 46.20% Train F1:  44.10%  Valid acc:  62.67% Valid f1: 61.25%\n",
            "Valid confusion matrix: \n",
            "[[352 268]\n",
            " [ 53 187]]\n",
            "(13216,)\n",
            "Epoch:  44 Train Loss:  0.58 Train Acc: 45.63% Train F1:  43.34%  Valid acc:  64.77% Valid f1: 62.94%\n",
            "Valid confusion matrix: \n",
            "[[374 246]\n",
            " [ 57 183]]\n",
            "(13216,)\n",
            "Epoch:  45 Train Loss:  0.58 Train Acc: 46.53% Train F1:  44.51%  Valid acc:  64.07% Valid f1: 62.37%\n",
            "Valid confusion matrix: \n",
            "[[367 253]\n",
            " [ 56 184]]\n",
            "(13216,)\n",
            "Epoch:  46 Train Loss:  0.58 Train Acc: 46.05% Train F1:  43.87%  Valid acc:  63.26% Valid f1: 61.83%\n",
            "Valid confusion matrix: \n",
            "[[355 265]\n",
            " [ 51 189]]\n",
            "(13216,)\n",
            "Epoch:  47 Train Loss:  0.58 Train Acc: 47.13% Train F1:  45.26%  Valid acc:  65.35% Valid f1: 63.15%\n",
            "Valid confusion matrix: \n",
            "[[386 234]\n",
            " [ 64 176]]\n",
            "(13216,)\n",
            "Epoch:  48 Train Loss:  0.58 Train Acc: 48.51% Train F1:  46.96%  Valid acc:  68.84% Valid f1: 66.39%\n",
            "Valid confusion matrix: \n",
            "[[412 208]\n",
            " [ 60 180]]\n",
            "(13216,)\n",
            "Epoch:  49 Train Loss:  0.58 Train Acc: 49.80% Train F1:  48.55%  Valid acc:  67.09% Valid f1: 64.58%\n",
            "Valid confusion matrix: \n",
            "[[403 217]\n",
            " [ 66 174]]\n",
            "(13216,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.69      0.76       620\n",
            "         1.0       0.47      0.71      0.56       240\n",
            "\n",
            "    accuracy                           0.69       860\n",
            "   macro avg       0.66      0.70      0.66       860\n",
            "weighted avg       0.75      0.69      0.71       860\n",
            "\n",
            "Epoch:  50 Train Loss:  0.58 Train Acc: 50.79% Train F1:  49.70%  Valid acc:  69.42% Valid f1: 66.42%\n",
            "Valid confusion matrix: \n",
            "[[427 193]\n",
            " [ 70 170]]\n",
            "(13216,)\n",
            "Epoch:  01 Train Loss:  0.70 Train Acc: 33.24% Train F1:  24.95%  Valid acc:  27.91% Valid f1: 21.82%\n",
            "Valid confusion matrix: \n",
            "[[  0 620]\n",
            " [  0 240]]\n",
            "(13216,)\n",
            "Epoch:  02 Train Loss:  0.69 Train Acc: 33.26% Train F1:  24.96%  Valid acc:  27.91% Valid f1: 21.82%\n",
            "Valid confusion matrix: \n",
            "[[  0 620]\n",
            " [  0 240]]\n",
            "(13216,)\n",
            "Epoch:  03 Train Loss:  0.69 Train Acc: 33.24% Train F1:  24.95%  Valid acc:  28.26% Valid f1: 22.47%\n",
            "Valid confusion matrix: \n",
            "[[  4 616]\n",
            " [  1 239]]\n",
            "(13216,)\n",
            "Epoch:  04 Train Loss:  0.69 Train Acc: 33.24% Train F1:  24.95%  Valid acc:  28.84% Valid f1: 23.66%\n",
            "Valid confusion matrix: \n",
            "[[ 12 608]\n",
            " [  4 236]]\n",
            "(13216,)\n",
            "Epoch:  05 Train Loss:  0.69 Train Acc: 33.22% Train F1:  24.93%  Valid acc:  29.30% Valid f1: 24.25%\n",
            "Valid confusion matrix: \n",
            "[[ 15 605]\n",
            " [  3 237]]\n",
            "(13216,)\n",
            "Epoch:  06 Train Loss:  0.69 Train Acc: 33.25% Train F1:  24.95%  Valid acc:  30.23% Valid f1: 25.54%\n",
            "Valid confusion matrix: \n",
            "[[ 22 598]\n",
            " [  2 238]]\n",
            "(13216,)\n",
            "Epoch:  07 Train Loss:  0.67 Train Acc: 33.22% Train F1:  24.93%  Valid acc:  28.02% Valid f1: 22.00%\n",
            "Valid confusion matrix: \n",
            "[[  1 619]\n",
            " [  0 240]]\n",
            "(13216,)\n",
            "Epoch:  08 Train Loss:  0.65 Train Acc: 33.23% Train F1:  24.94%  Valid acc:  28.49% Valid f1: 22.82%\n",
            "Valid confusion matrix: \n",
            "[[  6 614]\n",
            " [  1 239]]\n",
            "(13216,)\n",
            "Epoch:  09 Train Loss:  0.64 Train Acc: 33.27% Train F1:  24.96%  Valid acc:  28.26% Valid f1: 22.68%\n",
            "Valid confusion matrix: \n",
            "[[  6 614]\n",
            " [  3 237]]\n",
            "(13216,)\n",
            "Epoch:  10 Train Loss:  0.62 Train Acc: 33.24% Train F1:  24.95%  Valid acc:  28.37% Valid f1: 22.65%\n",
            "Valid confusion matrix: \n",
            "[[  5 615]\n",
            " [  1 239]]\n",
            "(13216,)\n",
            "Epoch:  11 Train Loss:  0.61 Train Acc: 33.24% Train F1:  24.95%  Valid acc:  28.84% Valid f1: 23.25%\n",
            "Valid confusion matrix: \n",
            "[[  8 612]\n",
            " [  0 240]]\n",
            "(13216,)\n",
            "Epoch:  12 Train Loss:  0.61 Train Acc: 33.25% Train F1:  24.95%  Valid acc:  29.07% Valid f1: 24.01%\n",
            "Valid confusion matrix: \n",
            "[[ 14 606]\n",
            " [  4 236]]\n",
            "(13216,)\n",
            "Epoch:  13 Train Loss:  0.60 Train Acc: 33.22% Train F1:  24.93%  Valid acc:  33.02% Valid f1: 29.59%\n",
            "Valid confusion matrix: \n",
            "[[ 47 573]\n",
            " [  3 237]]\n",
            "(13216,)\n",
            "Epoch:  14 Train Loss:  0.60 Train Acc: 33.26% Train F1:  24.96%  Valid acc:  33.37% Valid f1: 30.28%\n",
            "Valid confusion matrix: \n",
            "[[ 53 567]\n",
            " [  6 234]]\n",
            "(13216,)\n",
            "Epoch:  15 Train Loss:  0.60 Train Acc: 33.21% Train F1:  24.93%  Valid acc:  34.88% Valid f1: 32.17%\n",
            "Valid confusion matrix: \n",
            "[[ 64 556]\n",
            " [  4 236]]\n",
            "(13216,)\n",
            "Epoch:  16 Train Loss:  0.59 Train Acc: 33.23% Train F1:  24.94%  Valid acc:  34.07% Valid f1: 31.01%\n",
            "Valid confusion matrix: \n",
            "[[ 56 564]\n",
            " [  3 237]]\n",
            "(13216,)\n",
            "Epoch:  17 Train Loss:  0.59 Train Acc: 33.25% Train F1:  24.95%  Valid acc:  38.14% Valid f1: 36.60%\n",
            "Valid confusion matrix: \n",
            "[[ 97 523]\n",
            " [  9 231]]\n",
            "(13216,)\n",
            "Epoch:  18 Train Loss:  0.59 Train Acc: 33.21% Train F1:  24.94%  Valid acc:  43.14% Valid f1: 42.60%\n",
            "Valid confusion matrix: \n",
            "[[144 476]\n",
            " [ 13 227]]\n",
            "(13216,)\n",
            "Epoch:  19 Train Loss:  0.59 Train Acc: 33.24% Train F1:  24.96%  Valid acc:  43.49% Valid f1: 43.07%\n",
            "Valid confusion matrix: \n",
            "[[150 470]\n",
            " [ 16 224]]\n",
            "(13216,)\n",
            "Epoch:  20 Train Loss:  0.59 Train Acc: 33.26% Train F1:  24.96%  Valid acc:  45.58% Valid f1: 45.37%\n",
            "Valid confusion matrix: \n",
            "[[169 451]\n",
            " [ 17 223]]\n",
            "(13216,)\n",
            "Epoch:  21 Train Loss:  0.59 Train Acc: 33.26% Train F1:  24.98%  Valid acc:  45.00% Valid f1: 44.70%\n",
            "Valid confusion matrix: \n",
            "[[162 458]\n",
            " [ 15 225]]\n",
            "(13216,)\n",
            "Epoch:  22 Train Loss:  0.59 Train Acc: 33.28% Train F1:  25.02%  Valid acc:  46.63% Valid f1: 46.51%\n",
            "Valid confusion matrix: \n",
            "[[180 440]\n",
            " [ 19 221]]\n",
            "(13216,)\n",
            "Epoch:  23 Train Loss:  0.59 Train Acc: 33.32% Train F1:  25.09%  Valid acc:  46.63% Valid f1: 46.53%\n",
            "Valid confusion matrix: \n",
            "[[182 438]\n",
            " [ 21 219]]\n",
            "(13216,)\n",
            "Epoch:  24 Train Loss:  0.59 Train Acc: 33.41% Train F1:  25.22%  Valid acc:  46.28% Valid f1: 46.14%\n",
            "Valid confusion matrix: \n",
            "[[177 443]\n",
            " [ 19 221]]\n",
            "(13216,)\n",
            "Epoch:  25 Train Loss:  0.59 Train Acc: 33.58% Train F1:  25.54%  Valid acc:  51.05% Valid f1: 51.04%\n",
            "Valid confusion matrix: \n",
            "[[226 394]\n",
            " [ 27 213]]\n",
            "(13216,)\n",
            "Epoch:  26 Train Loss:  0.58 Train Acc: 33.87% Train F1:  26.02%  Valid acc:  53.60% Valid f1: 53.45%\n",
            "Valid confusion matrix: \n",
            "[[255 365]\n",
            " [ 34 206]]\n",
            "(13216,)\n",
            "Epoch:  27 Train Loss:  0.58 Train Acc: 33.79% Train F1:  25.93%  Valid acc:  50.23% Valid f1: 50.23%\n",
            "Valid confusion matrix: \n",
            "[[217 403]\n",
            " [ 25 215]]\n",
            "(13216,)\n",
            "Epoch:  28 Train Loss:  0.58 Train Acc: 34.00% Train F1:  26.30%  Valid acc:  51.51% Valid f1: 51.51%\n",
            "Valid confusion matrix: \n",
            "[[224 396]\n",
            " [ 21 219]]\n",
            "(13216,)\n",
            "Epoch:  29 Train Loss:  0.58 Train Acc: 34.47% Train F1:  27.01%  Valid acc:  53.26% Valid f1: 53.18%\n",
            "Valid confusion matrix: \n",
            "[[246 374]\n",
            " [ 28 212]]\n",
            "(13216,)\n",
            "Epoch:  30 Train Loss:  0.58 Train Acc: 34.84% Train F1:  27.63%  Valid acc:  54.88% Valid f1: 54.66%\n",
            "Valid confusion matrix: \n",
            "[[266 354]\n",
            " [ 34 206]]\n",
            "(13216,)\n",
            "Epoch:  31 Train Loss:  0.58 Train Acc: 34.87% Train F1:  27.66%  Valid acc:  52.79% Valid f1: 52.69%\n",
            "Valid confusion matrix: \n",
            "[[247 373]\n",
            " [ 33 207]]\n",
            "(13216,)\n",
            "Epoch:  32 Train Loss:  0.58 Train Acc: 35.21% Train F1:  28.27%  Valid acc:  50.00% Valid f1: 50.00%\n",
            "Valid confusion matrix: \n",
            "[[218 402]\n",
            " [ 28 212]]\n",
            "(13216,)\n",
            "Epoch:  33 Train Loss:  0.58 Train Acc: 35.76% Train F1:  29.12%  Valid acc:  58.60% Valid f1: 58.17%\n",
            "Valid confusion matrix: \n",
            "[[296 324]\n",
            " [ 32 208]]\n",
            "(13216,)\n",
            "Epoch:  34 Train Loss:  0.58 Train Acc: 36.38% Train F1:  30.14%  Valid acc:  55.70% Valid f1: 55.47%\n",
            "Valid confusion matrix: \n",
            "[[270 350]\n",
            " [ 31 209]]\n",
            "(13216,)\n",
            "Epoch:  35 Train Loss:  0.58 Train Acc: 38.08% Train F1:  32.73%  Valid acc:  58.37% Valid f1: 57.80%\n",
            "Valid confusion matrix: \n",
            "[[301 319]\n",
            " [ 39 201]]\n",
            "(13216,)\n",
            "Epoch:  36 Train Loss:  0.58 Train Acc: 38.51% Train F1:  33.40%  Valid acc:  56.63% Valid f1: 56.28%\n",
            "Valid confusion matrix: \n",
            "[[282 338]\n",
            " [ 35 205]]\n",
            "(13216,)\n",
            "Epoch:  37 Train Loss:  0.58 Train Acc: 38.76% Train F1:  33.79%  Valid acc:  57.79% Valid f1: 57.33%\n",
            "Valid confusion matrix: \n",
            "[[293 327]\n",
            " [ 36 204]]\n",
            "(13216,)\n",
            "Epoch:  38 Train Loss:  0.58 Train Acc: 39.65% Train F1:  35.09%  Valid acc:  56.40% Valid f1: 56.11%\n",
            "Valid confusion matrix: \n",
            "[[277 343]\n",
            " [ 32 208]]\n",
            "(13216,)\n",
            "Epoch:  39 Train Loss:  0.58 Train Acc: 40.10% Train F1:  35.78%  Valid acc:  61.28% Valid f1: 60.44%\n",
            "Valid confusion matrix: \n",
            "[[326 294]\n",
            " [ 39 201]]\n",
            "(13216,)\n",
            "Epoch:  40 Train Loss:  0.58 Train Acc: 41.46% Train F1:  37.66%  Valid acc:  58.37% Valid f1: 57.95%\n",
            "Valid confusion matrix: \n",
            "[[294 326]\n",
            " [ 32 208]]\n",
            "(13216,)\n",
            "Epoch:  41 Train Loss:  0.58 Train Acc: 42.07% Train F1:  38.53%  Valid acc:  60.12% Valid f1: 59.31%\n",
            "Valid confusion matrix: \n",
            "[[319 301]\n",
            " [ 42 198]]\n",
            "(13216,)\n",
            "Epoch:  42 Train Loss:  0.58 Train Acc: 42.24% Train F1:  38.77%  Valid acc:  60.58% Valid f1: 59.70%\n",
            "Valid confusion matrix: \n",
            "[[324 296]\n",
            " [ 43 197]]\n",
            "(13216,)\n",
            "Epoch:  43 Train Loss:  0.58 Train Acc: 42.25% Train F1:  38.79%  Valid acc:  57.91% Valid f1: 57.52%\n",
            "Valid confusion matrix: \n",
            "[[290 330]\n",
            " [ 32 208]]\n",
            "(13216,)\n",
            "Epoch:  44 Train Loss:  0.58 Train Acc: 42.54% Train F1:  39.19%  Valid acc:  63.72% Valid f1: 62.58%\n",
            "Valid confusion matrix: \n",
            "[[349 271]\n",
            " [ 41 199]]\n",
            "(13216,)\n",
            "Epoch:  45 Train Loss:  0.58 Train Acc: 42.97% Train F1:  39.77%  Valid acc:  62.09% Valid f1: 61.23%\n",
            "Valid confusion matrix: \n",
            "[[331 289]\n",
            " [ 37 203]]\n",
            "(13216,)\n",
            "Epoch:  46 Train Loss:  0.58 Train Acc: 43.34% Train F1:  40.28%  Valid acc:  60.47% Valid f1: 59.63%\n",
            "Valid confusion matrix: \n",
            "[[322 298]\n",
            " [ 42 198]]\n",
            "(13216,)\n",
            "Epoch:  47 Train Loss:  0.58 Train Acc: 43.94% Train F1:  41.07%  Valid acc:  62.67% Valid f1: 61.67%\n",
            "Valid confusion matrix: \n",
            "[[339 281]\n",
            " [ 40 200]]\n",
            "(13216,)\n",
            "Epoch:  48 Train Loss:  0.58 Train Acc: 45.58% Train F1:  43.21%  Valid acc:  60.12% Valid f1: 59.39%\n",
            "Valid confusion matrix: \n",
            "[[316 304]\n",
            " [ 39 201]]\n",
            "(13216,)\n",
            "Epoch:  49 Train Loss:  0.58 Train Acc: 45.54% Train F1:  43.22%  Valid acc:  62.56% Valid f1: 61.42%\n",
            "Valid confusion matrix: \n",
            "[[343 277]\n",
            " [ 45 195]]\n",
            "(13216,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.88      0.58      0.70       620\n",
            "         1.0       0.43      0.80      0.56       240\n",
            "\n",
            "    accuracy                           0.64       860\n",
            "   macro avg       0.65      0.69      0.63       860\n",
            "weighted avg       0.76      0.64      0.66       860\n",
            "\n",
            "Epoch:  50 Train Loss:  0.58 Train Acc: 46.52% Train F1:  44.44%  Valid acc:  64.30% Valid f1: 62.87%\n",
            "Valid confusion matrix: \n",
            "[[361 259]\n",
            " [ 48 192]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDwBXfi-zuu7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpz9u4-T3RHc",
        "colab_type": "text"
      },
      "source": [
        "# Keras Attention\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmm-2eqe8QXG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import Input\n",
        "from keras.layers import Concatenate\n",
        "\n",
        "def createEmbeddingLayer(wordIndex,not_static):\n",
        "    embedding_dim=300\n",
        "    vocabulary_size=min(len(wordIndex)+1,25000)\n",
        "    embedding_matrix = np.zeros((vocabulary_size, embedding_dim))\n",
        "    for word, i in wordIndex.items():\n",
        "        if i>=25000:\n",
        "          continue\n",
        "        try:\n",
        "            embedding_vector = w2v_model[word]\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "        except KeyError:\n",
        "          embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),embedding_dim)\n",
        "\n",
        "    custom_embedding_layer = Embedding(vocabulary_size,\n",
        "                                embedding_dim,\n",
        "                                weights=[embedding_matrix],\n",
        "                                trainable=not_static)\n",
        "    return custom_embedding_layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SC5Ya96I3Y96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ldNGzhE4iMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "from keras import initializers, regularizers, constraints\n",
        "from keras.engine.topology import Layer\n",
        "\n",
        "class Attention_feed_forward(Layer):\n",
        "    def __init__(self, step_dim,\n",
        "                 W_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "     \n",
        "        self.supports_masking = True\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        self.step_dim = step_dim\n",
        "        self.features_dim = 0\n",
        "\n",
        "        super(Attention_feed_forward, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight((input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        self.features_dim = input_shape[-1]\n",
        "\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight((input_shape[1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        features_dim = self.features_dim\n",
        "        step_dim = self.step_dim\n",
        "\n",
        "        e = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))  \n",
        "        if self.bias:\n",
        "            e += self.b\n",
        "        e = K.tanh(e)\n",
        "\n",
        "        a = K.exp(e)\n",
        "        if mask is not None:\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "  \n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "        a = K.expand_dims(a)\n",
        "\n",
        "        c = K.sum(a * x, axis=1)\n",
        "        return c\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0], self.features_dim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fi23TD8Xdqn8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import Input, Model\n",
        "from keras.layers import Embedding, Dense, Dropout, Bidirectional, CuDNNLSTM\n",
        "\n",
        "\n",
        "\n",
        "class Attention_BiLSTM(object):\n",
        "    def __init__(self, maxlen, embedding_dims,\n",
        "                 class_num=1,\n",
        "                 last_activation='sigmoid'):\n",
        "        self.maxlen = maxlen\n",
        "        self.embedding_dims = embedding_dims\n",
        "        self.class_num = class_num\n",
        "        self.last_activation = last_activation\n",
        "\n",
        "    def get_model(self):\n",
        "        input = Input((self.maxlen,))\n",
        "        embedding = createEmbeddingLayer(wordIndex_initial,False)(input)\n",
        "        x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(embedding)  # LSTM or GRU\n",
        "        x = Attention_feed_forward(self.maxlen)(x)\n",
        "\n",
        "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
        "        model = Model(inputs=input, outputs=output)\n",
        "        return model\n",
        "\n",
        "#Benefited from https://github.com/ShawnyXiao/TextClassification-Keras#5-textattbirnn\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoD0taBLdxgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def sigmoid(x): \n",
        "    return 1.0/(1 + np.exp(-x))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ikooAQXegqd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "074c118b-2f78-4f4d-8618-d370c461b973"
      },
      "source": [
        "def train_att(X_train_initial,X_test_initial,y_train,y_test) : \n",
        "  bilstm_attention_model= Attention_BiLSTM(100, 300).get_model()\n",
        "\n",
        "  early_stopping = [EarlyStopping(monitor='val_acc',min_delta=0,restore_best_weights=True, patience=50,verbose=1, mode='auto')]\n",
        "  bilstm_attention_model.compile(optimizer=keras.optimizers.Adam(lr=0.0005),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['acc',f1_m,precision_m, recall_m])\n",
        "  bilstm_attention_model.summary()\n",
        "\n",
        "  bilstm_attention_model.fit(X_train_initial, y_train,\n",
        "            batch_size=32,\n",
        "            epochs=150,\n",
        "            callbacks=early_stopping,\n",
        "            validation_split=0.1)\n",
        "\n",
        "  loss, accuracy, f1_score, precision, recall = bilstm_attention_model.evaluate(X_train_initial, y_train, verbose=1)\n",
        "  print(\"Attention- BiRNN Training Loss: {:.4f}\".format(loss))\n",
        "  print(\"Attention- BiRNN Accuracy: {:.4f}\".format(accuracy))\n",
        "  print(\"Attention- BiRNN f1 score: {:.4f}\".format(f1_score))\n",
        "  print(\"Attention- BiRNN Precision: {:.4f}\".format(precision))\n",
        "  print(\"Attention- BiRNN Recall: {:.4f}\".format(recall))\n",
        "\n",
        "  loss, accuracy, f1_score, precision, recall = bilstm_attention_model.evaluate(X_test_initial, y_test, verbose=1)\n",
        "  print(\"Attention- BiRNN Test Loss: {:.4f}\".format(loss))\n",
        "  print(\"Attention- BiRNN Test Accuracy: {:.4f}\".format(accuracy))\n",
        "  print(\"Attention- BiRNN Test f1 score: {:.4f}\".format(f1_score))\n",
        "  print(\"Attention- BiRNN Test Precision: {:.4f}\".format(precision))\n",
        "  print(\"Attention- BiRNN Test Recall: {:.4f}\".format(recall))\n",
        " \n",
        "  probs = bilstm_attention_model.predict(X_test_initial, verbose=1)\n",
        "  print('lenght of probs : ' ,len(probs))\n",
        "  #predicted_classes = probs.argmax(axis=-1)\n",
        "  predicted_classes=[0 if i < 0.5 else 1 for i in probs]\n",
        "  #predicted_classes = bilstm_attention_model.predict_classes(X_test_initial, verbose=1)\n",
        "  #print(predicted_classes)\n",
        "\n",
        "  print(classification_report(y_test, predicted_classes))\n",
        "\n",
        "train_att(np.array(X_train_initial),np.array(X_test_initial),np.array(y_train),np.array(y_test))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "embedding_8 (Embedding)      (None, 100, 300)          5436600   \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 100, 256)          440320    \n",
            "_________________________________________________________________\n",
            "attention_2 (Attention)      (None, 256)               356       \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 5,877,533\n",
            "Trainable params: 440,933\n",
            "Non-trainable params: 5,436,600\n",
            "_________________________________________________________________\n",
            "Train on 11916 samples, validate on 1324 samples\n",
            "Epoch 1/150\n",
            "11916/11916 [==============================] - 22s 2ms/step - loss: 0.5547 - acc: 0.7169 - f1_m: 0.3810 - precision_m: 0.5331 - recall_m: 0.3456 - val_loss: 0.5030 - val_acc: 0.7523 - val_f1_m: 0.6169 - val_precision_m: 0.6252 - val_recall_m: 0.6285\n",
            "Epoch 2/150\n",
            "11916/11916 [==============================] - 17s 1ms/step - loss: 0.4756 - acc: 0.7739 - f1_m: 0.6067 - precision_m: 0.7054 - recall_m: 0.5635 - val_loss: 0.4877 - val_acc: 0.7681 - val_f1_m: 0.5270 - val_precision_m: 0.7804 - val_recall_m: 0.4125\n",
            "Epoch 3/150\n",
            "11916/11916 [==============================] - 17s 1ms/step - loss: 0.4506 - acc: 0.7914 - f1_m: 0.6388 - precision_m: 0.7382 - recall_m: 0.5881 - val_loss: 0.4583 - val_acc: 0.7938 - val_f1_m: 0.6169 - val_precision_m: 0.7525 - val_recall_m: 0.5394\n",
            "Epoch 4/150\n",
            "11916/11916 [==============================] - 16s 1ms/step - loss: 0.4359 - acc: 0.7993 - f1_m: 0.6592 - precision_m: 0.7448 - recall_m: 0.6179 - val_loss: 0.4487 - val_acc: 0.8014 - val_f1_m: 0.6556 - val_precision_m: 0.7459 - val_recall_m: 0.6004\n",
            "Epoch 5/150\n",
            "11916/11916 [==============================] - 17s 1ms/step - loss: 0.4199 - acc: 0.8045 - f1_m: 0.6627 - precision_m: 0.7509 - recall_m: 0.6205 - val_loss: 0.4630 - val_acc: 0.7938 - val_f1_m: 0.6649 - val_precision_m: 0.7029 - val_recall_m: 0.6551\n",
            "Epoch 6/150\n",
            "11916/11916 [==============================] - 17s 1ms/step - loss: 0.4064 - acc: 0.8157 - f1_m: 0.6878 - precision_m: 0.7729 - recall_m: 0.6446 - val_loss: 0.4867 - val_acc: 0.7779 - val_f1_m: 0.6473 - val_precision_m: 0.6646 - val_recall_m: 0.6505\n",
            "Epoch 7/150\n",
            "11916/11916 [==============================] - 16s 1ms/step - loss: 0.3965 - acc: 0.8244 - f1_m: 0.6999 - precision_m: 0.7855 - recall_m: 0.6581 - val_loss: 0.4661 - val_acc: 0.7946 - val_f1_m: 0.6082 - val_precision_m: 0.7969 - val_recall_m: 0.5110\n",
            "Epoch 8/150\n",
            "11916/11916 [==============================] - 16s 1ms/step - loss: 0.3761 - acc: 0.8361 - f1_m: 0.7256 - precision_m: 0.7964 - recall_m: 0.6888 - val_loss: 0.4681 - val_acc: 0.7855 - val_f1_m: 0.6431 - val_precision_m: 0.7058 - val_recall_m: 0.6125\n",
            "Epoch 9/150\n",
            "11916/11916 [==============================] - 17s 1ms/step - loss: 0.3585 - acc: 0.8433 - f1_m: 0.7316 - precision_m: 0.8079 - recall_m: 0.6924 - val_loss: 0.4741 - val_acc: 0.7817 - val_f1_m: 0.6066 - val_precision_m: 0.7275 - val_recall_m: 0.5420\n",
            "Epoch 10/150\n",
            "11916/11916 [==============================] - 17s 1ms/step - loss: 0.3401 - acc: 0.8545 - f1_m: 0.7573 - precision_m: 0.8239 - recall_m: 0.7203 - val_loss: 0.5104 - val_acc: 0.7764 - val_f1_m: 0.6380 - val_precision_m: 0.6806 - val_recall_m: 0.6244\n",
            "Epoch 11/150\n",
            "11916/11916 [==============================] - 17s 1ms/step - loss: 0.3250 - acc: 0.8624 - f1_m: 0.7709 - precision_m: 0.8386 - recall_m: 0.7319 - val_loss: 0.5109 - val_acc: 0.7681 - val_f1_m: 0.6250 - val_precision_m: 0.6476 - val_recall_m: 0.6231\n",
            "Epoch 12/150\n",
            "11916/11916 [==============================] - 17s 1ms/step - loss: 0.3002 - acc: 0.8751 - f1_m: 0.7909 - precision_m: 0.8542 - recall_m: 0.7558 - val_loss: 0.5564 - val_acc: 0.7742 - val_f1_m: 0.5784 - val_precision_m: 0.7289 - val_recall_m: 0.5009\n",
            "Epoch 13/150\n",
            "11916/11916 [==============================] - 17s 1ms/step - loss: 0.2887 - acc: 0.8827 - f1_m: 0.8025 - precision_m: 0.8709 - recall_m: 0.7634 - val_loss: 0.5791 - val_acc: 0.7674 - val_f1_m: 0.6242 - val_precision_m: 0.6651 - val_recall_m: 0.6107\n",
            "Epoch 14/150\n",
            "11916/11916 [==============================] - 16s 1ms/step - loss: 0.2734 - acc: 0.8899 - f1_m: 0.8145 - precision_m: 0.8679 - recall_m: 0.7830 - val_loss: 0.6361 - val_acc: 0.7538 - val_f1_m: 0.5959 - val_precision_m: 0.6286 - val_recall_m: 0.5907\n",
            "Epoch 15/150\n",
            "11916/11916 [==============================] - 16s 1ms/step - loss: 0.2614 - acc: 0.8954 - f1_m: 0.8267 - precision_m: 0.8805 - recall_m: 0.7973 - val_loss: 0.6580 - val_acc: 0.7545 - val_f1_m: 0.6179 - val_precision_m: 0.6416 - val_recall_m: 0.6211\n",
            "Epoch 16/150\n",
            "11916/11916 [==============================] - 16s 1ms/step - loss: 0.2363 - acc: 0.9062 - f1_m: 0.8464 - precision_m: 0.8967 - recall_m: 0.8154 - val_loss: 0.6386 - val_acc: 0.7644 - val_f1_m: 0.6268 - val_precision_m: 0.6412 - val_recall_m: 0.6317\n",
            "Epoch 17/150\n",
            "11916/11916 [==============================] - 16s 1ms/step - loss: 0.2183 - acc: 0.9164 - f1_m: 0.8643 - precision_m: 0.9082 - recall_m: 0.8373 - val_loss: 0.6741 - val_acc: 0.7576 - val_f1_m: 0.6085 - val_precision_m: 0.6386 - val_recall_m: 0.6024\n",
            "Epoch 18/150\n",
            "11916/11916 [==============================] - 17s 1ms/step - loss: 0.2010 - acc: 0.9238 - f1_m: 0.8738 - precision_m: 0.9130 - recall_m: 0.8489 - val_loss: 0.7029 - val_acc: 0.7470 - val_f1_m: 0.5848 - val_precision_m: 0.6280 - val_recall_m: 0.5677\n",
            "Epoch 19/150\n",
            "11916/11916 [==============================] - 17s 1ms/step - loss: 0.1849 - acc: 0.9312 - f1_m: 0.8878 - precision_m: 0.9259 - recall_m: 0.8628 - val_loss: 0.7583 - val_acc: 0.7492 - val_f1_m: 0.6068 - val_precision_m: 0.6214 - val_recall_m: 0.6165\n",
            "Epoch 20/150\n",
            "11916/11916 [==============================] - 16s 1ms/step - loss: 0.1806 - acc: 0.9319 - f1_m: 0.8879 - precision_m: 0.9289 - recall_m: 0.8628 - val_loss: 0.8042 - val_acc: 0.7470 - val_f1_m: 0.6053 - val_precision_m: 0.6161 - val_recall_m: 0.6218\n",
            "Epoch 21/150\n",
            "11916/11916 [==============================] - 17s 1ms/step - loss: 0.1673 - acc: 0.9388 - f1_m: 0.9002 - precision_m: 0.9312 - recall_m: 0.8811 - val_loss: 0.7707 - val_acc: 0.7477 - val_f1_m: 0.5567 - val_precision_m: 0.6529 - val_recall_m: 0.5055\n",
            "Epoch 22/150\n",
            "11916/11916 [==============================] - 17s 1ms/step - loss: 0.1586 - acc: 0.9423 - f1_m: 0.9078 - precision_m: 0.9373 - recall_m: 0.8892 - val_loss: 0.7540 - val_acc: 0.7628 - val_f1_m: 0.6293 - val_precision_m: 0.6512 - val_recall_m: 0.6276\n",
            "Epoch 23/150\n",
            "11916/11916 [==============================] - 17s 1ms/step - loss: 0.1408 - acc: 0.9511 - f1_m: 0.9212 - precision_m: 0.9490 - recall_m: 0.9028 - val_loss: 0.8502 - val_acc: 0.7560 - val_f1_m: 0.5863 - val_precision_m: 0.6590 - val_recall_m: 0.5548\n",
            "Epoch 24/150\n",
            "11916/11916 [==============================] - 17s 1ms/step - loss: 0.1302 - acc: 0.9567 - f1_m: 0.9290 - precision_m: 0.9527 - recall_m: 0.9134 - val_loss: 0.7808 - val_acc: 0.7523 - val_f1_m: 0.5852 - val_precision_m: 0.6406 - val_recall_m: 0.5608\n",
            "Epoch 25/150\n",
            "11916/11916 [==============================] - 16s 1ms/step - loss: 0.1279 - acc: 0.9557 - f1_m: 0.9286 - precision_m: 0.9554 - recall_m: 0.9107 - val_loss: 0.8883 - val_acc: 0.7296 - val_f1_m: 0.5766 - val_precision_m: 0.5848 - val_recall_m: 0.5946\n",
            "Epoch 26/150\n",
            "11916/11916 [==============================] - 16s 1ms/step - loss: 0.1167 - acc: 0.9626 - f1_m: 0.9405 - precision_m: 0.9625 - recall_m: 0.9257 - val_loss: 0.9304 - val_acc: 0.7538 - val_f1_m: 0.6005 - val_precision_m: 0.6408 - val_recall_m: 0.5858\n",
            "Epoch 27/150\n",
            "11916/11916 [==============================] - 16s 1ms/step - loss: 0.1046 - acc: 0.9655 - f1_m: 0.9439 - precision_m: 0.9645 - recall_m: 0.9301 - val_loss: 0.9327 - val_acc: 0.7409 - val_f1_m: 0.6128 - val_precision_m: 0.5923 - val_recall_m: 0.6550\n",
            "Epoch 28/150\n",
            "11916/11916 [==============================] - 16s 1ms/step - loss: 0.1016 - acc: 0.9684 - f1_m: 0.9478 - precision_m: 0.9667 - recall_m: 0.9346 - val_loss: 0.9468 - val_acc: 0.7553 - val_f1_m: 0.5794 - val_precision_m: 0.6648 - val_recall_m: 0.5353\n",
            "Epoch 29/150\n",
            "11916/11916 [==============================] - 17s 1ms/step - loss: 0.1032 - acc: 0.9672 - f1_m: 0.9460 - precision_m: 0.9656 - recall_m: 0.9323 - val_loss: 0.9546 - val_acc: 0.7364 - val_f1_m: 0.5600 - val_precision_m: 0.6147 - val_recall_m: 0.5346\n",
            "Epoch 30/150\n",
            "11916/11916 [==============================] - 17s 1ms/step - loss: 0.0811 - acc: 0.9768 - f1_m: 0.9635 - precision_m: 0.9773 - recall_m: 0.9535 - val_loss: 0.9963 - val_acc: 0.7477 - val_f1_m: 0.5947 - val_precision_m: 0.6244 - val_recall_m: 0.5924\n",
            "Epoch 31/150\n",
            "11916/11916 [==============================] - 17s 1ms/step - loss: 0.0821 - acc: 0.9747 - f1_m: 0.9594 - precision_m: 0.9751 - recall_m: 0.9485 - val_loss: 1.0625 - val_acc: 0.7379 - val_f1_m: 0.5905 - val_precision_m: 0.6051 - val_recall_m: 0.5936\n",
            "Epoch 32/150\n",
            "11916/11916 [==============================] - 16s 1ms/step - loss: 0.0836 - acc: 0.9742 - f1_m: 0.9601 - precision_m: 0.9749 - recall_m: 0.9500 - val_loss: 1.1161 - val_acc: 0.7432 - val_f1_m: 0.6054 - val_precision_m: 0.6135 - val_recall_m: 0.6220\n",
            "Epoch 33/150\n",
            "11916/11916 [==============================] - 17s 1ms/step - loss: 0.0717 - acc: 0.9776 - f1_m: 0.9637 - precision_m: 0.9758 - recall_m: 0.9554 - val_loss: 1.1386 - val_acc: 0.7477 - val_f1_m: 0.5720 - val_precision_m: 0.6354 - val_recall_m: 0.5373\n",
            "Epoch 34/150\n",
            "11916/11916 [==============================] - 17s 1ms/step - loss: 0.0726 - acc: 0.9784 - f1_m: 0.9653 - precision_m: 0.9812 - recall_m: 0.9538 - val_loss: 1.0719 - val_acc: 0.7304 - val_f1_m: 0.5678 - val_precision_m: 0.5962 - val_recall_m: 0.5625\n",
            "Epoch 35/150\n",
            "11916/11916 [==============================] - 17s 1ms/step - loss: 0.0675 - acc: 0.9796 - f1_m: 0.9664 - precision_m: 0.9778 - recall_m: 0.9590 - val_loss: 1.1616 - val_acc: 0.7523 - val_f1_m: 0.6061 - val_precision_m: 0.6312 - val_recall_m: 0.6013\n",
            "Epoch 36/150\n",
            "11916/11916 [==============================] - 17s 1ms/step - loss: 0.0644 - acc: 0.9801 - f1_m: 0.9693 - precision_m: 0.9788 - recall_m: 0.9633 - val_loss: 1.1611 - val_acc: 0.7326 - val_f1_m: 0.5619 - val_precision_m: 0.5968 - val_recall_m: 0.5512\n",
            "Epoch 37/150\n",
            "11916/11916 [==============================] - 17s 1ms/step - loss: 0.0558 - acc: 0.9830 - f1_m: 0.9730 - precision_m: 0.9835 - recall_m: 0.9653 - val_loss: 1.1792 - val_acc: 0.7387 - val_f1_m: 0.5853 - val_precision_m: 0.6110 - val_recall_m: 0.5780\n",
            "Epoch 38/150\n",
            "11916/11916 [==============================] - 16s 1ms/step - loss: 0.0643 - acc: 0.9815 - f1_m: 0.9707 - precision_m: 0.9823 - recall_m: 0.9626 - val_loss: 1.1833 - val_acc: 0.7349 - val_f1_m: 0.5959 - val_precision_m: 0.5940 - val_recall_m: 0.6126\n",
            "Epoch 39/150\n",
            "11916/11916 [==============================] - 16s 1ms/step - loss: 0.0570 - acc: 0.9815 - f1_m: 0.9710 - precision_m: 0.9837 - recall_m: 0.9613 - val_loss: 1.2436 - val_acc: 0.7409 - val_f1_m: 0.5673 - val_precision_m: 0.6209 - val_recall_m: 0.5359\n",
            "Epoch 40/150\n",
            "11916/11916 [==============================] - 16s 1ms/step - loss: 0.0584 - acc: 0.9836 - f1_m: 0.9737 - precision_m: 0.9830 - recall_m: 0.9673 - val_loss: 1.0888 - val_acc: 0.7470 - val_f1_m: 0.5836 - val_precision_m: 0.6293 - val_recall_m: 0.5584\n",
            "Epoch 41/150\n",
            "11916/11916 [==============================] - 17s 1ms/step - loss: 0.0534 - acc: 0.9832 - f1_m: 0.9736 - precision_m: 0.9832 - recall_m: 0.9668 - val_loss: 1.2402 - val_acc: 0.7553 - val_f1_m: 0.5801 - val_precision_m: 0.6461 - val_recall_m: 0.5474\n",
            "Epoch 42/150\n",
            "11916/11916 [==============================] - 16s 1ms/step - loss: 0.0518 - acc: 0.9840 - f1_m: 0.9749 - precision_m: 0.9825 - recall_m: 0.9699 - val_loss: 1.3118 - val_acc: 0.7394 - val_f1_m: 0.6217 - val_precision_m: 0.5968 - val_recall_m: 0.6705\n",
            "Epoch 43/150\n",
            "11916/11916 [==============================] - 17s 1ms/step - loss: 0.0351 - acc: 0.9898 - f1_m: 0.9829 - precision_m: 0.9900 - recall_m: 0.9782 - val_loss: 1.3533 - val_acc: 0.7326 - val_f1_m: 0.6071 - val_precision_m: 0.5825 - val_recall_m: 0.6543\n",
            "Epoch 44/150\n",
            "11916/11916 [==============================] - 16s 1ms/step - loss: 0.0409 - acc: 0.9877 - f1_m: 0.9808 - precision_m: 0.9875 - recall_m: 0.9762 - val_loss: 1.2374 - val_acc: 0.7326 - val_f1_m: 0.5811 - val_precision_m: 0.5939 - val_recall_m: 0.5863\n",
            "Epoch 45/150\n",
            "11916/11916 [==============================] - 17s 1ms/step - loss: 0.0482 - acc: 0.9840 - f1_m: 0.9744 - precision_m: 0.9847 - recall_m: 0.9670 - val_loss: 1.2662 - val_acc: 0.7477 - val_f1_m: 0.6032 - val_precision_m: 0.6193 - val_recall_m: 0.6076\n",
            "Epoch 46/150\n",
            "11916/11916 [==============================] - 17s 1ms/step - loss: 0.0464 - acc: 0.9853 - f1_m: 0.9771 - precision_m: 0.9832 - recall_m: 0.9733 - val_loss: 1.2808 - val_acc: 0.7545 - val_f1_m: 0.6153 - val_precision_m: 0.6297 - val_recall_m: 0.6217\n",
            "Epoch 47/150\n",
            "11916/11916 [==============================] - 16s 1ms/step - loss: 0.0339 - acc: 0.9893 - f1_m: 0.9833 - precision_m: 0.9893 - recall_m: 0.9792 - val_loss: 1.4205 - val_acc: 0.7379 - val_f1_m: 0.5328 - val_precision_m: 0.6329 - val_recall_m: 0.4792\n",
            "Epoch 48/150\n",
            "11916/11916 [==============================] - 17s 1ms/step - loss: 0.0341 - acc: 0.9892 - f1_m: 0.9833 - precision_m: 0.9876 - recall_m: 0.9807 - val_loss: 1.3098 - val_acc: 0.7198 - val_f1_m: 0.5480 - val_precision_m: 0.5736 - val_recall_m: 0.5424\n",
            "Epoch 49/150\n",
            "11916/11916 [==============================] - 17s 1ms/step - loss: 0.0243 - acc: 0.9930 - f1_m: 0.9890 - precision_m: 0.9929 - recall_m: 0.9863 - val_loss: 1.5059 - val_acc: 0.7462 - val_f1_m: 0.5844 - val_precision_m: 0.6281 - val_recall_m: 0.5655\n",
            "Epoch 50/150\n",
            "11916/11916 [==============================] - 16s 1ms/step - loss: 0.0478 - acc: 0.9841 - f1_m: 0.9749 - precision_m: 0.9819 - recall_m: 0.9710 - val_loss: 1.4064 - val_acc: 0.7530 - val_f1_m: 0.6148 - val_precision_m: 0.6280 - val_recall_m: 0.6254\n",
            "Epoch 51/150\n",
            "11916/11916 [==============================] - 16s 1ms/step - loss: 0.0346 - acc: 0.9888 - f1_m: 0.9817 - precision_m: 0.9864 - recall_m: 0.9792 - val_loss: 1.3617 - val_acc: 0.7455 - val_f1_m: 0.5767 - val_precision_m: 0.6162 - val_recall_m: 0.5575\n",
            "Epoch 52/150\n",
            "11916/11916 [==============================] - 16s 1ms/step - loss: 0.0350 - acc: 0.9893 - f1_m: 0.9829 - precision_m: 0.9888 - recall_m: 0.9790 - val_loss: 1.4402 - val_acc: 0.7349 - val_f1_m: 0.5845 - val_precision_m: 0.6091 - val_recall_m: 0.5825\n",
            "Epoch 53/150\n",
            "11916/11916 [==============================] - 16s 1ms/step - loss: 0.0332 - acc: 0.9894 - f1_m: 0.9835 - precision_m: 0.9877 - recall_m: 0.9810 - val_loss: 1.3667 - val_acc: 0.7334 - val_f1_m: 0.5818 - val_precision_m: 0.5984 - val_recall_m: 0.5855\n",
            "Epoch 54/150\n",
            "11916/11916 [==============================] - 17s 1ms/step - loss: 0.0212 - acc: 0.9934 - f1_m: 0.9894 - precision_m: 0.9927 - recall_m: 0.9873 - val_loss: 1.5141 - val_acc: 0.7409 - val_f1_m: 0.5851 - val_precision_m: 0.6189 - val_recall_m: 0.5816\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00054: early stopping\n",
            "13240/13240 [==============================] - 6s 482us/step\n",
            "Attention- BiRNN Training Loss: 0.4208\n",
            "Attention- BiRNN Accuracy: 0.8103\n",
            "Attention- BiRNN f1 score: 0.6731\n",
            "Attention- BiRNN Precision: 0.7642\n",
            "Attention- BiRNN Recall: 0.6200\n",
            "860/860 [==============================] - 0s 514us/step\n",
            "Attention- BiRNN Test Loss: 0.4056\n",
            "Attention- BiRNN Test Accuracy: 0.8291\n",
            "Attention- BiRNN Test f1 score: 0.6134\n",
            "Attention- BiRNN Test Precision: 0.7718\n",
            "Attention- BiRNN Test Recall: 0.5305\n",
            "860/860 [==============================] - 1s 1ms/step\n",
            "lenght of probs :  860\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.94      0.89       620\n",
            "           1       0.78      0.54      0.64       240\n",
            "\n",
            "    accuracy                           0.83       860\n",
            "   macro avg       0.81      0.74      0.76       860\n",
            "weighted avg       0.82      0.83      0.82       860\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utB9x0Xtjj1G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "a35f643f-847a-4ea4-9f13-99432c0955c8"
      },
      "source": [
        "probs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-936a79872551>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'probs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgCraKOUkWvt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzJFUXdAwn0o",
        "colab_type": "text"
      },
      "source": [
        "#BERT\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-5ZPIjMwrTv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
        "from keras.layers.embeddings import Embedding\n",
        "from sklearn.metrics import classification_report\n",
        "def recall_m(true_Y, pred_Y):\n",
        "        TP = K.sum(K.round(K.clip(true_Y * pred_Y, 0, 1)))\n",
        "        possible_pos = K.sum(K.round(K.clip(true_Y, 0, 1)))\n",
        "        rec = TP / (possible_pos + K.epsilon())\n",
        "        return rec\n",
        "\n",
        "def precision_m(true_Y, pred_Y):\n",
        "        true_positives = K.sum(K.round(K.clip(true_Y * pred_Y, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(pred_Y, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "\n",
        "def f1_m(true_Y, pred_Y):\n",
        "    pres = precision_m(true_Y, pred_Y)\n",
        "    rec = recall_m(true_Y, pred_Y)\n",
        "    return 2*((pres*rec)/(pres+rec+K.epsilon()))\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Femq2DqywxHA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "e3d44f5d-e88a-4636-a634-c24d87e6c257"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "!pip install pytorch_pretrained_bert\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
        "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.10.40)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.28.1)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.17.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.9)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2019.11.28)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.40 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.13.40)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.9.4)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->pytorch_pretrained_bert) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\"->botocore<1.14.0,>=1.13.40->boto3->pytorch_pretrained_bert) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzxrC9nzwyXP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Data pre-processing before BERT\n",
        "tweets_train = X_train.tweet_initial_nontoken.values\n",
        "tweets_test = X_test.tweet_initial_nontoken.values\n",
        "\n",
        "\n",
        "tweets_train = [\"[CLS] \" + tweet_initial_nontoken + \" [SEP]\" for tweet_initial_nontoken in tweets_train]\n",
        "tweets_test = [\"[CLS] \" + tweet_initial_nontoken + \" [SEP]\" for tweet_initial_nontoken in tweets_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqBzK3bgw1xa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X=tweets_train+tweets_test\n",
        "Y=y_train+y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iq8X7CsDw2_L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "dedf5ed3-338f-4de0-8ea7-2c2c2582ef99"
      },
      "source": [
        "print(np.array(X).shape)\n",
        "print(np.array(Y).shape)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(14100,)\n",
            "(14100,)\n",
            "(13240, 9)\n",
            "(860, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9RElQ56w4Gg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "98498e80-498b-43c6-eb50-a5aefd836640"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "tokenized_X = [tokenizer.tokenize(sent) for sent in X]\n",
        "\n",
        "\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_X[0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenize the first sentence:\n",
            "['[CLS]', 'user', 'she', 'should', 'ask', 'a', 'few', 'native', 'americans', 'what', 'their', 'take', 'on', 'this', 'is', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUS_cb9Tw97F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "11217931-3ca9-4912-d0c4-dc41bc29516a"
      },
      "source": [
        "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_X]\n",
        "\n",
        "# Pad our input tokens\n",
        "MAX_LEN=120\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)\n",
        "\n",
        "\n",
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "#train_inputs = input_ids[0:13240]\n",
        "#validation_inputs=input_ds[13240:]\n",
        "#train_labels=Y[0:13240]\n",
        "#validation_labels=Y[13240:]\n",
        "#train_masks=[atten]\n",
        "\n",
        "\n",
        "#küçük bir hesaplama gerekiyor!! test_size = 860/14100\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, Y, \n",
        "                                                            random_state=2018, test_size=860/14100)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2018, test_size=860/14100)\n",
        "\n",
        "print(train_inputs.shape)\n",
        "print(validation_inputs.shape)\n",
        "print(train_labels)\n",
        "print(validation_labels)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(13240, 120)\n",
            "(860, 120)\n",
            "[1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1]\n",
            "[0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdwMiGnmxDRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)\n",
        "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
        "batch_size = 32\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sh1ugathxG75",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d68d7318-3f3a-4c8e-9885-349ac9877bd1"
      },
      "source": [
        "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIelc7XoxeAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfyMFVUoxmUj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "6c6ee3e5-18d7-4885-c4a6-d6589d9337f2"
      },
      "source": [
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "                     lr=2e-5,\n",
        "                     )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t_total value of -1 results in schedule not being applied\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-GRpFcXxIey",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bcb66512-eaed-4a76-c8af-e1af5b7bf89e"
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "from sklearn.metrics import confusion_matrix\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    report=classification_report(labels_flat , pred_flat)\n",
        "    \n",
        "    print('pred_flat:',pred_flat)\n",
        "    print('labels:',labels_flat)\n",
        "    \n",
        "\n",
        "\n",
        "    return (np.sum(pred_flat == labels_flat) / len(labels_flat)) ,report\n",
        "def concat(preds,labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    report=classification_report(labels_flat , pred_flat)\n",
        "    \n",
        "    print('pred_flat:',pred_flat)\n",
        "    print('labels:',labels_flat)\n",
        "\n",
        "\n",
        "    return list(labels_flat) ,list(pred_flat)\n",
        "\n",
        "# Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 8\n",
        "\n",
        "# trange is a tqdm wrapper around the normal python range\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "\n",
        "  \n",
        "  # Training\n",
        "  \n",
        "  # Set our model to training mode (as opposed to evaluation mode)\n",
        "  model.train()\n",
        "  \n",
        "  # Tracking variables\n",
        "  tr_loss = 0\n",
        "  nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "  # Train the data for one epoch\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Clear out the gradients (by default they accumulate)\n",
        "    optimizer.zero_grad()\n",
        "    # Forward pass\n",
        "\n",
        "    loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels) # CUDA OUT OF MEMORY\n",
        "\n",
        "    train_loss_set.append(loss.item())    \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    # Update parameters and take a step using the computed gradient\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update tracking variables\n",
        "    tr_loss += loss.item()\n",
        "    nb_tr_examples += b_input_ids.size(0)\n",
        "    nb_tr_steps += 1\n",
        "\n",
        "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "    \n",
        "    \n",
        "  # Validation\n",
        "\n",
        "  # Put model in evaluation mode to evaluate loss on the validation set\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  eval_loss, eval_accuracy = 0, 0\n",
        "  nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "  # Evaluate data for one epoch\n",
        "  LABELS=[]\n",
        "  PREDICTION=[]\n",
        "  for batch in validation_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "    \n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    tmp_eval_accuracy,report = flat_accuracy(logits, label_ids)\n",
        "    p,l=concat(logits, label_ids)\n",
        "    LABELS+=l\n",
        "    PREDICTION+=p    \n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "    \n",
        "  report=classification_report(LABELS , PREDICTION)  \n",
        "  print(report)\n",
        "  print('confusion_matrix :', confusion_matrix(LABELS, PREDICTION))\n",
        "\n",
        "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   0%|          | 0/8 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.4712742041202559\n",
            "pred_flat: [0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1]\n",
            "labels: [0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1]\n",
            "labels: [0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1]\n",
            "pred_flat: [1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1]\n",
            "labels: [1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "pred_flat: [1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1]\n",
            "labels: [1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "pred_flat: [1 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0]\n",
            "pred_flat: [1 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0]\n",
            "pred_flat: [0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "pred_flat: [0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "pred_flat: [1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 0 0 1 1]\n",
            "labels: [1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0]\n",
            "pred_flat: [1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 0 0 1 1]\n",
            "labels: [1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0]\n",
            "pred_flat: [0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 0 0]\n",
            "labels: [0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0]\n",
            "pred_flat: [0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 0 0]\n",
            "labels: [0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0]\n",
            "pred_flat: [1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "labels: [0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0]\n",
            "pred_flat: [1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "labels: [0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0]\n",
            "pred_flat: [0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0]\n",
            "labels: [1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0]\n",
            "pred_flat: [0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0]\n",
            "labels: [1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1 0 1 0 1 0 0 0]\n",
            "labels: [0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0]\n",
            "pred_flat: [0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1 0 1 0 1 0 0 0]\n",
            "labels: [0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1]\n",
            "labels: [0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1]\n",
            "labels: [0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1]\n",
            "labels: [1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1]\n",
            "pred_flat: [1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1]\n",
            "labels: [1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0]\n",
            "labels: [0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0]\n",
            "labels: [0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1]\n",
            "pred_flat: [1 1 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "labels: [0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1]\n",
            "pred_flat: [1 1 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "labels: [0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1]\n",
            "pred_flat: [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0]\n",
            "labels: [0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "pred_flat: [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0]\n",
            "labels: [0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0]\n",
            "labels: [0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0]\n",
            "labels: [0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0]\n",
            "pred_flat: [0 0 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1]\n",
            "labels: [0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1]\n",
            "labels: [0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 0 0]\n",
            "labels: [0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            "pred_flat: [0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 0 0]\n",
            "labels: [0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            "pred_flat: [0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "labels: [0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "labels: [0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1]\n",
            "labels: [1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1]\n",
            "labels: [1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            "labels: [0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            "labels: [0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 1 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0]\n",
            "labels: [0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0]\n",
            "pred_flat: [0 1 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0]\n",
            "labels: [0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0]\n",
            "pred_flat: [1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            "labels: [0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0]\n",
            "pred_flat: [1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            "labels: [0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1]\n",
            "pred_flat: [0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1]\n",
            "pred_flat: [0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  12%|█▎        | 1/8 [09:48<1:08:42, 588.93s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "pred_flat: [0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "labels: [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "pred_flat: [0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "labels: [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.83      0.86       616\n",
            "           1       0.64      0.75      0.69       244\n",
            "\n",
            "    accuracy                           0.81       860\n",
            "   macro avg       0.77      0.79      0.78       860\n",
            "weighted avg       0.82      0.81      0.81       860\n",
            "\n",
            "confusion_matrix : [[514 102]\n",
            " [ 61 183]]\n",
            "Validation Accuracy: 0.8111772486772487\n",
            "Train loss: 0.3340197319641781\n",
            "pred_flat: [0 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0]\n",
            "labels: [0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0]\n",
            "labels: [0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1]\n",
            "labels: [1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1]\n",
            "labels: [1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "pred_flat: [1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0]\n",
            "pred_flat: [1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0]\n",
            "pred_flat: [0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0]\n",
            "labels: [0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "pred_flat: [0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0]\n",
            "labels: [0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "pred_flat: [1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1]\n",
            "labels: [1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0]\n",
            "pred_flat: [1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1]\n",
            "labels: [1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0]\n",
            "pred_flat: [0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 0]\n",
            "labels: [0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0]\n",
            "pred_flat: [0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 0]\n",
            "labels: [0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0]\n",
            "pred_flat: [1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 0 0 1 0]\n",
            "labels: [0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0]\n",
            "pred_flat: [1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 0 0 1 0]\n",
            "labels: [0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0]\n",
            "pred_flat: [0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0]\n",
            "labels: [1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0]\n",
            "pred_flat: [0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0]\n",
            "labels: [1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 0]\n",
            "labels: [0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0]\n",
            "pred_flat: [0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 0]\n",
            "labels: [0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1]\n",
            "labels: [0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1]\n",
            "labels: [0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1]\n",
            "labels: [1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1]\n",
            "pred_flat: [1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1]\n",
            "labels: [1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0]\n",
            "labels: [0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0]\n",
            "labels: [0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1]\n",
            "pred_flat: [1 1 0 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "labels: [0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1]\n",
            "pred_flat: [1 1 0 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "labels: [0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1]\n",
            "pred_flat: [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 1 0]\n",
            "labels: [0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "pred_flat: [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 1 0]\n",
            "labels: [0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0]\n",
            "labels: [0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0]\n",
            "labels: [0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0]\n",
            "pred_flat: [0 0 0 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1]\n",
            "labels: [0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1]\n",
            "labels: [0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 1 0 0]\n",
            "labels: [0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            "pred_flat: [0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 1 0 0]\n",
            "labels: [0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            "pred_flat: [1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "labels: [0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "labels: [0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0]\n",
            "labels: [1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0]\n",
            "labels: [1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            "labels: [0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            "labels: [0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0]\n",
            "labels: [0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0]\n",
            "pred_flat: [0 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0]\n",
            "labels: [0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0]\n",
            "pred_flat: [0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1]\n",
            "pred_flat: [0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1]\n",
            "pred_flat: [0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  25%|██▌       | 2/8 [19:37<58:53, 588.90s/it]  "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "pred_flat: [0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "labels: [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "labels: [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.84      0.86       613\n",
            "           1       0.65      0.75      0.70       247\n",
            "\n",
            "    accuracy                           0.81       860\n",
            "   macro avg       0.77      0.79      0.78       860\n",
            "weighted avg       0.82      0.81      0.82       860\n",
            "\n",
            "confusion_matrix : [[513 100]\n",
            " [ 62 185]]\n",
            "Validation Accuracy: 0.8121693121693121\n",
            "Train loss: 0.20501367467936973\n",
            "pred_flat: [0 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0]\n",
            "labels: [0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0]\n",
            "labels: [0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1]\n",
            "labels: [1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1]\n",
            "labels: [1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "pred_flat: [1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0]\n",
            "pred_flat: [1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0]\n",
            "pred_flat: [0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0]\n",
            "labels: [0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "pred_flat: [0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0]\n",
            "labels: [0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "pred_flat: [1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1]\n",
            "labels: [1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0]\n",
            "pred_flat: [1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1]\n",
            "labels: [1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0]\n",
            "pred_flat: [0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0]\n",
            "labels: [0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0]\n",
            "pred_flat: [0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0]\n",
            "labels: [0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0]\n",
            "pred_flat: [1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
            "labels: [0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0]\n",
            "pred_flat: [1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
            "labels: [0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0]\n",
            "pred_flat: [0 1 0 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0]\n",
            "labels: [1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0]\n",
            "pred_flat: [0 1 0 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0]\n",
            "labels: [1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 0]\n",
            "labels: [0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0]\n",
            "pred_flat: [0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 0]\n",
            "labels: [0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1]\n",
            "labels: [0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1]\n",
            "labels: [0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1]\n",
            "labels: [1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1]\n",
            "pred_flat: [1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1]\n",
            "labels: [1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0]\n",
            "labels: [0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1]\n",
            "pred_flat: [0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0]\n",
            "labels: [0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1]\n",
            "pred_flat: [1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1]\n",
            "labels: [0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1]\n",
            "pred_flat: [1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1]\n",
            "labels: [0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1]\n",
            "pred_flat: [0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0]\n",
            "labels: [0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "pred_flat: [0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0]\n",
            "labels: [0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0]\n",
            "labels: [0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0]\n",
            "labels: [0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0]\n",
            "pred_flat: [0 0 0 0 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1]\n",
            "labels: [0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1]\n",
            "labels: [0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0]\n",
            "labels: [0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            "pred_flat: [0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0]\n",
            "labels: [0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            "pred_flat: [1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "labels: [0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "labels: [0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1]\n",
            "labels: [1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0]\n",
            "pred_flat: [1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1]\n",
            "labels: [1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0]\n",
            "labels: [0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0]\n",
            "pred_flat: [0 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0]\n",
            "labels: [0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0]\n",
            "pred_flat: [0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1]\n",
            "labels: [0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1]\n",
            "pred_flat: [0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1]\n",
            "labels: [0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1]\n",
            "pred_flat: [0 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  38%|███▊      | 3/8 [29:27<49:05, 589.18s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "pred_flat: [0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1]\n",
            "labels: [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1]\n",
            "labels: [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.84      0.85       588\n",
            "           1       0.67      0.70      0.69       272\n",
            "\n",
            "    accuracy                           0.80       860\n",
            "   macro avg       0.76      0.77      0.77       860\n",
            "weighted avg       0.80      0.80      0.80       860\n",
            "\n",
            "confusion_matrix : [[494  94]\n",
            " [ 81 191]]\n",
            "Validation Accuracy: 0.7966269841269842\n",
            "Train loss: 0.10183893029852478\n",
            "pred_flat: [0 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0]\n",
            "labels: [0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0]\n",
            "labels: [0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1]\n",
            "labels: [1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1]\n",
            "labels: [1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0]\n",
            "labels: [0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0]\n",
            "labels: [0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "pred_flat: [1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0]\n",
            "labels: [0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0]\n",
            "pred_flat: [1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0]\n",
            "labels: [0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0]\n",
            "pred_flat: [0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "pred_flat: [0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "pred_flat: [1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1]\n",
            "labels: [1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0]\n",
            "pred_flat: [1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1]\n",
            "labels: [1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0]\n",
            "pred_flat: [0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 0 0]\n",
            "labels: [0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0]\n",
            "pred_flat: [0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 0 0]\n",
            "labels: [0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0]\n",
            "pred_flat: [1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "labels: [0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0]\n",
            "pred_flat: [1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "labels: [0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0]\n",
            "pred_flat: [0 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0]\n",
            "labels: [1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0]\n",
            "pred_flat: [0 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0]\n",
            "labels: [1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0 0]\n",
            "labels: [0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0]\n",
            "pred_flat: [0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0 0]\n",
            "labels: [0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1]\n",
            "labels: [0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1]\n",
            "labels: [0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0]\n",
            "labels: [1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0]\n",
            "labels: [1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0]\n",
            "labels: [0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0]\n",
            "labels: [0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1]\n",
            "pred_flat: [1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0]\n",
            "labels: [0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1]\n",
            "pred_flat: [1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0]\n",
            "labels: [0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1]\n",
            "pred_flat: [0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "pred_flat: [0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0]\n",
            "labels: [0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0]\n",
            "labels: [0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0]\n",
            "pred_flat: [0 1 0 0 0 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1]\n",
            "labels: [0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 1 0 0 0 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1]\n",
            "labels: [0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 1 1 0 0]\n",
            "labels: [0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            "pred_flat: [0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 1 1 0 0]\n",
            "labels: [0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            "pred_flat: [1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "labels: [0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "labels: [0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1]\n",
            "labels: [1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1]\n",
            "labels: [1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 1 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0]\n",
            "labels: [0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0]\n",
            "pred_flat: [0 0 1 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0]\n",
            "labels: [0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0]\n",
            "pred_flat: [0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            "labels: [0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            "labels: [0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1]\n",
            "labels: [0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1]\n",
            "pred_flat: [0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1]\n",
            "labels: [0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1]\n",
            "pred_flat: [0 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  50%|█████     | 4/8 [39:17<39:18, 589.51s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "pred_flat: [0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 1]\n",
            "labels: [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 1]\n",
            "labels: [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.82      0.85       607\n",
            "           1       0.62      0.70      0.66       253\n",
            "\n",
            "    accuracy                           0.79       860\n",
            "   macro avg       0.75      0.76      0.75       860\n",
            "weighted avg       0.80      0.79      0.79       860\n",
            "\n",
            "confusion_matrix : [[500 107]\n",
            " [ 75 178]]\n",
            "Validation Accuracy: 0.7886904761904762\n",
            "Train loss: 0.058316372510423695\n",
            "pred_flat: [0 1 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0]\n",
            "labels: [0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 1 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0]\n",
            "labels: [0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1]\n",
            "labels: [1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1]\n",
            "labels: [1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "pred_flat: [1 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0]\n",
            "labels: [0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0]\n",
            "pred_flat: [1 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0]\n",
            "labels: [0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0]\n",
            "pred_flat: [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "pred_flat: [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "pred_flat: [1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1]\n",
            "labels: [1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0]\n",
            "pred_flat: [1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1]\n",
            "labels: [1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0]\n",
            "pred_flat: [0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 0 0]\n",
            "labels: [0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0]\n",
            "pred_flat: [0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 0 0]\n",
            "labels: [0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0]\n",
            "pred_flat: [1 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 0 0 1 0]\n",
            "labels: [0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0]\n",
            "pred_flat: [1 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 0 0 1 0]\n",
            "labels: [0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0]\n",
            "pred_flat: [0 1 0 1 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0]\n",
            "labels: [1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0]\n",
            "pred_flat: [0 1 0 1 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0]\n",
            "labels: [1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0 0]\n",
            "labels: [0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0]\n",
            "pred_flat: [0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0 0]\n",
            "labels: [0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1]\n",
            "labels: [0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1]\n",
            "labels: [0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0]\n",
            "labels: [1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1]\n",
            "pred_flat: [1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0]\n",
            "labels: [1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0]\n",
            "labels: [0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0]\n",
            "labels: [0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1]\n",
            "pred_flat: [1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0]\n",
            "labels: [0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1]\n",
            "pred_flat: [1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0]\n",
            "labels: [0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1]\n",
            "pred_flat: [0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0]\n",
            "labels: [0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "pred_flat: [0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0]\n",
            "labels: [0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0]\n",
            "labels: [0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0]\n",
            "labels: [0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0]\n",
            "pred_flat: [0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1]\n",
            "labels: [0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1]\n",
            "labels: [0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 0 0]\n",
            "labels: [0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            "pred_flat: [0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 0 0]\n",
            "labels: [0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            "pred_flat: [1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0]\n",
            "labels: [0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0]\n",
            "labels: [0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1]\n",
            "labels: [1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0]\n",
            "pred_flat: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1]\n",
            "labels: [1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 1 1 1 1 1 0 1 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0]\n",
            "labels: [0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0]\n",
            "pred_flat: [0 1 1 1 1 1 0 1 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0]\n",
            "labels: [0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0]\n",
            "pred_flat: [1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            "labels: [0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0]\n",
            "pred_flat: [1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            "labels: [0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1]\n",
            "labels: [0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1]\n",
            "pred_flat: [0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1]\n",
            "labels: [0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1]\n",
            "pred_flat: [0 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  62%|██████▎   | 5/8 [49:07<29:28, 589.56s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "pred_flat: [0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 1]\n",
            "labels: [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 1]\n",
            "labels: [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.82      0.85       622\n",
            "           1       0.60      0.72      0.65       238\n",
            "\n",
            "    accuracy                           0.79       860\n",
            "   macro avg       0.74      0.77      0.75       860\n",
            "weighted avg       0.81      0.79      0.79       860\n",
            "\n",
            "confusion_matrix : [[508 114]\n",
            " [ 67 171]]\n",
            "Validation Accuracy: 0.7898478835978836\n",
            "Train loss: 0.04790061545796728\n",
            "pred_flat: [0 1 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0]\n",
            "labels: [0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 1 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0]\n",
            "labels: [0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1]\n",
            "labels: [1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1]\n",
            "labels: [1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "pred_flat: [1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0]\n",
            "labels: [0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0]\n",
            "pred_flat: [1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0]\n",
            "labels: [0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0]\n",
            "pred_flat: [0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "pred_flat: [0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "pred_flat: [1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1]\n",
            "labels: [1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0]\n",
            "pred_flat: [1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1]\n",
            "labels: [1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0]\n",
            "pred_flat: [0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 0]\n",
            "labels: [0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0]\n",
            "pred_flat: [0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 0]\n",
            "labels: [0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0]\n",
            "pred_flat: [1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0]\n",
            "labels: [0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0]\n",
            "pred_flat: [1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0]\n",
            "labels: [0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0]\n",
            "pred_flat: [0 1 0 1 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0]\n",
            "labels: [1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0]\n",
            "pred_flat: [0 1 0 1 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0]\n",
            "labels: [1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 1 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0 0]\n",
            "labels: [0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0]\n",
            "pred_flat: [0 0 1 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0 0]\n",
            "labels: [0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1]\n",
            "labels: [0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1]\n",
            "labels: [0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0]\n",
            "labels: [1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1]\n",
            "pred_flat: [1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0]\n",
            "labels: [1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0]\n",
            "labels: [0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0]\n",
            "labels: [0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1]\n",
            "pred_flat: [1 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 0]\n",
            "labels: [0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1]\n",
            "pred_flat: [1 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 0]\n",
            "labels: [0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1]\n",
            "pred_flat: [1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 1 1 1 0]\n",
            "labels: [0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "pred_flat: [1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 1 1 1 0]\n",
            "labels: [0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "pred_flat: [0 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0]\n",
            "labels: [0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0]\n",
            "pred_flat: [0 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0]\n",
            "labels: [0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0]\n",
            "pred_flat: [0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1]\n",
            "labels: [0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1]\n",
            "labels: [0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0]\n",
            "labels: [0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            "pred_flat: [0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0]\n",
            "labels: [0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            "pred_flat: [1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1]\n",
            "labels: [0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1]\n",
            "labels: [0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0]\n",
            "pred_flat: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 1 1 1 1 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0]\n",
            "labels: [0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0]\n",
            "pred_flat: [0 1 1 1 1 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0]\n",
            "labels: [0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0]\n",
            "pred_flat: [1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0]\n",
            "pred_flat: [1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 1]\n",
            "labels: [0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1]\n",
            "pred_flat: [0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 1]\n",
            "labels: [0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1]\n",
            "pred_flat: [0 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  75%|███████▌  | 6/8 [58:57<19:39, 589.64s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "pred_flat: [0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "labels: [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "labels: [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.83      0.83       574\n",
            "           1       0.67      0.66      0.67       286\n",
            "\n",
            "    accuracy                           0.78       860\n",
            "   macro avg       0.75      0.75      0.75       860\n",
            "weighted avg       0.78      0.78      0.78       860\n",
            "\n",
            "confusion_matrix : [[479  95]\n",
            " [ 96 190]]\n",
            "Validation Accuracy: 0.7786044973544973\n",
            "Train loss: 0.0353348590163649\n",
            "pred_flat: [0 0 0 1 1 0 0 1 1 1 0 0 1 1 0 1 1 1 0 0 1 1 0 0 1 1 1 0 0 0 0 0]\n",
            "labels: [0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 1 1 0 0 1 1 1 0 0 1 1 0 1 1 1 0 0 1 1 0 0 1 1 1 0 0 0 0 0]\n",
            "labels: [0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 1 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1]\n",
            "labels: [1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 1 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1]\n",
            "labels: [1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "pred_flat: [1 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0]\n",
            "pred_flat: [1 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0]\n",
            "pred_flat: [0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "pred_flat: [0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "pred_flat: [1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 0 0 1 0 0 0 0 0 1 1]\n",
            "labels: [1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0]\n",
            "pred_flat: [1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 0 0 1 0 0 0 0 0 1 1]\n",
            "labels: [1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0]\n",
            "pred_flat: [0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 0 0]\n",
            "labels: [0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0]\n",
            "pred_flat: [0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 0 0]\n",
            "labels: [0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0]\n",
            "pred_flat: [1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 0 0 1 0]\n",
            "labels: [0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0]\n",
            "pred_flat: [1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 0 0 1 0]\n",
            "labels: [0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0]\n",
            "pred_flat: [0 1 0 1 0 0 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0]\n",
            "labels: [1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0]\n",
            "pred_flat: [0 1 0 1 0 0 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0]\n",
            "labels: [1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 1 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 0]\n",
            "labels: [0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0]\n",
            "pred_flat: [0 0 1 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 0]\n",
            "labels: [0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1]\n",
            "labels: [0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1]\n",
            "labels: [0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0]\n",
            "labels: [1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1]\n",
            "pred_flat: [1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0]\n",
            "labels: [1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0]\n",
            "labels: [0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0]\n",
            "labels: [0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1]\n",
            "pred_flat: [1 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 1 1]\n",
            "labels: [0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1]\n",
            "pred_flat: [1 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 1 1]\n",
            "labels: [0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1]\n",
            "pred_flat: [0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 0]\n",
            "labels: [0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "pred_flat: [0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 0]\n",
            "labels: [0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "pred_flat: [0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0]\n",
            "labels: [0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0]\n",
            "pred_flat: [0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0]\n",
            "labels: [0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0]\n",
            "pred_flat: [0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1]\n",
            "labels: [0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1]\n",
            "labels: [0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0]\n",
            "labels: [0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            "pred_flat: [0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0]\n",
            "labels: [0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            "pred_flat: [1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            "labels: [0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            "labels: [0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1]\n",
            "labels: [1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0]\n",
            "pred_flat: [1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1]\n",
            "labels: [1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 1 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0]\n",
            "labels: [0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0]\n",
            "pred_flat: [0 1 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0]\n",
            "labels: [0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0]\n",
            "pred_flat: [1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0]\n",
            "pred_flat: [1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0]\n",
            "pred_flat: [0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1]\n",
            "labels: [0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1]\n",
            "pred_flat: [0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1]\n",
            "labels: [0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1]\n",
            "pred_flat: [0 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  88%|████████▊ | 7/8 [1:08:47<09:49, 589.69s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "pred_flat: [0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 1]\n",
            "labels: [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 1]\n",
            "labels: [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.84      0.84       582\n",
            "           1       0.67      0.69      0.68       278\n",
            "\n",
            "    accuracy                           0.79       860\n",
            "   macro avg       0.76      0.76      0.76       860\n",
            "weighted avg       0.79      0.79      0.79       860\n",
            "\n",
            "confusion_matrix : [[488  94]\n",
            " [ 87 191]]\n",
            "Validation Accuracy: 0.7898478835978836\n",
            "Train loss: 0.02760725040697371\n",
            "pred_flat: [0 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 1 0 0 0 0 0]\n",
            "labels: [0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 1 0 0 0 0 0]\n",
            "labels: [0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1]\n",
            "labels: [1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1]\n",
            "labels: [1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0]\n",
            "labels: [0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0]\n",
            "labels: [0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "pred_flat: [1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0]\n",
            "pred_flat: [1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0]\n",
            "pred_flat: [0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "pred_flat: [0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "pred_flat: [1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 0 1 1]\n",
            "labels: [1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0]\n",
            "pred_flat: [1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 0 1 1]\n",
            "labels: [1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0]\n",
            "pred_flat: [0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 0 0]\n",
            "labels: [0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0]\n",
            "pred_flat: [0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 0 0]\n",
            "labels: [0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0]\n",
            "pred_flat: [1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "labels: [0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0]\n",
            "pred_flat: [1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "labels: [0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0]\n",
            "pred_flat: [0 1 0 1 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0]\n",
            "labels: [1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0]\n",
            "pred_flat: [0 1 0 1 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0]\n",
            "labels: [1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 1 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1 0 1 1 1 0 0 0]\n",
            "labels: [0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0]\n",
            "pred_flat: [0 0 1 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1 0 1 1 1 0 0 0]\n",
            "labels: [0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1]\n",
            "labels: [0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1]\n",
            "labels: [0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [1 0 0 0 1 0 0 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 0 0 0 0 0 0 1]\n",
            "labels: [1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1]\n",
            "pred_flat: [1 0 0 0 1 0 0 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 0 0 0 0 0 0 1]\n",
            "labels: [1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0]\n",
            "labels: [0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0]\n",
            "labels: [0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1]\n",
            "pred_flat: [1 1 0 1 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0]\n",
            "labels: [0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1]\n",
            "pred_flat: [1 1 0 1 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0]\n",
            "labels: [0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1]\n",
            "pred_flat: [0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 0]\n",
            "labels: [0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "pred_flat: [0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 0]\n",
            "labels: [0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "pred_flat: [0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0]\n",
            "labels: [0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0]\n",
            "pred_flat: [0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0]\n",
            "labels: [0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0]\n",
            "pred_flat: [0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 1]\n",
            "labels: [0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 1]\n",
            "labels: [0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 1 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 0 0]\n",
            "labels: [0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            "pred_flat: [0 1 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 0 0]\n",
            "labels: [0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            "pred_flat: [1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1]\n",
            "labels: [0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1]\n",
            "labels: [0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0]\n",
            "labels: [0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0]\n",
            "pred_flat: [0 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0]\n",
            "labels: [0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0]\n",
            "pred_flat: [1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0]\n",
            "pred_flat: [1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 1 1]\n",
            "labels: [0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1]\n",
            "pred_flat: [0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 1 1]\n",
            "labels: [0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1]\n",
            "pred_flat: [0 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch: 100%|██████████| 8/8 [1:18:36<00:00, 589.46s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "pred_flat: [0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 1]\n",
            "labels: [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 1]\n",
            "labels: [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.84      0.85       590\n",
            "           1       0.66      0.70      0.68       270\n",
            "\n",
            "    accuracy                           0.79       860\n",
            "   macro avg       0.76      0.77      0.76       860\n",
            "weighted avg       0.80      0.79      0.80       860\n",
            "\n",
            "confusion_matrix : [[494  96]\n",
            " [ 81 189]]\n",
            "Validation Accuracy: 0.7944775132275133\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm3-yi60xTUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RISkdM5Fx6Gb",
        "colab_type": "text"
      },
      "source": [
        "#XLNET\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LF9sfJKUx9fh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "outputId": "243c3652-dc80-4736-ef56-f441ea759c0f"
      },
      "source": [
        "!pip install pytorch-transformers\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "from pytorch_transformers import XLNetModel, XLNetTokenizer, XLNetForSequenceClassification\n",
        "from pytorch_transformers import AdamW\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-transformers in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (0.1.85)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2019.12.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (4.28.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.10.40)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.17.4)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.3.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (0.0.35)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (1.24.3)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.40 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (1.13.40)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (1.12.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->pytorch-transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->pytorch-transformers) (2.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9CSF0NDzcaR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "outputId": "1c811c8b-51ef-473f-a7cd-5cf00f962278"
      },
      "source": [
        "#Data pre-processing before BERT\n",
        "tweets_train = X_train.tweet_initial_nontoken.values\n",
        "tweets_test = X_test.tweet_initial_nontoken.values\n",
        "\n",
        "\n",
        "tweets_train = [tweet_initial_nontoken + \" [SEP] [CLS]\" for tweet_initial_nontoken in tweets_train]\n",
        "tweets_test = [ tweet_initial_nontoken + \" [SEP] [CLS]\" for tweet_initial_nontoken in tweets_test]\n",
        "\n",
        "X=tweets_train+tweets_test\n",
        "Y=y_train+y_test\n",
        "\n",
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)\n",
        "\n",
        "tokenized_X = [tokenizer.tokenize(sent) for sent in X]\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_X[0])\n",
        "\n",
        "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_X]\n",
        "\n",
        "# Pad our input tokens\n",
        "MAX_LEN=120\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)\n",
        "\n",
        "#küçük bir hesaplama gerekiyor!! test_size = 860/14100\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, Y, \n",
        "                                                            random_state=2018, test_size=860/14100)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2018, test_size=860/14100)\n",
        "\n",
        "print(train_inputs.shape)\n",
        "print(validation_inputs.shape)\n",
        "print(train_labels)\n",
        "print(validation_labels)\n",
        "\n",
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)\n",
        "\n",
        "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
        "batch_size = 32\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenize the first sentence:\n",
            "['▁user', '▁she', '▁should', '▁ask', '▁a', '▁few', '▁native', '▁', 'american', 's', '▁what', '▁their', '▁take', '▁on', '▁this', '▁is', '▁[', 's', 'ep', ']', '▁[', 'cl', 's', ']']\n",
            "(13240, 120)\n",
            "(860, 120)\n",
            "[1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1]\n",
            "[0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkFNpUWCzfNG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "32ab289e-48a8-4e97-a65e-4ec11e97183b"
      },
      "source": [
        "model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n",
        "model.cuda()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XLNetForSequenceClassification(\n",
              "  (transformer): XLNetModel(\n",
              "    (word_embedding): Embedding(32000, 768)\n",
              "    (layer): ModuleList(\n",
              "      (0): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (3): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (4): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (5): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (6): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (7): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (8): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (9): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (10): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (11): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (sequence_summary): SequenceSummary(\n",
              "    (summary): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "    (first_dropout): Identity()\n",
              "    (last_dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (logits_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEpGQbtPzg0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "\n",
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = AdamW(optimizer_grouped_parameters,\n",
        "                     lr=2e-5,\n",
        "                     )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RuZHyYrzjX0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "74547d1c-21a0-4e4a-ce2d-840c0b3499e3"
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "from sklearn.metrics import confusion_matrix\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    report=classification_report(labels_flat , pred_flat)\n",
        "    \n",
        "    print('pred_flat:',pred_flat)\n",
        "    print('labels:',labels_flat)\n",
        "    \n",
        "\n",
        "\n",
        "    return (np.sum(pred_flat == labels_flat) / len(labels_flat)) ,report\n",
        "def concat(preds,labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    report=classification_report(labels_flat , pred_flat)\n",
        "    \n",
        "    print('pred_flat:',pred_flat)\n",
        "    print('labels:',labels_flat)\n",
        "\n",
        "\n",
        "    return list(labels_flat) ,list(pred_flat)\n",
        "\n",
        "# Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 8\n",
        "# trange is a tqdm wrapper around the normal python range\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "  \n",
        "  \n",
        "  # Training\n",
        "  \n",
        "  # Set our model to training mode (as opposed to evaluation mode)\n",
        "  model.train()\n",
        "  \n",
        "  # Tracking variables\n",
        "  tr_loss = 0\n",
        "  nb_tr_examples, nb_tr_steps = 0, 0\n",
        "  \n",
        "  # Train the data for one epoch\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Clear out the gradients (by default they accumulate)\n",
        "    optimizer.zero_grad()\n",
        "    # Forward pass\n",
        "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "    loss = outputs[0]\n",
        "    logits = outputs[1]\n",
        "    train_loss_set.append(loss.item())    \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    # Update parameters and take a step using the computed gradient\n",
        "    optimizer.step()\n",
        "    \n",
        "    \n",
        "    # Update tracking variables\n",
        "    tr_loss += loss.item()\n",
        "    nb_tr_examples += b_input_ids.size(0)\n",
        "    nb_tr_steps += 1\n",
        "\n",
        "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "    \n",
        "    \n",
        "  # Validation\n",
        "\n",
        "  # Put model in evaluation mode to evaluate loss on the validation set\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  eval_loss, eval_accuracy = 0, 0\n",
        "  nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "  # Evaluate data for one epoch\n",
        "  LABELS=[]\n",
        "  PREDICTION=[]\n",
        "  for batch in validation_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "      logits = output[0]\n",
        "    \n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    tmp_eval_accuracy,report = flat_accuracy(logits, label_ids)    \n",
        "    p,l=concat(logits, label_ids)\n",
        "    LABELS+=l\n",
        "    PREDICTION+=p   \n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "    \n",
        "  report=classification_report(LABELS , PREDICTION)  \n",
        "  print(report)\n",
        "  print('confusion_matrix :', confusion_matrix(LABELS, PREDICTION))\n",
        "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   0%|          | 0/8 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.5215217381859747\n",
            "pred_flat: [0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 0 1 1 0 0 1 1 1 0 0 0 0 1]\n",
            "labels: [0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 0 1 1 0 0 1 1 1 0 0 0 0 1]\n",
            "labels: [0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1]\n",
            "labels: [1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1]\n",
            "labels: [1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0]\n",
            "labels: [0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0]\n",
            "labels: [0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "pred_flat: [0 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0]\n",
            "labels: [0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0]\n",
            "pred_flat: [0 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0]\n",
            "labels: [0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0]\n",
            "pred_flat: [0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0]\n",
            "labels: [0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "pred_flat: [0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0]\n",
            "labels: [0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "pred_flat: [1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0 0 0 1 1]\n",
            "labels: [1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0]\n",
            "pred_flat: [1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0 0 0 1 1]\n",
            "labels: [1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0]\n",
            "pred_flat: [0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 0 0]\n",
            "labels: [0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0]\n",
            "pred_flat: [0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 0 0]\n",
            "labels: [0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0]\n",
            "pred_flat: [1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 1 1]\n",
            "labels: [0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0]\n",
            "pred_flat: [1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 1 1]\n",
            "labels: [0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0]\n",
            "pred_flat: [0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0]\n",
            "labels: [1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0]\n",
            "pred_flat: [0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0]\n",
            "labels: [1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0]\n",
            "labels: [0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0]\n",
            "pred_flat: [0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0]\n",
            "labels: [0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 1]\n",
            "labels: [0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 1]\n",
            "labels: [0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1]\n",
            "labels: [1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1]\n",
            "pred_flat: [1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1]\n",
            "labels: [1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 1 0 1]\n",
            "labels: [0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 1 0 1]\n",
            "labels: [0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1]\n",
            "pred_flat: [1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 1 1 1 0 0 0 1 1]\n",
            "labels: [0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1]\n",
            "pred_flat: [1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 1 1 1 0 0 0 1 1]\n",
            "labels: [0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1]\n",
            "pred_flat: [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 1 1 0]\n",
            "labels: [0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "pred_flat: [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 1 1 0]\n",
            "labels: [0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 1 0]\n",
            "labels: [0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 1 0]\n",
            "labels: [0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0]\n",
            "pred_flat: [0 0 0 0 0 1 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1]\n",
            "labels: [0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 0 1 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1]\n",
            "labels: [0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0]\n",
            "labels: [0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            "pred_flat: [0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0]\n",
            "labels: [0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            "pred_flat: [1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "labels: [0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "labels: [0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "labels: [1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "labels: [1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            "labels: [0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            "labels: [0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0]\n",
            "labels: [0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0]\n",
            "pred_flat: [0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0]\n",
            "labels: [0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0]\n",
            "pred_flat: [0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0]\n",
            "labels: [0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1]\n",
            "pred_flat: [0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0]\n",
            "labels: [0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1]\n",
            "pred_flat: [0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  12%|█▎        | 1/8 [13:00<1:31:01, 780.27s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "pred_flat: [0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "labels: [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "pred_flat: [0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "labels: [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.83      0.84       585\n",
            "           1       0.66      0.68      0.67       275\n",
            "\n",
            "    accuracy                           0.79       860\n",
            "   macro avg       0.75      0.76      0.76       860\n",
            "weighted avg       0.79      0.79      0.79       860\n",
            "\n",
            "confusion_matrix : [[488  97]\n",
            " [ 87 188]]\n",
            "Validation Accuracy: 0.7867063492063492\n",
            "Train loss: 0.41604933175055875\n",
            "pred_flat: [0 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0]\n",
            "labels: [0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0]\n",
            "labels: [0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0]\n",
            "labels: [1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0]\n",
            "labels: [1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "pred_flat: [1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0]\n",
            "labels: [0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0]\n",
            "pred_flat: [1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0]\n",
            "labels: [0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0]\n",
            "pred_flat: [0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0]\n",
            "labels: [0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "pred_flat: [0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0]\n",
            "labels: [0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "pred_flat: [1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1]\n",
            "labels: [1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0]\n",
            "pred_flat: [1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1]\n",
            "labels: [1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0]\n",
            "pred_flat: [0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 0 0]\n",
            "labels: [0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0]\n",
            "pred_flat: [0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 0 0]\n",
            "labels: [0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0]\n",
            "pred_flat: [1 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "labels: [0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0]\n",
            "pred_flat: [1 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "labels: [0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0]\n",
            "pred_flat: [0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0]\n",
            "labels: [1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0]\n",
            "pred_flat: [0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0]\n",
            "labels: [1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 0]\n",
            "labels: [0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0]\n",
            "pred_flat: [0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 0]\n",
            "labels: [0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 1]\n",
            "labels: [0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 1]\n",
            "labels: [0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1]\n",
            "labels: [1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1]\n",
            "labels: [1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1]\n",
            "labels: [0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1]\n",
            "labels: [0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1]\n",
            "pred_flat: [1 1 0 1 1 1 1 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1]\n",
            "labels: [0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1]\n",
            "pred_flat: [1 1 0 1 1 1 1 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1]\n",
            "labels: [0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1]\n",
            "pred_flat: [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 1 1 0]\n",
            "labels: [0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "pred_flat: [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 1 1 0]\n",
            "labels: [0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0]\n",
            "labels: [0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0]\n",
            "labels: [0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0]\n",
            "pred_flat: [0 0 0 0 0 1 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 1 1 1]\n",
            "labels: [0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 0 1 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 1 1 1]\n",
            "labels: [0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0]\n",
            "labels: [0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            "pred_flat: [0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0]\n",
            "labels: [0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            "pred_flat: [1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "labels: [0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "labels: [0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1]\n",
            "labels: [1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1]\n",
            "labels: [1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            "labels: [0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            "labels: [0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 1 1 1 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0]\n",
            "labels: [0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0]\n",
            "pred_flat: [0 1 1 1 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0]\n",
            "labels: [0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0]\n",
            "pred_flat: [0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0]\n",
            "labels: [0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1]\n",
            "pred_flat: [0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0]\n",
            "labels: [0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1]\n",
            "pred_flat: [0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  25%|██▌       | 2/8 [26:00<1:18:01, 780.26s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "pred_flat: [0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "labels: [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "labels: [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.82      0.85       609\n",
            "           1       0.62      0.71      0.66       251\n",
            "\n",
            "    accuracy                           0.79       860\n",
            "   macro avg       0.75      0.77      0.76       860\n",
            "weighted avg       0.80      0.79      0.79       860\n",
            "\n",
            "confusion_matrix : [[502 107]\n",
            " [ 73 178]]\n",
            "Validation Accuracy: 0.7913359788359787\n",
            "Train loss: 0.3637518386046092\n",
            "pred_flat: [0 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1]\n",
            "labels: [0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1]\n",
            "labels: [0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 1 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1]\n",
            "labels: [1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 1 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1]\n",
            "labels: [1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "pred_flat: [1 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 0 1 1 0 0]\n",
            "labels: [0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0]\n",
            "pred_flat: [1 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 0 1 1 0 0]\n",
            "labels: [0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0]\n",
            "pred_flat: [0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 0 0]\n",
            "labels: [0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "pred_flat: [0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 0 0]\n",
            "labels: [0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "pred_flat: [1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 0 0 1 1]\n",
            "labels: [1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0]\n",
            "pred_flat: [1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 0 0 1 1]\n",
            "labels: [1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0]\n",
            "pred_flat: [0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 0 0]\n",
            "labels: [0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0]\n",
            "pred_flat: [0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 0 0]\n",
            "labels: [0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0]\n",
            "pred_flat: [1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 1 1]\n",
            "labels: [0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0]\n",
            "pred_flat: [1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 1 1]\n",
            "labels: [0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0]\n",
            "pred_flat: [0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0]\n",
            "labels: [1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0]\n",
            "pred_flat: [0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0]\n",
            "labels: [1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0]\n",
            "labels: [0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0]\n",
            "pred_flat: [0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0]\n",
            "labels: [0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 1]\n",
            "labels: [0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 1]\n",
            "labels: [0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [1 1 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1]\n",
            "labels: [1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1]\n",
            "labels: [1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 1 0 1]\n",
            "labels: [0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 1 0 1]\n",
            "labels: [0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1]\n",
            "pred_flat: [0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 0 1 0 0 0 1 1]\n",
            "labels: [0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1]\n",
            "pred_flat: [0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 0 1 0 0 0 1 1]\n",
            "labels: [0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1]\n",
            "pred_flat: [0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 1 1 0 0 0 0 1 1 1 0]\n",
            "labels: [0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "pred_flat: [0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 1 1 0 0 0 0 1 1 1 0]\n",
            "labels: [0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 1 1 1 0]\n",
            "labels: [0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 1 1 1 0]\n",
            "labels: [0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0]\n",
            "pred_flat: [0 1 1 0 1 1 1 1 0 0 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 1 1 0 1 1 1]\n",
            "labels: [0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 1 1 0 1 1 1 1 0 0 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 1 1 0 1 1 1]\n",
            "labels: [0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0]\n",
            "labels: [0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            "pred_flat: [0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0]\n",
            "labels: [0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            "pred_flat: [1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1]\n",
            "labels: [0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1]\n",
            "labels: [0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 1]\n",
            "labels: [1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 1]\n",
            "labels: [1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0]\n",
            "labels: [0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0]\n",
            "labels: [0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0]\n",
            "labels: [0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0]\n",
            "pred_flat: [0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0]\n",
            "labels: [0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0]\n",
            "pred_flat: [1 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0]\n",
            "labels: [0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0]\n",
            "pred_flat: [1 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0]\n",
            "labels: [0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1]\n",
            "labels: [0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1]\n",
            "pred_flat: [0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1]\n",
            "labels: [0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1]\n",
            "pred_flat: [0 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  38%|███▊      | 3/8 [39:00<1:05:00, 780.18s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "pred_flat: [0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "labels: [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "labels: [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.87      0.82       520\n",
            "           1       0.76      0.64      0.69       340\n",
            "\n",
            "    accuracy                           0.78       860\n",
            "   macro avg       0.77      0.75      0.76       860\n",
            "weighted avg       0.77      0.78      0.77       860\n",
            "\n",
            "confusion_matrix : [[451  69]\n",
            " [124 216]]\n",
            "Validation Accuracy: 0.7762896825396824\n",
            "Train loss: 0.3034337074428365\n",
            "pred_flat: [0 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0]\n",
            "labels: [0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0]\n",
            "labels: [0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1]\n",
            "labels: [1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1]\n",
            "labels: [1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "pred_flat: [1 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 0 1 1 0 0]\n",
            "labels: [0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0]\n",
            "pred_flat: [1 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 0 1 1 0 0]\n",
            "labels: [0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0]\n",
            "pred_flat: [0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0]\n",
            "labels: [0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "pred_flat: [0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0]\n",
            "labels: [0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "pred_flat: [1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1]\n",
            "labels: [1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0]\n",
            "pred_flat: [1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1]\n",
            "labels: [1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0]\n",
            "pred_flat: [0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 0 1 0 1 0 0]\n",
            "labels: [0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0]\n",
            "pred_flat: [0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 0 1 0 1 0 0]\n",
            "labels: [0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0]\n",
            "pred_flat: [1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1]\n",
            "labels: [0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0]\n",
            "pred_flat: [1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1]\n",
            "labels: [0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0]\n",
            "pred_flat: [0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0]\n",
            "labels: [1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0]\n",
            "pred_flat: [0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0]\n",
            "labels: [1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0]\n",
            "labels: [0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0]\n",
            "pred_flat: [1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0]\n",
            "labels: [0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 1]\n",
            "labels: [0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 1]\n",
            "labels: [0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1]\n",
            "labels: [1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1]\n",
            "labels: [1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 1 0 1]\n",
            "labels: [0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 1 0 1]\n",
            "labels: [0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1]\n",
            "pred_flat: [1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 0 1 0 0 0 0 1]\n",
            "labels: [0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1]\n",
            "pred_flat: [1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 0 1 0 0 0 0 1]\n",
            "labels: [0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1]\n",
            "pred_flat: [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "labels: [0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "pred_flat: [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "labels: [0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "pred_flat: [0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 1 1 1 0]\n",
            "labels: [0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0]\n",
            "pred_flat: [0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 1 1 1 0]\n",
            "labels: [0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0]\n",
            "pred_flat: [0 0 1 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1 0 0 1 1]\n",
            "labels: [0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 1 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1 0 0 1 1]\n",
            "labels: [0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0]\n",
            "labels: [0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            "pred_flat: [0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0]\n",
            "labels: [0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            "pred_flat: [1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1]\n",
            "labels: [0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1]\n",
            "labels: [0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1]\n",
            "labels: [1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1]\n",
            "labels: [1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0]\n",
            "labels: [0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0]\n",
            "labels: [0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 1 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0]\n",
            "labels: [0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0]\n",
            "pred_flat: [0 1 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0]\n",
            "labels: [0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0]\n",
            "pred_flat: [0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 1 1 1]\n",
            "labels: [0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1]\n",
            "pred_flat: [0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 1 1 1]\n",
            "labels: [0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1]\n",
            "pred_flat: [1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  50%|█████     | 4/8 [52:00<52:00, 780.15s/it]  "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "pred_flat: [0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "labels: [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "labels: [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.84      0.82       557\n",
            "           1       0.68      0.64      0.66       303\n",
            "\n",
            "    accuracy                           0.77       860\n",
            "   macro avg       0.75      0.74      0.74       860\n",
            "weighted avg       0.76      0.77      0.77       860\n",
            "\n",
            "confusion_matrix : [[466  91]\n",
            " [109 194]]\n",
            "Validation Accuracy: 0.7681878306878306\n",
            "Train loss: 0.23743077867398948\n",
            "pred_flat: [0 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1]\n",
            "labels: [0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1]\n",
            "labels: [0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1]\n",
            "labels: [1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1]\n",
            "labels: [1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "pred_flat: [1 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 0 1 1 0 0]\n",
            "labels: [0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0]\n",
            "pred_flat: [1 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 0 1 1 0 0]\n",
            "labels: [0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0]\n",
            "pred_flat: [0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0]\n",
            "labels: [0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "pred_flat: [0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0]\n",
            "labels: [0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "pred_flat: [1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0]\n",
            "labels: [1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0]\n",
            "pred_flat: [1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0]\n",
            "labels: [1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0]\n",
            "pred_flat: [0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 0 1 0 1 1 0]\n",
            "labels: [0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0]\n",
            "pred_flat: [0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 0 1 0 1 1 0]\n",
            "labels: [0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0]\n",
            "pred_flat: [1 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "labels: [0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0]\n",
            "pred_flat: [1 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "labels: [0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0]\n",
            "pred_flat: [0 1 0 1 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0]\n",
            "labels: [1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0]\n",
            "pred_flat: [0 1 0 1 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0]\n",
            "labels: [1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0]\n",
            "labels: [0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0]\n",
            "pred_flat: [1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0]\n",
            "labels: [0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 1]\n",
            "labels: [0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 1]\n",
            "labels: [0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0]\n",
            "labels: [1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0]\n",
            "labels: [1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 1 0 1]\n",
            "labels: [0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 1 0 1]\n",
            "labels: [0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1]\n",
            "pred_flat: [1 1 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 1]\n",
            "labels: [0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1]\n",
            "pred_flat: [1 1 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 1]\n",
            "labels: [0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1]\n",
            "pred_flat: [0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 1 1 0 0 0 0 1 1 1 0]\n",
            "labels: [0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "pred_flat: [0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 1 1 0 0 0 0 1 1 1 0]\n",
            "labels: [0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 1 0]\n",
            "labels: [0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 1 0]\n",
            "labels: [0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0]\n",
            "pred_flat: [0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 1 1 0 1 1 1]\n",
            "labels: [0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 1 1 0 1 1 1]\n",
            "labels: [0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 1 1 1 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0]\n",
            "labels: [0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            "pred_flat: [0 1 1 1 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0]\n",
            "labels: [0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            "pred_flat: [1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "labels: [0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "labels: [0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1]\n",
            "labels: [1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1]\n",
            "labels: [1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0]\n",
            "labels: [0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0]\n",
            "labels: [0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 1 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 0]\n",
            "labels: [0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0]\n",
            "pred_flat: [0 1 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 0]\n",
            "labels: [0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0]\n",
            "pred_flat: [0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1]\n",
            "labels: [0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1]\n",
            "pred_flat: [0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1]\n",
            "labels: [0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1]\n",
            "pred_flat: [0 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  62%|██████▎   | 5/8 [1:05:00<39:00, 780.13s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "pred_flat: [0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "labels: [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "labels: [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.85      0.82       541\n",
            "           1       0.71      0.64      0.67       319\n",
            "\n",
            "    accuracy                           0.77       860\n",
            "   macro avg       0.76      0.74      0.75       860\n",
            "weighted avg       0.77      0.77      0.77       860\n",
            "\n",
            "confusion_matrix : [[459  82]\n",
            " [116 203]]\n",
            "Validation Accuracy: 0.7705026455026455\n",
            "Train loss: 0.17129547721683835\n",
            "pred_flat: [0 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0]\n",
            "labels: [0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0]\n",
            "labels: [0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 1 1]\n",
            "labels: [1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 1 1]\n",
            "labels: [1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0]\n",
            "labels: [0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0]\n",
            "labels: [0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "pred_flat: [1 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0]\n",
            "labels: [0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0]\n",
            "pred_flat: [1 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0]\n",
            "labels: [0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0]\n",
            "pred_flat: [0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0]\n",
            "labels: [0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "pred_flat: [0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0]\n",
            "labels: [0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "pred_flat: [1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1]\n",
            "labels: [1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0]\n",
            "pred_flat: [1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1]\n",
            "labels: [1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0]\n",
            "pred_flat: [0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 0 1 0 0 1 0]\n",
            "labels: [0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0]\n",
            "pred_flat: [0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 0 1 0 0 1 0]\n",
            "labels: [0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0]\n",
            "pred_flat: [1 0 1 1 1 0 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0]\n",
            "labels: [0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0]\n",
            "pred_flat: [1 0 1 1 1 0 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0]\n",
            "labels: [0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0]\n",
            "pred_flat: [0 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0]\n",
            "labels: [1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0]\n",
            "pred_flat: [0 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0]\n",
            "labels: [1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0]\n",
            "labels: [0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0]\n",
            "labels: [0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0]\n",
            "labels: [0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0]\n",
            "pred_flat: [0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0]\n",
            "labels: [0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 0 0 1 1 1]\n",
            "labels: [0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 0 0 1 1 1]\n",
            "labels: [0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0]\n",
            "labels: [1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0]\n",
            "labels: [1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 1 0 1]\n",
            "labels: [0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 1 0 1]\n",
            "labels: [0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1]\n",
            "pred_flat: [1 1 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0 0 1 1 0 0 0 0 0 1]\n",
            "labels: [0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1]\n",
            "pred_flat: [1 1 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0 0 1 1 0 0 0 0 0 1]\n",
            "labels: [0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1]\n",
            "pred_flat: [0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "labels: [0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "pred_flat: [0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "labels: [0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "pred_flat: [0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 1 0]\n",
            "labels: [0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0]\n",
            "pred_flat: [0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 1 0]\n",
            "labels: [0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0]\n",
            "pred_flat: [0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 1 1 1 0 1 1 1]\n",
            "labels: [0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 1 1 1 0 1 1 1]\n",
            "labels: [0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0]\n",
            "labels: [0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            "pred_flat: [0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0]\n",
            "labels: [0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            "pred_flat: [0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "labels: [0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "labels: [0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1]\n",
            "labels: [1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1]\n",
            "labels: [1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            "labels: [0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            "labels: [0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 1 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0]\n",
            "labels: [0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0]\n",
            "pred_flat: [0 1 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0]\n",
            "labels: [0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0]\n",
            "pred_flat: [0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1]\n",
            "labels: [0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1]\n",
            "pred_flat: [0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1]\n",
            "labels: [0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1]\n",
            "pred_flat: [1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0]\n",
            "labels: [0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0]\n",
            "labels: [0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  75%|███████▌  | 6/8 [1:18:00<26:00, 780.09s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "pred_flat: [0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "labels: [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "labels: [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.82      0.82       567\n",
            "           1       0.65      0.63      0.64       293\n",
            "\n",
            "    accuracy                           0.76       860\n",
            "   macro avg       0.73      0.73      0.73       860\n",
            "weighted avg       0.76      0.76      0.76       860\n",
            "\n",
            "confusion_matrix : [[467 100]\n",
            " [108 185]]\n",
            "Validation Accuracy: 0.7589285714285714\n",
            "Train loss: 0.12361355615860742\n",
            "pred_flat: [0 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0]\n",
            "labels: [0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0]\n",
            "labels: [0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 1 1]\n",
            "labels: [1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 1 1]\n",
            "labels: [1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "pred_flat: [1 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0]\n",
            "labels: [0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0]\n",
            "pred_flat: [1 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0]\n",
            "labels: [0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0]\n",
            "pred_flat: [0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0]\n",
            "labels: [0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "pred_flat: [0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0]\n",
            "labels: [0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "pred_flat: [1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 0 0 1 0 0 0 0 0 1 1]\n",
            "labels: [1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0]\n",
            "pred_flat: [1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 0 0 1 0 0 0 0 0 1 1]\n",
            "labels: [1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0]\n",
            "pred_flat: [0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 0 1 0 0 1 0]\n",
            "labels: [0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0]\n",
            "pred_flat: [0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 0 1 0 0 1 0]\n",
            "labels: [0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0]\n",
            "pred_flat: [1 0 1 1 1 0 0 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "labels: [0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0]\n",
            "pred_flat: [1 0 1 1 1 0 0 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "labels: [0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0]\n",
            "pred_flat: [0 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0]\n",
            "labels: [1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0]\n",
            "pred_flat: [0 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0]\n",
            "labels: [1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0]\n",
            "pred_flat: [0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0]\n",
            "labels: [0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0]\n",
            "pred_flat: [0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0]\n",
            "labels: [0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 1 1 1]\n",
            "labels: [0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 1 1 1]\n",
            "labels: [0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0]\n",
            "labels: [1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0]\n",
            "labels: [1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 1 0 1]\n",
            "labels: [0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 1 0 1]\n",
            "labels: [0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1]\n",
            "pred_flat: [1 1 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0 0 1 1 0 0 0 0 0 1]\n",
            "labels: [0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1]\n",
            "pred_flat: [1 1 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0 0 1 1 0 0 0 0 0 1]\n",
            "labels: [0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1]\n",
            "pred_flat: [0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "labels: [0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "pred_flat: [0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "labels: [0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0]\n",
            "labels: [0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0]\n",
            "labels: [0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0]\n",
            "pred_flat: [0 0 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 1 1 1]\n",
            "labels: [0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 1 1 1]\n",
            "labels: [0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0]\n",
            "labels: [0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            "pred_flat: [0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0]\n",
            "labels: [0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            "pred_flat: [1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "labels: [0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "labels: [0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1]\n",
            "labels: [1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1]\n",
            "labels: [1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            "labels: [0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            "labels: [0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0]\n",
            "labels: [0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0]\n",
            "pred_flat: [0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0]\n",
            "labels: [0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0]\n",
            "pred_flat: [0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1]\n",
            "labels: [0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1]\n",
            "pred_flat: [0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1]\n",
            "labels: [0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1]\n",
            "pred_flat: [1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  88%|████████▊ | 7/8 [1:31:00<13:00, 780.09s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "pred_flat: [0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "labels: [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "labels: [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.83      0.82       558\n",
            "           1       0.67      0.64      0.65       302\n",
            "\n",
            "    accuracy                           0.76       860\n",
            "   macro avg       0.74      0.73      0.74       860\n",
            "weighted avg       0.76      0.76      0.76       860\n",
            "\n",
            "confusion_matrix : [[465  93]\n",
            " [110 192]]\n",
            "Validation Accuracy: 0.7647156084656084\n",
            "Train loss: 0.09313824945376907\n",
            "pred_flat: [0 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0]\n",
            "labels: [0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0]\n",
            "labels: [0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 1 1]\n",
            "labels: [1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 1 1]\n",
            "labels: [1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0]\n",
            "labels: [0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "pred_flat: [1 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0]\n",
            "pred_flat: [1 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0]\n",
            "pred_flat: [0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0]\n",
            "labels: [0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "pred_flat: [0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0]\n",
            "labels: [0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
            "pred_flat: [1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1]\n",
            "labels: [1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0]\n",
            "pred_flat: [1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1]\n",
            "labels: [1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0]\n",
            "pred_flat: [0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 0 1 0 1 1 0]\n",
            "labels: [0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0]\n",
            "pred_flat: [0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 0 1 0 1 1 0]\n",
            "labels: [0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0]\n",
            "pred_flat: [1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0]\n",
            "labels: [0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0]\n",
            "pred_flat: [1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0]\n",
            "labels: [0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0]\n",
            "pred_flat: [0 1 0 1 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0]\n",
            "labels: [1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0]\n",
            "pred_flat: [0 1 0 1 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0]\n",
            "labels: [1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0]\n",
            "labels: [0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [1 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0]\n",
            "labels: [0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0]\n",
            "pred_flat: [1 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0]\n",
            "labels: [0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 1]\n",
            "labels: [0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 1]\n",
            "labels: [0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0]\n",
            "labels: [1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1]\n",
            "pred_flat: [1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0]\n",
            "labels: [1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1]\n",
            "labels: [0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1]\n",
            "labels: [0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1]\n",
            "pred_flat: [1 1 0 1 1 1 1 0 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 1]\n",
            "labels: [0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1]\n",
            "pred_flat: [1 1 0 1 1 1 1 0 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 1]\n",
            "labels: [0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1]\n",
            "pred_flat: [1 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 1 1 0 0 0 0 1 1 1 0]\n",
            "labels: [0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "pred_flat: [1 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 1 1 0 0 0 0 1 1 1 0]\n",
            "labels: [0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0]\n",
            "pred_flat: [0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 1 0]\n",
            "labels: [0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0]\n",
            "pred_flat: [0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 1 0]\n",
            "labels: [0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0]\n",
            "pred_flat: [0 1 1 0 1 1 1 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1 0 1 1 1]\n",
            "labels: [0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 1 1 0 1 1 1 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1 0 1 1 1]\n",
            "labels: [0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0]\n",
            "labels: [0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            "pred_flat: [0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0]\n",
            "labels: [0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            "pred_flat: [1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "labels: [0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "labels: [0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            "pred_flat: [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 1]\n",
            "labels: [1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 1]\n",
            "labels: [1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1]\n",
            "labels: [0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1]\n",
            "labels: [0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0]\n",
            "pred_flat: [0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0]\n",
            "labels: [0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0]\n",
            "pred_flat: [0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0]\n",
            "labels: [0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0]\n",
            "pred_flat: [0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 0]\n",
            "labels: [0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0]\n",
            "pred_flat: [0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 0]\n",
            "labels: [0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0]\n",
            "pred_flat: [0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1]\n",
            "labels: [0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1]\n",
            "pred_flat: [0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1]\n",
            "labels: [0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1]\n",
            "pred_flat: [1 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 0 0]\n",
            "labels: [0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0]\n",
            "pred_flat: [1 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 0 0]\n",
            "labels: [0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch: 100%|██████████| 8/8 [1:44:00<00:00, 780.10s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "pred_flat: [0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "labels: [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "pred_flat: [0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "labels: [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.82      0.79       542\n",
            "           1       0.66      0.59      0.62       318\n",
            "\n",
            "    accuracy                           0.73       860\n",
            "   macro avg       0.71      0.70      0.71       860\n",
            "weighted avg       0.73      0.73      0.73       860\n",
            "\n",
            "confusion_matrix : [[444  98]\n",
            " [131 187]]\n",
            "Validation Accuracy: 0.734457671957672\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujcOkR1b0rXH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "b7c59dcc-b178-4a1d-9ffd-8c5d9c3bdeaf"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-931765772341>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3GBbthL63g-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJR7G1Se1_se",
        "colab_type": "text"
      },
      "source": [
        "#ELMO\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDGEjC4d2Btn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Elmo\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "url = \"https://tfhub.dev/google/elmo/2\"\n",
        "embed = hub.Module(url)\n",
        "\n",
        "from keras.layers import Input, Lambda, Dense,Layer\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "\n",
        "# Initialize session\n",
        "sess = tf.Session()\n",
        "K.set_session(sess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRTOzE6F2DhI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainx=X_train['tweet_initial_nontoken'].tolist()\n",
        "trainx = [' '.join(t.split()[0:150]) for t in trainx]\n",
        "trainx = np.array(trainx, dtype=object)[:, np.newaxis]\n",
        "\n",
        "testx = X_test['tweet_initial_nontoken'].tolist()\n",
        "testx = [' '.join(t.split()[0:150]) for t in testx]\n",
        "testx = np.array(testx, dtype=object)[:, np.newaxis]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QS6IYQt2F1v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a custom layer that allows us to update weights (lambda layers do not have trainable parameters!)\n",
        "\n",
        "class ElmoEmbeddingLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        self.dimensions = 1024\n",
        "        self.trainable=True\n",
        "        super(ElmoEmbeddingLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.elmo = hub.Module('https://tfhub.dev/google/elmo/2', trainable=self.trainable,\n",
        "                               name=\"{}_module\".format(self.name))\n",
        "\n",
        "      #  self.trainable_weights += K.tf.trainable_variables(scope=\"^{}_module/.*\".format(self.name))\n",
        "      #  super(ElmoEmbeddingLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        result = self.elmo(K.squeeze(K.cast(x, tf.string), axis=1),\n",
        "                      as_dict=True,\n",
        "                      signature='default',\n",
        "                      )['default']\n",
        "        return result\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return K.not_equal(inputs, '--PAD--')\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.dimensions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehTmlMvX2HtD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Function to build model\n",
        "\"\"\"\n",
        "  model.add(Conv1D(128, 3, activation='relu'))\n",
        "  model.add(MaxPooling1D(pool_size=4))\n",
        "  model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "  model.add(layers.Dense(100, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\"\"\"\n",
        "\n",
        "def build_model(): \n",
        "  input_text = layers.Input(shape=(1,), dtype=\"string\")\n",
        "  embedding = ElmoEmbeddingLayer()(input_text)\n",
        "  #(embedding.shape)\n",
        "  #conv#1 = Conv1D(128, kernel_size=3, activation='relu')(K.expand_dims(embedding,0))\n",
        "  #pool1 = MaxPooling1D(pool_size=4)(conv1)\n",
        "  #conv2 = Conv1D(16, kernel_size=1, activation='relu')(pool1)\n",
        "  #pol2 = MaxPooling1D(pool_size=2)(conv2)\n",
        "  #flat = Flatten()(K.get_shape(pool1,-1,1))\n",
        "  #print(\"a\")\n",
        "  dense = layers.Dense(128, activation='relu')(embedding)\n",
        "\n",
        "  pred = layers.Dense(1, activation='sigmoid')(dense)\n",
        "\n",
        "  model = Model(inputs=[input_text], outputs=pred)\n",
        "\n",
        "  #model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  \n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqbELUdr2Nj2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e7e076bc-8872-4f17-8756-fa71e3196339"
      },
      "source": [
        "\n",
        "model_elmo=build_model()\n",
        "early_stopping = [EarlyStopping(monitor='val_acc',min_delta=0,restore_best_weights=True, patience=3,verbose=1, mode='auto')]\n",
        "model_elmo.compile(optimizer=keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False),loss='binary_crossentropy',metrics=['acc',f1_m,precision_m, recall_m])\n",
        "\n",
        "model_elmo.fit(trainx, y_train,\n",
        "            batch_size=16,\n",
        "            epochs=5,\n",
        "            callbacks=early_stopping,\n",
        "            validation_split=0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 1)                 0         \n",
            "_________________________________________________________________\n",
            "elmo_embedding_layer_1 (Elmo (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               131200    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 131,329\n",
            "Trainable params: 131,329\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 11916 samples, validate on 1324 samples\n",
            "Epoch 1/5\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "11916/11916 [==============================] - 198s 17ms/step - loss: 0.5668 - acc: 0.7062 - f1_m: 0.3057 - precision_m: 0.4938 - recall_m: 0.2513 - val_loss: 0.5406 - val_acc: 0.7107 - val_f1_m: 0.2914 - val_precision_m: 0.5571 - val_recall_m: 0.2134\n",
            "Epoch 2/5\n",
            "11916/11916 [==============================] - 177s 15ms/step - loss: 0.5152 - acc: 0.7450 - f1_m: 0.4974 - precision_m: 0.6483 - recall_m: 0.4497 - val_loss: 0.5135 - val_acc: 0.7417 - val_f1_m: 0.5499 - val_precision_m: 0.6346 - val_recall_m: 0.5238\n",
            "Epoch 3/5\n",
            "11916/11916 [==============================] - 177s 15ms/step - loss: 0.5010 - acc: 0.7542 - f1_m: 0.5401 - precision_m: 0.6760 - recall_m: 0.4934 - val_loss: 0.5140 - val_acc: 0.7432 - val_f1_m: 0.4621 - val_precision_m: 0.6434 - val_recall_m: 0.3833\n",
            "Epoch 4/5\n",
            "11916/11916 [==============================] - 174s 15ms/step - loss: 0.4948 - acc: 0.7638 - f1_m: 0.5621 - precision_m: 0.6897 - recall_m: 0.5213 - val_loss: 0.5005 - val_acc: 0.7485 - val_f1_m: 0.5331 - val_precision_m: 0.6472 - val_recall_m: 0.4912\n",
            "Epoch 5/5\n",
            "11916/11916 [==============================] - 172s 14ms/step - loss: 0.4887 - acc: 0.7660 - f1_m: 0.5602 - precision_m: 0.6831 - recall_m: 0.5197 - val_loss: 0.5107 - val_acc: 0.7591 - val_f1_m: 0.5229 - val_precision_m: 0.6867 - val_recall_m: 0.4605\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbeede0ce80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Cn4xY8O2Pch",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "36c18c55-a121-45e6-8ebf-e17fc6e6eb45"
      },
      "source": [
        "loss, accuracy, f1_score, precision, recall = model_elmo.evaluate(testx, y_test, verbose=1)\n",
        "print(\"elmo Test Loss: {:.4f}\".format(loss))\n",
        "print(\"elmo Test Accuracy: {:.4f}\".format(accuracy))\n",
        "print(\"elmo Test f1 score: {:.4f}\".format(f1_score))\n",
        "print(\"elmo  Test Precision: {:.4f}\".format(precision))\n",
        "print(\"elmo BiRNN Test Recall: {:.4f}\".format(recall))\n",
        "model_elmo.save('ElmoModel.h5')\n",
        "\n",
        "probs = model_elmo.predict(testx, verbose=1)\n",
        "print('lenght of probs : ' ,len(probs))\n",
        "  #predicted_classes = probs.argmax(axis=-1)\n",
        "predicted_classes=[0 if i < 0.5 else 1 for i in probs]\n",
        "  #predicted_classes = attention_bilstm_model.predict_classes(X_test_initial, verbose=1)\n",
        "  #print(predicted_classes)\n",
        "\n",
        "print(classification_report(y_test, predicted_classes))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "860/860 [==============================] - 7s 9ms/step\n",
            "elmo Test Loss: 0.4640\n",
            "elmo Test Accuracy: 0.7860\n",
            "elmo Test f1 score: 0.4968\n",
            "elmo  Test Precision: 0.7331\n",
            "elmo BiRNN Test Recall: 0.4123\n",
            "860/860 [==============================] - 7s 8ms/step\n",
            "lenght of probs :  860\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.93      0.86       620\n",
            "           1       0.70      0.41      0.52       240\n",
            "\n",
            "    accuracy                           0.79       860\n",
            "   macro avg       0.75      0.67      0.69       860\n",
            "weighted avg       0.77      0.79      0.77       860\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEFLh0mk6DjW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "8b6ab758-a151-4c14-946f-8937bd16b72a"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "confusion_matrix(y_test,predicted_classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[578,  42],\n",
              "       [142,  98]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwY81L5D6VbU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}