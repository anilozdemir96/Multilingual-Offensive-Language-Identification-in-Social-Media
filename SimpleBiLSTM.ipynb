{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM-offensevalturk",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNAT-9bMUNYn",
        "colab_type": "code",
        "outputId": "8927d1fb-e0f0-4b27-db38-7c18fb72d007",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 882
        }
      },
      "source": [
        "\"\"\"\n",
        "Created on Tue Oct 15 00:14:08 2019\n",
        "\n",
        "@author: anil\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "     \n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "import nltk\n",
        "nltk.download(\"popular\")\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "print(accelerator)\n",
        "import csv\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n",
            "cu80\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnL9D6xPUa8N",
        "colab_type": "text"
      },
      "source": [
        "## Data Reading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wxQ5Kw-w8YP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "c\"\"\"\n",
        "\n",
        "train=pd.read_csv('/content/drive/My Drive/TURKISH-DATA/TRAIN-TURK-ENCODED')\n",
        "test=pd.read_csv('/content/drive/My Drive/TURKISH-DATA/TEST-TURK-ENCODED')\n",
        "\n",
        "X_train_FULL=train[['id','tweet_initial_nontoken','tweet','tweet_initial','subtask_a']]\n",
        "Y_TRAIN_ENCODED_FULL=train['subtask_a']\n",
        "X_test_FULL=test[['id','tweet_initial_nontoken','tweet','tweet_initial','subtask_a']]\n",
        "Y_TEST_ENCODED_FULL=test['subtask_a']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmWvt_RYxUXx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "TEST MODE WİTH DOWNSAMPLING\n",
        "\"\"\"\n",
        "\n",
        "train=pd.read_csv('/content/drive/My Drive/TURKISH-DATA/TRAIN-TURK-ENCODED')\n",
        "test=pd.read_csv('/content/drive/My Drive/TURKISH-DATA/TEST-TURK-ENCODED')\n",
        "\n",
        "X_train=train[['id','tweet_initial_nontoken','tweet','tweet_initial','subtask_a']]\n",
        "y_train_encoded=train['subtask_a']\n",
        "X_test=test[['id','tweet_initial_nontoken','tweet','tweet_initial','subtask_a']]\n",
        "y_test_encoded=test['subtask_a']\n",
        "\n",
        "import csv\n",
        "X_train_FULL=pd.concat([X_train,X_test])\n",
        "X_test_FULL=pd.read_csv('/content/drive/My Drive/TURKISH-DATA/offenseval-tr-testset-v1.tsv',sep='\\t',encoding='utf8',quoting=csv.QUOTE_NONE)\n",
        "Y_TRAIN_ENCODED_FULL=pd.concat([y_train_encoded,y_test_encoded])\n",
        "Y_TEST=pd.read_csv('/content/drive/My Drive/TURKISH-DATA/turkish-goldlabels.tsv',sep=',',encoding='utf8',quoting=csv.QUOTE_NONE,header=None)\n",
        "Y_TEST_ENCODED_FULL = [1 if i ==  'OFF' else 0 for i in Y_TEST[1]]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3l8_Bv6xfIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_TEST_ENCODED_FULL"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffp3fnF2UaSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "Test mode Without downsampling\n",
        "\n",
        "\"\"\"\n",
        "olid_training=pd.read_csv(\"/content/drive/My Drive/OFFENSEVAL20-DATA/offenseval-tr-training-v1.tsv\",sep=\"\\t\")\n",
        "X_train_FULL=olid_training[[\"id\",\"tweet\",\"subtask_a\"]] \n",
        "Y_train_FULL=olid_training[\"subtask_a\"]\n",
        "X_test_FULL=pd.read_csv('/content/drive/My Drive/TURKISH-DATA/offenseval-tr-testset-v1.tsv',sep='\\t',encoding='utf8',quoting=csv.QUOTE_NONE)\n",
        "Y_TEST=pd.read_csv('/content/drive/My Drive/TURKISH-DATA/turkish-goldlabels.tsv',sep=',',encoding='utf8',quoting=csv.QUOTE_NONE,header=None)\n",
        "Y_TRAIN_ENCODED_FULL=[1 if i ==  'OFF' else 0 for i in Y_train_FULL]\n",
        "Y_TEST_ENCODED_FULL = [1 if i ==  'OFF' else 0 for i in Y_TEST[1]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqg5asuBGMgQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"VALID MODE without downsampling\n",
        "\"\"\"\n",
        "from nltk.tokenize import word_tokenize \n",
        "import nltk\n",
        "olid_training=pd.read_csv(\"/content/drive/My Drive/OFFENSEVAL20-DATA/offenseval-tr-training-v1.tsv\",sep=\"\\t\")\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_olid, test_olid = train_test_split(olid_training, test_size=0.20,random_state=11)\n",
        "\n",
        "X_train_FULL=train_olid[[\"id\",\"tweet\",\"subtask_a\"]] \n",
        "Y_train_FULL=train_olid[\"subtask_a\"]\n",
        "X_test_FULL=test_olid[[\"id\",\"tweet\",\"subtask_a\"]]\n",
        "Y_test_FULL=np.array(test_olid[[\"subtask_a\"]])\n",
        "\n",
        "Y_TRAIN_ENCODED_FULL=[1 if i ==  'OFF' else 0 for i in Y_train_FULL]\n",
        "Y_TEST_ENCODED_FULL = [1 if i ==  'OFF' else 0 for i in Y_test_FULL]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQ-8QYQyUiEi",
        "colab_type": "text"
      },
      "source": [
        "## Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pq8Z8T-WUf0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "\t Converting all text to lowercase \n",
        "\t Removing all \"\\#\" symbols\n",
        "\t Removing all punctuation\n",
        "\t Removing @user tokens\n",
        "\"\"\"\n",
        "X_train_FULL.tweet = X_train_FULL.tweet.str.lower()\n",
        "\n",
        "filtered_tweets=[]\n",
        "for tweet in X_train_FULL[\"tweet\"]:\n",
        "    tweet_tokens = word_tokenize(tweet) \n",
        "\n",
        "    filtered_sentence = [w for w in tweet_tokens if (( w!='user' and w!='@' and w!=',' and w!= \"'\" and w!='.'and w!='#' and w!='?'))] \n",
        "      \n",
        "    filtered_tweets.append(filtered_sentence)\n",
        "\n",
        "X_train_FULL[\"tweet_initial\"]=filtered_tweets\n",
        "\n",
        "\n",
        "X_test_FULL.tweet = X_test_FULL.tweet.str.lower()\n",
        "\n",
        "filtered_tweets=[]\n",
        "for tweet in X_test_FULL[\"tweet\"]:\n",
        "    tweet_tokens = word_tokenize(tweet) \n",
        "\n",
        "    filtered_sentence = [w for w in tweet_tokens if (( w!='user' and w!='@' and w!=',' and w!= \"'\" and w!='.'and w!='#' and w!='?'))] \n",
        "      \n",
        "    filtered_tweets.append(filtered_sentence)\n",
        "\n",
        "X_test_FULL[\"tweet_initial\"]=filtered_tweets\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#FOR TEST\n",
        "\n",
        "z=[]\n",
        "for tweet in X_train_FULL[\"tweet_initial\"]:\n",
        "    d=\" \".join(tweet)\n",
        "    z.append(d)\n",
        "X_train_FULL[\"tweet_initial_nontoken\"]=z\n",
        "\n",
        "\n",
        "\n",
        "#FOR TEST\n",
        "\n",
        "z=[]\n",
        "for tweet in X_test_FULL[\"tweet_initial\"]:\n",
        "    d=\" \".join(tweet)\n",
        "    z.append(d)\n",
        "X_test_FULL[\"tweet_initial_nontoken\"]=z\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhV28_hnUtgu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Importing Keras dependencies / Write Custom Evaluation Metrices\n",
        "\"\"\"\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import initializers, regularizers, constraints\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "def recall_m(true_Y, pred_Y):\n",
        "        TP = K.sum(K.round(K.clip(true_Y * pred_Y, 0, 1)))\n",
        "        possible_pos = K.sum(K.round(K.clip(true_Y, 0, 1)))\n",
        "        rec = TP / (possible_pos + K.epsilon())\n",
        "        return rec\n",
        "\n",
        "def precision_m(true_Y, pred_Y):\n",
        "        true_positives = K.sum(K.round(K.clip(true_Y * pred_Y, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(pred_Y, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "def f1_m(true_Y, pred_Y):\n",
        "    pres = precision_m(true_Y, pred_Y)\n",
        "    rec = recall_m(true_Y, pred_Y)\n",
        "    return 2*((pres*rec)/(pres+rec+K.epsilon()))\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8cBxCvTUyql",
        "colab_type": "text"
      },
      "source": [
        "## Tweeter Word2vec /"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAEMEieTU1E7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fname= \"/content/drive/My Drive/Twitter/Word2Vec/w2v_model_word.vec\"\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "word_vectors = gensim.models.KeyedVectors.load_word2vec_format(fname)  # you can load this saved keyedvectors model later\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1_dAlHqRXml",
        "colab_type": "text"
      },
      "source": [
        "## Public Word2vec\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmZ2RW2ERWtj",
        "colab_type": "code",
        "outputId": "3b7b094b-f173-41b3-b588-f1660d981f81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        }
      },
      "source": [
        "\n",
        "#fname='/content/drive/My Drive/Public-Embeddings/word2vec/haber-P1_S1_L0-100V_5E_1547557988.bin.wv.vectors.npy'\n",
        "fname= \"/content/drive/My Drive/Public-Embeddings/word2vec/haber-P1_S1_L0-100V_5E_1547557988.bin\"\n",
        "\n",
        "#fname= '/content/drive/My Drive/OFFENSEVAL20-DATA/haber-P0_S1_L0-100V_5E_v2v.bin'\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "\n",
        "#word_vectors = gensim.models.KeyedVectors.load_word2vec_format(fname,encoding='utf8',binary=True)  # you can load this saved keyedvectors model later\n",
        "\n",
        "#word_vectors=gensim.models.Word2Vec.load(fname)\n",
        "\n",
        "word_vectors=gensim.models.KeyedVectors.load(fname)\n",
        "#models = gensim.models.Word2Vec.load(fname)\n",
        "\n",
        "#gensim.models.KeyedVectors.load_word2vec_format(fname,encoding='utf8')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnpicklingError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-20d53c1c1d32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#word_vectors=gensim.models.Word2Vec.load(fname)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mword_vectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m#models = gensim.models.Word2Vec.load(fname)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseKeyedVectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0;31m# Because of loading from S3 load can't be used (missing readline in smart_open)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '\\x11'."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQ4BtSZbFR9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xljY3PE7oZJ5",
        "colab_type": "text"
      },
      "source": [
        "## Tweeter Fasttext\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvVA5L6eogCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "from gensim.models import FastText\n",
        "\n",
        "word_vectors = gensim.models.FastText.load_fasttext_format('/content/drive/My Drive/Twitter/FastText/fastText_25022020.bin',encoding='utf-8') # use that if you want to use fasttedxt \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQDi-vR_IxcV",
        "colab_type": "text"
      },
      "source": [
        "## Public FastText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uF_tZUUAI2Jo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_vectors = gensim.models.FastText.load_fasttext_format('/content/drive/My Drive/OFFENSEVAL20-DATA/haber-P1_S0_L0.bin',encoding='utf-8') # use that if you want to use fasttedxt \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPLhmD1cVTBH",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizing / creating vocabulary and wordindex using keras functinalities\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5Co9ZwWVSjN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "We will use word indexes as look-up table during embedding layer.\n",
        "\"\"\"\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=98790)  #the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.\n",
        "tokenizer.fit_on_texts(X_train_FULL[\"tweet_initial_nontoken\"])\n",
        "X_train_initial = tokenizer.texts_to_sequences(X_train_FULL[\"tweet_initial_nontoken\"])\n",
        "X_test_initial = tokenizer.texts_to_sequences(X_test_FULL[\"tweet_initial_nontoken\"])\n",
        "vocab_size_initial = len(tokenizer.word_index) + 1 \n",
        "wordIndex_initial=tokenizer.word_index # it is  index\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "max_len = 50\n",
        "\n",
        "\"\"\"\n",
        "Padding\n",
        "\n",
        "\"\"\"\n",
        "X_train_initial = pad_sequences(X_train_initial, padding='post', maxlen=max_len)\n",
        "X_test_initial = pad_sequences(X_test_initial, padding='post', maxlen=max_len)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2yizmkyVVk3",
        "colab_type": "text"
      },
      "source": [
        "Custom Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jv0z0q7xVW72",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \"\"\"\n",
        "Custom Embedding Layer with Word2vec weights\n",
        " \"\"\"\n",
        "\n",
        "def createEmbeddingLayer(wordIndex,not_static):\n",
        "  a=[]\n",
        "  embedding_dim=300\n",
        "  vocabulary_size=len(wordIndex)+1\n",
        "  embedding_matrix = np.zeros((vocabulary_size, embedding_dim))\n",
        "  missed=0\n",
        "  for word, i in wordIndex.items():\n",
        "    \n",
        "          \n",
        "      try:\n",
        "          embedding_vector = word_vectors[word] # or fast text\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "            \n",
        "\n",
        "      except KeyError: # If word is not found in the word2vec vocabulary , assign random weights\n",
        "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),embedding_dim)\n",
        "        missed+=1\n",
        "        a.append(word)\n",
        "\n",
        "  print('missed_words :' , missed)\n",
        "\n",
        "  custom_embedding_layer = Embedding(vocabulary_size,\n",
        "                                embedding_dim,\n",
        "                                weights=[embedding_matrix],\n",
        "                                trainable=not_static )# Controls the updating weights )\n",
        "  return custom_embedding_layer\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8j38UqiVjId",
        "colab_type": "text"
      },
      "source": [
        "## BiLSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2F_zFFfEVl8q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "BiLSTM-Attention Network\n",
        "\n",
        "Benefited from https://github.com/ShawnyXiao/TextClassification-Keras#5-textattbirnn\n",
        "\n",
        "Implementation of https://arxiv.org/abs/1512.08756\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import initializers, regularizers, constraints\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import Embedding, Dense, Dropout, Bidirectional\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCeITLUsVtDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Embedding, Dense, Bidirectional,LSTM\n",
        "\n",
        "def sigmoid(x): \n",
        "    return 1.0/(1 + np.exp(-x))\n",
        "\n",
        "\n",
        "class biLSTM(object):\n",
        "    def __init__(self, maxlen, embedding_dims,\n",
        "                 class_num=1,\n",
        "                 last_activation='sigmoid'):\n",
        "        self.maxlen = maxlen\n",
        "        self.embedding_dims = embedding_dims\n",
        "        self.class_num = class_num\n",
        "        self.last_activation = last_activation\n",
        "\n",
        "    def get_model(self):\n",
        "        input = Input((self.maxlen,))\n",
        "        embedding = createEmbeddingLayer(wordIndex_initial,False)(input) #change\n",
        "        x = Bidirectional(LSTM(128, return_sequences=False))(embedding)  # LSTM or GRU\n",
        "        x = Dense(128, activation='relu')(x)\n",
        "\n",
        "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
        "        model = Model(inputs=input, outputs=output)\n",
        "        return model\n",
        "\n",
        "#Benefited from https://github.com/ShawnyXiao/TextClassification-Keras#5-textattbirnn\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzFQyPNZV15B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(X_train_initial,X_test_initial,y_train,y_test) : \n",
        "  LSTM_MODEL= biLSTM(50, 100).get_model()\n",
        "\n",
        "  early_stopping = [EarlyStopping(monitor='val_loss',min_delta=0,restore_best_weights=True, patience=10,verbose=1, mode='auto')]\n",
        "  LSTM_MODEL.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['acc',f1_m,precision_m, recall_m])\n",
        "  LSTM_MODEL.summary()\n",
        "\n",
        "  LSTM_MODEL.fit(X_train_initial, y_train,\n",
        "            batch_size=16,\n",
        "            epochs=30,\n",
        "            callbacks=early_stopping,\n",
        "            validation_split=0.1)\n",
        "\n",
        "  loss, accuracy, f1_score, precision, recall = LSTM_MODEL.evaluate(X_train_initial, y_train, verbose=1)\n",
        "  print(\" biLSTM Training Loss: {:.4f}\".format(loss))\n",
        "  print(\" biLSTM Accuracy: {:.4f}\".format(accuracy))\n",
        "  print(\"  biLSTM  f1 score: {:.4f}\".format(f1_score))\n",
        "  print(\"Attention- BiRNN Precision: {:.4f}\".format(precision))\n",
        "  print(\"Attention- BiRNN Recall: {:.4f}\".format(recall))\n",
        "\n",
        "  loss, accuracy, f1_score, precision, recall = LSTM_MODEL.evaluate(X_test_initial, y_test, verbose=1)\n",
        "  print(\" biLSTM Test Loss: {:.4f}\".format(loss))\n",
        "  print(\" biLSTM Test Accuracy: {:.4f}\".format(accuracy))\n",
        "  print(\"  biLSTM Test f1 score: {:.4f}\".format(f1_score))\n",
        "  print(\" biLSTM  Test Precision: {:.4f}\".format(precision))\n",
        "  print(\" biLSTM Test Recall: {:.4f}\".format(recall))\n",
        " \n",
        "  probs = LSTM_MODEL.predict(X_test_initial, verbose=1)\n",
        "  print('lenght of probs : ' ,len(probs))\n",
        "  predicted_classes=[0 if i < 0.5 else 1 for i in probs]\n",
        " \n",
        "\n",
        "  print(classification_report(y_test, predicted_classes,digits=3))\n",
        "  return (predicted_classes,probs)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0We6TrTaskjs",
        "colab_type": "code",
        "outputId": "250c4f8d-f115-4b18-dfe0-1946874aa044",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\"\"\"\n",
        " test  MODE WİTHOUT DOWNSAMPLING\n",
        " \"tweeter word2vec\" \n",
        "\"\"\"\n",
        "\n",
        "prediction_attention,probs_attention=train(np.array(X_train_initial),np.array(X_test_initial),np.array(Y_TRAIN_ENCODED_FULL),np.array(Y_TEST_ENCODED_FULL))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "missed_words : 23841\n",
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 50)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_5 (Embedding)      (None, 50, 300)           29637000  \n",
            "_________________________________________________________________\n",
            "bidirectional_4 (Bidirection (None, 256)               439296    \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 30,109,321\n",
            "Trainable params: 472,321\n",
            "Non-trainable params: 29,637,000\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "1760/1760 [==============================] - 16s 9ms/step - loss: 0.3476 - acc: 0.8606 - f1_m: 0.4797 - precision_m: 0.6224 - recall_m: 0.4347 - val_loss: 0.3209 - val_acc: 0.8756 - val_f1_m: 0.5261 - val_precision_m: 0.6887 - val_recall_m: 0.4613\n",
            "Epoch 2/30\n",
            "1760/1760 [==============================] - 15s 9ms/step - loss: 0.2797 - acc: 0.8902 - f1_m: 0.5990 - precision_m: 0.7327 - recall_m: 0.5535 - val_loss: 0.3326 - val_acc: 0.8705 - val_f1_m: 0.5630 - val_precision_m: 0.6629 - val_recall_m: 0.5327\n",
            "Epoch 3/30\n",
            "1760/1760 [==============================] - 15s 9ms/step - loss: 0.2121 - acc: 0.9159 - f1_m: 0.6896 - precision_m: 0.7867 - recall_m: 0.6604 - val_loss: 0.3826 - val_acc: 0.8552 - val_f1_m: 0.5499 - val_precision_m: 0.6215 - val_recall_m: 0.5454\n",
            "Epoch 4/30\n",
            "1760/1760 [==============================] - 15s 9ms/step - loss: 0.1268 - acc: 0.9503 - f1_m: 0.8142 - precision_m: 0.8629 - recall_m: 0.8058 - val_loss: 0.4608 - val_acc: 0.8488 - val_f1_m: 0.5329 - val_precision_m: 0.6149 - val_recall_m: 0.5258\n",
            "Epoch 5/30\n",
            "1760/1760 [==============================] - 15s 9ms/step - loss: 0.0757 - acc: 0.9717 - f1_m: 0.8791 - precision_m: 0.9006 - recall_m: 0.8823 - val_loss: 0.5653 - val_acc: 0.8414 - val_f1_m: 0.5394 - val_precision_m: 0.5969 - val_recall_m: 0.5531\n",
            "Epoch 6/30\n",
            "1760/1760 [==============================] - 15s 9ms/step - loss: 0.0517 - acc: 0.9813 - f1_m: 0.9042 - precision_m: 0.9165 - recall_m: 0.9075 - val_loss: 0.6823 - val_acc: 0.8440 - val_f1_m: 0.5468 - val_precision_m: 0.5900 - val_recall_m: 0.5693\n",
            "Epoch 7/30\n",
            "1760/1760 [==============================] - 15s 9ms/step - loss: 0.0381 - acc: 0.9863 - f1_m: 0.9265 - precision_m: 0.9348 - recall_m: 0.9303 - val_loss: 0.7880 - val_acc: 0.8386 - val_f1_m: 0.4989 - val_precision_m: 0.5593 - val_recall_m: 0.5038\n",
            "Epoch 8/30\n",
            "1760/1760 [==============================] - 15s 9ms/step - loss: 0.0385 - acc: 0.9872 - f1_m: 0.9229 - precision_m: 0.9308 - recall_m: 0.9259 - val_loss: 0.7862 - val_acc: 0.8520 - val_f1_m: 0.5249 - val_precision_m: 0.6153 - val_recall_m: 0.5087\n",
            "Epoch 9/30\n",
            "1760/1760 [==============================] - 15s 9ms/step - loss: 0.0314 - acc: 0.9886 - f1_m: 0.9409 - precision_m: 0.9467 - recall_m: 0.9448 - val_loss: 0.8151 - val_acc: 0.8494 - val_f1_m: 0.5394 - val_precision_m: 0.6094 - val_recall_m: 0.5447\n",
            "Epoch 10/30\n",
            "1760/1760 [==============================] - 15s 9ms/step - loss: 0.0308 - acc: 0.9890 - f1_m: 0.9343 - precision_m: 0.9399 - recall_m: 0.9381 - val_loss: 0.8030 - val_acc: 0.8472 - val_f1_m: 0.5609 - val_precision_m: 0.6213 - val_recall_m: 0.5746\n",
            "Epoch 11/30\n",
            "1757/1760 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9912 - f1_m: 0.9440 - precision_m: 0.9477 - recall_m: 0.9477Restoring model weights from the end of the best epoch.\n",
            "1760/1760 [==============================] - 15s 9ms/step - loss: 0.0252 - acc: 0.9912 - f1_m: 0.9439 - precision_m: 0.9478 - recall_m: 0.9475 - val_loss: 0.8978 - val_acc: 0.8446 - val_f1_m: 0.5424 - val_precision_m: 0.5866 - val_recall_m: 0.5706\n",
            "Epoch 00011: early stopping\n",
            "978/978 [==============================] - 5s 5ms/step - loss: 0.2757 - acc: 0.8923 - f1_m: 0.6232 - precision_m: 0.8500 - recall_m: 0.5220\n",
            " biLSTM Training Loss: 0.2757\n",
            " biLSTM Accuracy: 0.8923\n",
            "  biLSTM  f1 score: 0.6232\n",
            "Attention- BiRNN Precision: 0.8500\n",
            "Attention- BiRNN Recall: 0.5220\n",
            "111/111 [==============================] - 1s 5ms/step - loss: 0.3325 - acc: 0.8645 - f1_m: 0.5420 - precision_m: 0.7890 - recall_m: 0.4429\n",
            " biLSTM Test Loss: 0.3325\n",
            " biLSTM Test Accuracy: 0.8645\n",
            "  biLSTM Test f1 score: 0.5420\n",
            " biLSTM  Test Precision: 0.7890\n",
            " biLSTM Test Recall: 0.4429\n",
            "111/111 [==============================] - 0s 4ms/step\n",
            "lenght of probs :  3528\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.874     0.970     0.919      2812\n",
            "           1      0.793     0.450     0.574       716\n",
            "\n",
            "    accuracy                          0.865      3528\n",
            "   macro avg      0.833     0.710     0.747      3528\n",
            "weighted avg      0.857     0.865     0.849      3528\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgvmNt7Krq_o",
        "colab_type": "code",
        "outputId": "a57b1af2-cfe6-4f03-9cff-e3223211c811",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\"\"\"\n",
        " VALID  MODE WİTHOUT DOWNSAMPLING\n",
        " \"tweeter word2vec\" \n",
        "\"\"\"\n",
        "\n",
        "prediction_attention,probs_attention=train(np.array(X_train_initial),np.array(X_test_initial),np.array(Y_TRAIN_ENCODED_FULL),np.array(Y_TEST_ENCODED_FULL))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "missed_words : 18908\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, 50)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_4 (Embedding)      (None, 50, 300)           25218300  \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 256)               439296    \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 25,690,621\n",
            "Trainable params: 472,321\n",
            "Non-trainable params: 25,218,300\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "1408/1408 [==============================] - 13s 9ms/step - loss: 0.3538 - acc: 0.8572 - f1_m: 0.4637 - precision_m: 0.6040 - recall_m: 0.4178 - val_loss: 0.3062 - val_acc: 0.8825 - val_f1_m: 0.5612 - val_precision_m: 0.7159 - val_recall_m: 0.5080\n",
            "Epoch 2/30\n",
            "1408/1408 [==============================] - 12s 9ms/step - loss: 0.2791 - acc: 0.8897 - f1_m: 0.5889 - precision_m: 0.7193 - recall_m: 0.5447 - val_loss: 0.3153 - val_acc: 0.8761 - val_f1_m: 0.5915 - val_precision_m: 0.6406 - val_recall_m: 0.6002\n",
            "Epoch 3/30\n",
            "1408/1408 [==============================] - 12s 9ms/step - loss: 0.2012 - acc: 0.9190 - f1_m: 0.7018 - precision_m: 0.7940 - recall_m: 0.6721 - val_loss: 0.3556 - val_acc: 0.8646 - val_f1_m: 0.5367 - val_precision_m: 0.6361 - val_recall_m: 0.5141\n",
            "Epoch 4/30\n",
            "1408/1408 [==============================] - 12s 9ms/step - loss: 0.1147 - acc: 0.9561 - f1_m: 0.8366 - precision_m: 0.8769 - recall_m: 0.8328 - val_loss: 0.5083 - val_acc: 0.8646 - val_f1_m: 0.5665 - val_precision_m: 0.6600 - val_recall_m: 0.5520\n",
            "Epoch 5/30\n",
            "1408/1408 [==============================] - 12s 9ms/step - loss: 0.0628 - acc: 0.9769 - f1_m: 0.8970 - precision_m: 0.9128 - recall_m: 0.9005 - val_loss: 0.6159 - val_acc: 0.8586 - val_f1_m: 0.5364 - val_precision_m: 0.6342 - val_recall_m: 0.5204\n",
            "Epoch 6/30\n",
            "1408/1408 [==============================] - 12s 9ms/step - loss: 0.0412 - acc: 0.9857 - f1_m: 0.9203 - precision_m: 0.9281 - recall_m: 0.9243 - val_loss: 0.7269 - val_acc: 0.8570 - val_f1_m: 0.5550 - val_precision_m: 0.6079 - val_recall_m: 0.5726\n",
            "Epoch 7/30\n",
            "1408/1408 [==============================] - 12s 9ms/step - loss: 0.0449 - acc: 0.9844 - f1_m: 0.9235 - precision_m: 0.9339 - recall_m: 0.9270 - val_loss: 0.7464 - val_acc: 0.8402 - val_f1_m: 0.5543 - val_precision_m: 0.5765 - val_recall_m: 0.5982\n",
            "Epoch 8/30\n",
            "1408/1408 [==============================] - 12s 9ms/step - loss: 0.0307 - acc: 0.9885 - f1_m: 0.9231 - precision_m: 0.9287 - recall_m: 0.9277 - val_loss: 0.8979 - val_acc: 0.8454 - val_f1_m: 0.5019 - val_precision_m: 0.5774 - val_recall_m: 0.4991\n",
            "Epoch 9/30\n",
            "1408/1408 [==============================] - 12s 9ms/step - loss: 0.0291 - acc: 0.9903 - f1_m: 0.9465 - precision_m: 0.9511 - recall_m: 0.9493 - val_loss: 0.7982 - val_acc: 0.8526 - val_f1_m: 0.5535 - val_precision_m: 0.5947 - val_recall_m: 0.5738\n",
            "Epoch 10/30\n",
            "1408/1408 [==============================] - 12s 9ms/step - loss: 0.0278 - acc: 0.9895 - f1_m: 0.9376 - precision_m: 0.9446 - recall_m: 0.9402 - val_loss: 0.8217 - val_acc: 0.8586 - val_f1_m: 0.5757 - val_precision_m: 0.6297 - val_recall_m: 0.5886\n",
            "Epoch 11/30\n",
            "1408/1408 [==============================] - ETA: 0s - loss: 0.0242 - acc: 0.9909 - f1_m: 0.9427 - precision_m: 0.9493 - recall_m: 0.9452Restoring model weights from the end of the best epoch.\n",
            "1408/1408 [==============================] - 12s 9ms/step - loss: 0.0242 - acc: 0.9909 - f1_m: 0.9427 - precision_m: 0.9493 - recall_m: 0.9452 - val_loss: 0.9520 - val_acc: 0.8506 - val_f1_m: 0.5516 - val_precision_m: 0.5977 - val_recall_m: 0.5751\n",
            "Epoch 00011: early stopping\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.2759 - acc: 0.8909 - f1_m: 0.6211 - precision_m: 0.8434 - recall_m: 0.5238\n",
            " biLSTM Training Loss: 0.2759\n",
            " biLSTM Accuracy: 0.8909\n",
            "  biLSTM  f1 score: 0.6211\n",
            "Attention- BiRNN Precision: 0.8434\n",
            "Attention- BiRNN Recall: 0.5238\n",
            "196/196 [==============================] - 1s 5ms/step - loss: 0.3407 - acc: 0.8668 - f1_m: 0.5004 - precision_m: 0.7657 - recall_m: 0.3979\n",
            " biLSTM Test Loss: 0.3407\n",
            " biLSTM Test Accuracy: 0.8668\n",
            "  biLSTM Test f1 score: 0.5004\n",
            " biLSTM  Test Precision: 0.7657\n",
            " biLSTM Test Recall: 0.3979\n",
            "196/196 [==============================] - 1s 4ms/step\n",
            "lenght of probs :  6256\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.872     0.979     0.922      5035\n",
            "           1      0.821     0.406     0.544      1221\n",
            "\n",
            "    accuracy                          0.867      6256\n",
            "   macro avg      0.846     0.692     0.733      6256\n",
            "weighted avg      0.862     0.867     0.848      6256\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPQGfyMcqqfK",
        "colab_type": "code",
        "outputId": "e1f5b8d1-9aa3-4886-cf11-7540844f726b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\"\"\"\n",
        " TEST MODE WİTH DOWNSAMPLING\n",
        " \"tweeter word2vec\" \n",
        "\"\"\"\n",
        "\n",
        "prediction_attention,probs_attention=train(np.array(X_train_initial),np.array(X_test_initial),np.array(Y_TRAIN_ENCODED_FULL),np.array(Y_TEST_ENCODED_FULL))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "missed_words : 17612\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 50)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_3 (Embedding)      (None, 50, 300)           24060600  \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 256)               439296    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 24,532,921\n",
            "Trainable params: 472,321\n",
            "Non-trainable params: 24,060,600\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "1300/1300 [==============================] - 12s 9ms/step - loss: 0.4118 - acc: 0.8211 - f1_m: 0.5690 - precision_m: 0.6998 - recall_m: 0.5307 - val_loss: 0.3198 - val_acc: 0.8727 - val_f1_m: 0.4754 - val_precision_m: 0.5974 - val_recall_m: 0.4413\n",
            "Epoch 2/30\n",
            "1300/1300 [==============================] - 11s 9ms/step - loss: 0.3212 - acc: 0.8660 - f1_m: 0.6939 - precision_m: 0.7991 - recall_m: 0.6591 - val_loss: 0.3335 - val_acc: 0.8667 - val_f1_m: 0.5401 - val_precision_m: 0.5926 - val_recall_m: 0.5636\n",
            "Epoch 3/30\n",
            "1300/1300 [==============================] - 11s 9ms/step - loss: 0.2286 - acc: 0.9057 - f1_m: 0.7857 - precision_m: 0.8485 - recall_m: 0.7729 - val_loss: 0.4225 - val_acc: 0.8307 - val_f1_m: 0.5178 - val_precision_m: 0.4973 - val_recall_m: 0.6154\n",
            "Epoch 4/30\n",
            "1300/1300 [==============================] - 11s 9ms/step - loss: 0.1277 - acc: 0.9508 - f1_m: 0.8841 - precision_m: 0.9083 - recall_m: 0.8858 - val_loss: 0.4903 - val_acc: 0.8528 - val_f1_m: 0.5152 - val_precision_m: 0.5476 - val_recall_m: 0.5590\n",
            "Epoch 5/30\n",
            "1300/1300 [==============================] - 11s 9ms/step - loss: 0.0673 - acc: 0.9750 - f1_m: 0.9368 - precision_m: 0.9482 - recall_m: 0.9383 - val_loss: 0.7492 - val_acc: 0.8113 - val_f1_m: 0.4840 - val_precision_m: 0.4467 - val_recall_m: 0.5987\n",
            "Epoch 6/30\n",
            "1300/1300 [==============================] - 11s 9ms/step - loss: 0.0505 - acc: 0.9818 - f1_m: 0.9558 - precision_m: 0.9617 - recall_m: 0.9592 - val_loss: 0.7762 - val_acc: 0.8420 - val_f1_m: 0.4899 - val_precision_m: 0.5064 - val_recall_m: 0.5352\n",
            "Epoch 7/30\n",
            "1300/1300 [==============================] - 11s 9ms/step - loss: 0.0464 - acc: 0.9831 - f1_m: 0.9594 - precision_m: 0.9652 - recall_m: 0.9621 - val_loss: 0.8504 - val_acc: 0.8346 - val_f1_m: 0.4763 - val_precision_m: 0.4991 - val_recall_m: 0.5313\n",
            "Epoch 8/30\n",
            "1300/1300 [==============================] - 11s 9ms/step - loss: 0.0321 - acc: 0.9887 - f1_m: 0.9718 - precision_m: 0.9761 - recall_m: 0.9747 - val_loss: 0.9510 - val_acc: 0.8299 - val_f1_m: 0.4906 - val_precision_m: 0.4928 - val_recall_m: 0.5570\n",
            "Epoch 9/30\n",
            "1300/1300 [==============================] - 11s 9ms/step - loss: 0.0345 - acc: 0.9880 - f1_m: 0.9664 - precision_m: 0.9696 - recall_m: 0.9693 - val_loss: 0.9240 - val_acc: 0.8152 - val_f1_m: 0.4812 - val_precision_m: 0.4688 - val_recall_m: 0.5664\n",
            "Epoch 10/30\n",
            "1300/1300 [==============================] - 11s 9ms/step - loss: 0.0315 - acc: 0.9881 - f1_m: 0.9686 - precision_m: 0.9729 - recall_m: 0.9713 - val_loss: 0.9186 - val_acc: 0.8307 - val_f1_m: 0.4570 - val_precision_m: 0.4782 - val_recall_m: 0.5077\n",
            "Epoch 11/30\n",
            "1298/1300 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9903 - f1_m: 0.9740 - precision_m: 0.9770 - recall_m: 0.9758Restoring model weights from the end of the best epoch.\n",
            "1300/1300 [==============================] - 11s 9ms/step - loss: 0.0276 - acc: 0.9902 - f1_m: 0.9740 - precision_m: 0.9771 - recall_m: 0.9757 - val_loss: 1.0570 - val_acc: 0.8268 - val_f1_m: 0.4904 - val_precision_m: 0.4783 - val_recall_m: 0.5784\n",
            "Epoch 00011: early stopping\n",
            "722/722 [==============================] - 3s 5ms/step - loss: 0.3253 - acc: 0.8632 - f1_m: 0.6502 - precision_m: 0.8733 - recall_m: 0.5414\n",
            "Attention- BiRNN Training Loss: 0.3253\n",
            "Attention- BiRNN Accuracy: 0.8632\n",
            "Attention- BiRNN f1 score: 0.6502\n",
            "Attention- BiRNN Precision: 0.8733\n",
            "Attention- BiRNN Recall: 0.5414\n",
            "111/111 [==============================] - 1s 5ms/step - loss: 0.3388 - acc: 0.8645 - f1_m: 0.5457 - precision_m: 0.7762 - recall_m: 0.4497\n",
            "Attention- BiRNN Test Loss: 0.3388\n",
            "Attention- BiRNN Test Accuracy: 0.8645\n",
            "Attention- BiRNN Test f1 score: 0.5457\n",
            "Attention- BiRNN Test Precision: 0.7762\n",
            "Attention- BiRNN Test Recall: 0.4497\n",
            "111/111 [==============================] - 0s 4ms/step\n",
            "lenght of probs :  3528\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.875     0.969     0.919      2812\n",
            "           1      0.787     0.455     0.577       716\n",
            "\n",
            "    accuracy                          0.865      3528\n",
            "   macro avg      0.831     0.712     0.748      3528\n",
            "weighted avg      0.857     0.865     0.850      3528\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYycE547o3P6",
        "colab_type": "code",
        "outputId": "80492c1c-92dd-4c82-b825-df57a5facb78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\"\"\"\n",
        " VALID MODE WİTH DOWNSAMPLING\n",
        " \"tweeter word2vec\" \n",
        "\"\"\"\n",
        "\n",
        "prediction_attention,probs_attention=train(np.array(X_train_initial),np.array(X_test_initial),np.array(Y_TRAIN_ENCODED_FULL),np.array(Y_TEST_ENCODED_FULL))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "missed_words : 13202\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 50)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 50, 300)           19428000  \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 256)               439296    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 19,900,321\n",
            "Trainable params: 472,321\n",
            "Non-trainable params: 19,428,000\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "948/948 [==============================] - 9s 10ms/step - loss: 0.4299 - acc: 0.8104 - f1_m: 0.5809 - precision_m: 0.7015 - recall_m: 0.5474 - val_loss: 0.4015 - val_acc: 0.8273 - val_f1_m: 0.6294 - val_precision_m: 0.7379 - val_recall_m: 0.5984\n",
            "Epoch 2/30\n",
            "948/948 [==============================] - 8s 9ms/step - loss: 0.3243 - acc: 0.8630 - f1_m: 0.7128 - precision_m: 0.7965 - recall_m: 0.6898 - val_loss: 0.4333 - val_acc: 0.8249 - val_f1_m: 0.6345 - val_precision_m: 0.7345 - val_recall_m: 0.6125\n",
            "Epoch 3/30\n",
            "948/948 [==============================] - 8s 9ms/step - loss: 0.2303 - acc: 0.9030 - f1_m: 0.7994 - precision_m: 0.8561 - recall_m: 0.7881 - val_loss: 0.4878 - val_acc: 0.8136 - val_f1_m: 0.6484 - val_precision_m: 0.6518 - val_recall_m: 0.6979\n",
            "Epoch 4/30\n",
            "948/948 [==============================] - 8s 9ms/step - loss: 0.1308 - acc: 0.9486 - f1_m: 0.8956 - precision_m: 0.9233 - recall_m: 0.8934 - val_loss: 0.7357 - val_acc: 0.8154 - val_f1_m: 0.6027 - val_precision_m: 0.7209 - val_recall_m: 0.5738\n",
            "Epoch 5/30\n",
            "948/948 [==============================] - 8s 9ms/step - loss: 0.0760 - acc: 0.9718 - f1_m: 0.9382 - precision_m: 0.9500 - recall_m: 0.9401 - val_loss: 0.8187 - val_acc: 0.7935 - val_f1_m: 0.6136 - val_precision_m: 0.6244 - val_recall_m: 0.6545\n",
            "Epoch 6/30\n",
            "948/948 [==============================] - 8s 9ms/step - loss: 0.0444 - acc: 0.9847 - f1_m: 0.9663 - precision_m: 0.9711 - recall_m: 0.9689 - val_loss: 0.8500 - val_acc: 0.8071 - val_f1_m: 0.6150 - val_precision_m: 0.6899 - val_recall_m: 0.6032\n",
            "Epoch 7/30\n",
            "948/948 [==============================] - 8s 9ms/step - loss: 0.0365 - acc: 0.9869 - f1_m: 0.9665 - precision_m: 0.9698 - recall_m: 0.9693 - val_loss: 0.9873 - val_acc: 0.7935 - val_f1_m: 0.6287 - val_precision_m: 0.6352 - val_recall_m: 0.6817\n",
            "Epoch 8/30\n",
            "948/948 [==============================] - 8s 9ms/step - loss: 0.0362 - acc: 0.9871 - f1_m: 0.9697 - precision_m: 0.9724 - recall_m: 0.9728 - val_loss: 1.1706 - val_acc: 0.8083 - val_f1_m: 0.5974 - val_precision_m: 0.6811 - val_recall_m: 0.5864\n",
            "Epoch 9/30\n",
            "948/948 [==============================] - 8s 9ms/step - loss: 0.0308 - acc: 0.9891 - f1_m: 0.9739 - precision_m: 0.9758 - recall_m: 0.9769 - val_loss: 1.1132 - val_acc: 0.7976 - val_f1_m: 0.6032 - val_precision_m: 0.6523 - val_recall_m: 0.6224\n",
            "Epoch 10/30\n",
            "948/948 [==============================] - 8s 9ms/step - loss: 0.0250 - acc: 0.9916 - f1_m: 0.9770 - precision_m: 0.9804 - recall_m: 0.9785 - val_loss: 1.1449 - val_acc: 0.7988 - val_f1_m: 0.6053 - val_precision_m: 0.6579 - val_recall_m: 0.6173\n",
            "Epoch 11/30\n",
            "943/948 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9904 - f1_m: 0.9801 - precision_m: 0.9829 - recall_m: 0.9823Restoring model weights from the end of the best epoch.\n",
            "948/948 [==============================] - 8s 9ms/step - loss: 0.0268 - acc: 0.9904 - f1_m: 0.9802 - precision_m: 0.9830 - recall_m: 0.9824 - val_loss: 1.2693 - val_acc: 0.7923 - val_f1_m: 0.6059 - val_precision_m: 0.6486 - val_recall_m: 0.6259\n",
            "Epoch 00011: early stopping\n",
            "527/527 [==============================] - 2s 5ms/step - loss: 0.3279 - acc: 0.8672 - f1_m: 0.7442 - precision_m: 0.8158 - recall_m: 0.7067\n",
            "Attention- BiRNN Training Loss: 0.3279\n",
            "Attention- BiRNN Accuracy: 0.8672\n",
            "Attention- BiRNN f1 score: 0.7442\n",
            "Attention- BiRNN Precision: 0.8158\n",
            "Attention- BiRNN Recall: 0.7067\n",
            "196/196 [==============================] - 1s 5ms/step - loss: 0.3559 - acc: 0.8565 - f1_m: 0.5694 - precision_m: 0.6458 - recall_m: 0.5483\n",
            "Attention- BiRNN Test Loss: 0.3559\n",
            "Attention- BiRNN Test Accuracy: 0.8565\n",
            "Attention- BiRNN Test f1 score: 0.5694\n",
            "Attention- BiRNN Test Precision: 0.6458\n",
            "Attention- BiRNN Test Recall: 0.5483\n",
            "196/196 [==============================] - 1s 4ms/step\n",
            "lenght of probs :  6251\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.899     0.927     0.913      5057\n",
            "           1      0.644     0.557     0.597      1194\n",
            "\n",
            "    accuracy                          0.857      6251\n",
            "   macro avg      0.771     0.742     0.755      6251\n",
            "weighted avg      0.850     0.857     0.852      6251\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEx0bRUNz8MN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bilstmatt_test_full_result= pd.DataFrame()\n",
        "bilstmatt_test_full_result['biLSTM-ATTENTION-PREDICTON']=prediction_attention\n",
        "bilstmatt_test_full_result['biLSTM-ATTENTION-PROBS']=probs_attention\n",
        "bilstmatt_test_full_result['Gold-label']=Y_TEST_ENCODED_FULL"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vHbqgJU1t47",
        "colab_type": "code",
        "outputId": "220c9e04-eb91-4159-eb04-f42d2c9a681a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "source": [
        "bilstmatt_test_full_result"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>biLSTM-ATTENTION-PREDICTON</th>\n",
              "      <th>biLSTM-ATTENTION-PROBS</th>\n",
              "      <th>Gold-label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.022655</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0.125425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0.069387</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0.402815</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0.570750</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3523</th>\n",
              "      <td>0</td>\n",
              "      <td>0.018026</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3524</th>\n",
              "      <td>0</td>\n",
              "      <td>0.006181</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3525</th>\n",
              "      <td>0</td>\n",
              "      <td>0.014974</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3526</th>\n",
              "      <td>0</td>\n",
              "      <td>0.037331</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3527</th>\n",
              "      <td>0</td>\n",
              "      <td>0.020106</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3528 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      biLSTM-ATTENTION-PREDICTON  biLSTM-ATTENTION-PROBS  Gold-label\n",
              "0                              0                0.022655           0\n",
              "1                              0                0.125425           0\n",
              "2                              0                0.069387           0\n",
              "3                              0                0.402815           1\n",
              "4                              1                0.570750           1\n",
              "...                          ...                     ...         ...\n",
              "3523                           0                0.018026           0\n",
              "3524                           0                0.006181           0\n",
              "3525                           0                0.014974           0\n",
              "3526                           0                0.037331           0\n",
              "3527                           0                0.020106           0\n",
              "\n",
              "[3528 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzLos_6k1ftp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bilstmatt_test_full_result.to_csv('bilstmatt_test_full_result.csv',header=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9lEVjsr3RO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bilstmatt_valid_full_result= pd.DataFrame()\n",
        "bilstmatt_valid_full_result['biLSTM-ATTENTION-PREDICTON']=prediction_attention\n",
        "bilstmatt_valid_full_result['biLSTM-ATTENTION-PROBS']=probs_attention\n",
        "bilstmatt_valid_full_result['Gold-label']=Y_TEST_ENCODED_FULL\n",
        "bilstmatt_valid_full_result.to_csv('bilstmatt_valid_full_result.csv',header=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhaSwLqVHMFs",
        "colab_type": "code",
        "outputId": "d3e518b6-eb4f-410d-b72d-105bf3475ab4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "source": [
        "bilstmatt_valid_full_result"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>biLSTM-ATTENTION-PREDICTON</th>\n",
              "      <th>biLSTM-ATTENTION-PROBS</th>\n",
              "      <th>Gold-label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.082957</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0.070353</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0.036576</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0.055747</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0.031174</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6251</th>\n",
              "      <td>0</td>\n",
              "      <td>0.045234</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6252</th>\n",
              "      <td>0</td>\n",
              "      <td>0.309828</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6253</th>\n",
              "      <td>0</td>\n",
              "      <td>0.022087</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6254</th>\n",
              "      <td>0</td>\n",
              "      <td>0.123876</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6255</th>\n",
              "      <td>0</td>\n",
              "      <td>0.055159</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6256 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      biLSTM-ATTENTION-PREDICTON  biLSTM-ATTENTION-PROBS  Gold-label\n",
              "0                              0                0.082957           0\n",
              "1                              0                0.070353           0\n",
              "2                              0                0.036576           0\n",
              "3                              0                0.055747           0\n",
              "4                              0                0.031174           0\n",
              "...                          ...                     ...         ...\n",
              "6251                           0                0.045234           0\n",
              "6252                           0                0.309828           0\n",
              "6253                           0                0.022087           0\n",
              "6254                           0                0.123876           1\n",
              "6255                           0                0.055159           1\n",
              "\n",
              "[6256 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZ83wVJqH429",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}