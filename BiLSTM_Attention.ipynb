{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BiLSTM-Attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNAT-9bMUNYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Created on Tue Oct 15 00:14:08 2019\n",
        "\n",
        "@author: anil\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "     \n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "import nltk\n",
        "nltk.download(\"popular\")\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "print(accelerator)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnL9D6xPUa8N",
        "colab_type": "text"
      },
      "source": [
        "## Data Reading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffp3fnF2UaSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "Test mode \n",
        "\n",
        "\"\"\"\n",
        "olid_training=pd.read_csv(\"/content/drive/My Drive/OFFENSEVAL20-DATA/offenseval-tr-training-v1.tsv\",sep=\"\\t\")\n",
        "X_train_FULL=olid_training[[\"id\",\"tweet\",\"subtask_a\"]] \n",
        "Y_train_FULL=olid_training[\"subtask_a\"]\n",
        "X_test_FULL=pd.read_csv('/content/drive/My Drive/TURKISH-DATA/offenseval-tr-testset-v1.tsv',sep='\\t',encoding='utf8',quoting=csv.QUOTE_NONE)\n",
        "Y_TEST=pd.read_csv('/content/drive/My Drive/TURKISH-DATA/turkish-goldlabels.tsv',sep=',',encoding='utf8',quoting=csv.QUOTE_NONE,header=None)\n",
        "Y_TRAIN_ENCODED_FULL=[1 if i ==  'OFF' else 0 for i in Y_train_FULL]\n",
        "Y_TEST_ENCODED_FULL = [1 if i ==  'OFF' else 0 for i in Y_TEST[1]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQ-8QYQyUiEi",
        "colab_type": "text"
      },
      "source": [
        "## Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pq8Z8T-WUf0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "\t Converting all text to lowercase \n",
        "\t Removing all \"\\#\" symbols\n",
        "\t Removing all punctuation\n",
        "\t Removing @user tokens\n",
        "\"\"\"\n",
        "X_train_FULL.tweet = X_train_FULL.tweet.str.lower()\n",
        "\n",
        "filtered_tweets=[]\n",
        "for tweet in X_train_FULL[\"tweet\"]:\n",
        "    tweet_tokens = word_tokenize(tweet) \n",
        "\n",
        "    filtered_sentence = [w for w in tweet_tokens if (( w!='user' and w!='@' and w!=',' and w!= \"'\" and w!='.'and w!='#' and w!='?'))] \n",
        "      \n",
        "    filtered_tweets.append(filtered_sentence)\n",
        "\n",
        "X_train_FULL[\"tweet_initial\"]=filtered_tweets\n",
        "\n",
        "\n",
        "X_test_FULL.tweet = X_test_FULL.tweet.str.lower()\n",
        "\n",
        "filtered_tweets=[]\n",
        "for tweet in X_test_FULL[\"tweet\"]:\n",
        "    tweet_tokens = word_tokenize(tweet) \n",
        "\n",
        "    filtered_sentence = [w for w in tweet_tokens if (( w!='user' and w!='@' and w!=',' and w!= \"'\" and w!='.'and w!='#' and w!='?'))] \n",
        "      \n",
        "    filtered_tweets.append(filtered_sentence)\n",
        "\n",
        "X_test_FULL[\"tweet_initial\"]=filtered_tweets\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#FOR TEST\n",
        "\n",
        "z=[]\n",
        "for tweet in X_train_FULL[\"tweet_initial\"]:\n",
        "    d=\" \".join(tweet)\n",
        "    z.append(d)\n",
        "X_train_FULL[\"tweet_initial_nontoken\"]=z\n",
        "\n",
        "\n",
        "\n",
        "#FOR TEST\n",
        "\n",
        "z=[]\n",
        "for tweet in X_test_FULL[\"tweet_initial\"]:\n",
        "    d=\" \".join(tweet)\n",
        "    z.append(d)\n",
        "X_test_FULL[\"tweet_initial_nontoken\"]=z\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhV28_hnUtgu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Importing Keras dependencies / Write Custom Evaluation Metrices\n",
        "\"\"\"\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import initializers, regularizers, constraints\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "def recall_m(true_Y, pred_Y):\n",
        "        TP = K.sum(K.round(K.clip(true_Y * pred_Y, 0, 1)))\n",
        "        possible_pos = K.sum(K.round(K.clip(true_Y, 0, 1)))\n",
        "        rec = TP / (possible_pos + K.epsilon())\n",
        "        return rec\n",
        "\n",
        "def precision_m(true_Y, pred_Y):\n",
        "        true_positives = K.sum(K.round(K.clip(true_Y * pred_Y, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(pred_Y, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "def f1_m(true_Y, pred_Y):\n",
        "    pres = precision_m(true_Y, pred_Y)\n",
        "    rec = recall_m(true_Y, pred_Y)\n",
        "    return 2*((pres*rec)/(pres+rec+K.epsilon()))\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8cBxCvTUyql",
        "colab_type": "text"
      },
      "source": [
        "## Word2vec /"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAEMEieTU1E7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fname= \"/content/drive/My Drive/Twitter/Word2Vec/w2v_model_word.vec\"\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "word_vectors = gensim.models.KeyedVectors.load_word2vec_format(fname)  # you can load this saved keyedvectors model later\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPLhmD1cVTBH",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizing / creating vocabulary and wordindex using keras functinalities\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5Co9ZwWVSjN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "We will use word indexes as look-up table during embedding layer.\n",
        "\"\"\"\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=80202)  #the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.\n",
        "tokenizer.fit_on_texts(X_train_FULL[\"tweet_initial_nontoken\"])\n",
        "X_train_initial = tokenizer.texts_to_sequences(X_train_FULL[\"tweet_initial_nontoken\"])\n",
        "X_test_initial = tokenizer.texts_to_sequences(X_test_FULL[\"tweet_initial_nontoken\"])\n",
        "vocab_size_initial = len(tokenizer.word_index) + 1 \n",
        "wordIndex_initial=tokenizer.word_index # it is  index\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "max_len = 50\n",
        "\n",
        "\"\"\"\n",
        "Padding\n",
        "\n",
        "\"\"\"\n",
        "X_train_initial = pad_sequences(X_train_initial, padding='post', maxlen=max_len)\n",
        "X_test_initial = pad_sequences(X_test_initial, padding='post', maxlen=max_len)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2yizmkyVVk3",
        "colab_type": "text"
      },
      "source": [
        "Custom Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jv0z0q7xVW72",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \"\"\"\n",
        "Custom Embedding Layer with Word2vec weights\n",
        " \"\"\"\n",
        "\n",
        "def createEmbeddingLayer(wordIndex,not_static):\n",
        "  a=[]\n",
        "  embedding_dim=300\n",
        "  vocabulary_size=len(wordIndex)+1\n",
        "  embedding_matrix = np.zeros((vocabulary_size, embedding_dim))\n",
        "  missed=0\n",
        "  for word, i in wordIndex.items():\n",
        "    \n",
        "          \n",
        "      try:\n",
        "          embedding_vector = word_vectors[word] # or fast text\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "            \n",
        "\n",
        "      except KeyError: # If word is not found in the word2vec vocabulary , assign random weights\n",
        "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),embedding_dim)\n",
        "        missed+=1\n",
        "        a.append(word)\n",
        "\n",
        "  print('missed_words :' , missed)\n",
        "\n",
        "  custom_embedding_layer = Embedding(vocabulary_size,\n",
        "                                embedding_dim,\n",
        "                                weights=[embedding_matrix],\n",
        "                                trainable=not_static # Controls the updating weights )\n",
        "  return custom_embedding_layer\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8j38UqiVjId",
        "colab_type": "text"
      },
      "source": [
        "## BiLSTM / Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2F_zFFfEVl8q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "BiLSTM-Attention Network\n",
        "\n",
        "Benefited from https://github.com/ShawnyXiao/TextClassification-Keras#5-textattbirnn\n",
        "\n",
        "Implementation of https://arxiv.org/abs/1512.08756\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import initializers, regularizers, constraints\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "class Attention_feed_forward(Layer):\n",
        "    def __init__(self, step_dim,\n",
        "                 W_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "     \n",
        "        self.supports_masking = True\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        self.step_dim = step_dim\n",
        "        self.features_dim = 0\n",
        "\n",
        "        super(Attention_feed_forward, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight( shape=(input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='w',\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        self.features_dim = input_shape[-1]\n",
        "\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight(shape=(input_shape[1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='b',\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        features_dim = self.features_dim\n",
        "        step_dim = self.step_dim\n",
        "\n",
        "        e = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))  \n",
        "        if self.bias:\n",
        "            e += self.b\n",
        "        e = K.tanh(e)\n",
        "\n",
        "        a = K.exp(e)\n",
        "        if mask is not None:\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "  \n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "        a = K.expand_dims(a)\n",
        "\n",
        "        c = K.sum(a * x, axis=1)\n",
        "        return c\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0], self.features_dim\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCeITLUsVtDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Embedding, Dense, Bidirectional,LSTM\n",
        "\n",
        "def sigmoid(x): \n",
        "    return 1.0/(1 + np.exp(-x))\n",
        "\n",
        "\n",
        "class Attention_BiLSTM(object):\n",
        "    def __init__(self, maxlen, embedding_dims,\n",
        "                 class_num=1,\n",
        "                 last_activation='sigmoid'):\n",
        "        self.maxlen = maxlen\n",
        "        self.embedding_dims = embedding_dims\n",
        "        self.class_num = class_num\n",
        "        self.last_activation = last_activation\n",
        "\n",
        "    def get_model(self):\n",
        "        input = Input((self.maxlen,))\n",
        "        embedding = createRandomTextEmbeddingLayer(wordIndex_initial,True)(input) #change\n",
        "        x = Bidirectional(LSTM(128, return_sequences=True))(embedding)  # LSTM or GRU\n",
        "        x = Attention_feed_forward(self.maxlen)(x)\n",
        "\n",
        "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
        "        model = Model(inputs=input, outputs=output)\n",
        "        return model\n",
        "\n",
        "#Benefited from https://github.com/ShawnyXiao/TextClassification-Keras#5-textattbirnn\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzFQyPNZV15B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(X_train_initial,X_test_initial,y_train,y_test) : \n",
        "  bilstm_attention_model= Attention_BiLSTM(50, 100).get_model()\n",
        "\n",
        "  early_stopping = [EarlyStopping(monitor='val_loss',min_delta=0,restore_best_weights=True, patience=10,verbose=1, mode='auto')]\n",
        "  bilstm_attention_model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['acc',f1_m,precision_m, recall_m])\n",
        "  bilstm_attention_model.summary()\n",
        "\n",
        "  bilstm_attention_model.fit(X_train_initial, y_train,\n",
        "            batch_size=16,\n",
        "            epochs=30,\n",
        "            callbacks=early_stopping,\n",
        "            validation_split=0.1)\n",
        "\n",
        "  loss, accuracy, f1_score, precision, recall = bilstm_attention_model.evaluate(X_train_initial, y_train, verbose=1)\n",
        "  print(\"Attention- BiRNN Training Loss: {:.4f}\".format(loss))\n",
        "  print(\"Attention- BiRNN Accuracy: {:.4f}\".format(accuracy))\n",
        "  print(\"Attention- BiRNN f1 score: {:.4f}\".format(f1_score))\n",
        "  print(\"Attention- BiRNN Precision: {:.4f}\".format(precision))\n",
        "  print(\"Attention- BiRNN Recall: {:.4f}\".format(recall))\n",
        "\n",
        "  loss, accuracy, f1_score, precision, recall = bilstm_attention_model.evaluate(X_test_initial, y_test, verbose=1)\n",
        "  print(\"Attention- BiRNN Test Loss: {:.4f}\".format(loss))\n",
        "  print(\"Attention- BiRNN Test Accuracy: {:.4f}\".format(accuracy))\n",
        "  print(\"Attention- BiRNN Test f1 score: {:.4f}\".format(f1_score))\n",
        "  print(\"Attention- BiRNN Test Precision: {:.4f}\".format(precision))\n",
        "  print(\"Attention- BiRNN Test Recall: {:.4f}\".format(recall))\n",
        " \n",
        "  probs = bilstm_attention_model.predict(X_test_initial, verbose=1)\n",
        "  print('lenght of probs : ' ,len(probs))\n",
        "  predicted_classes=[0 if i < 0.5 else 1 for i in probs]\n",
        " \n",
        "\n",
        "  print(classification_report(y_test, predicted_classes,digits=3))\n",
        "  return (predicted_classes,probs)\n",
        "\n",
        "\n",
        "prediction_attention,probs_attention=train(np.array(X_train_initial),np.array(X_test_initial),np.array(Y_TRAIN_ENCODED_FULL),np.array(Y_TEST_ENCODED_FULL))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}