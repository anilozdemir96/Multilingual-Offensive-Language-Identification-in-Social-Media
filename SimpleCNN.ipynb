{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "singleCNN_Offensevalturk.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvOhQBLWPTC0",
        "colab_type": "code",
        "outputId": "f5236a0e-365d-4d1a-8006-bf8595e3e2d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 898
        }
      },
      "source": [
        " \"\"\"\n",
        "Created on Tue Oct 15 00:14:08 2019\n",
        "@author: anil\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "     \n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from nltk.tokenize import word_tokenize \n",
        "import nltk\n",
        "nltk.download(\"popular\")\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "!pip install -q keras\n",
        "import keras\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "print(accelerator)\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import csv\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "cu80\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6HarFFqRCv_",
        "colab_type": "text"
      },
      "source": [
        "## Data Reading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MU-IAuVzyIeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "VALID MODE WİTH DOWNSAMPLING\n",
        "\"\"\"\n",
        "\n",
        "train=pd.read_csv('/content/drive/My Drive/TURKISH-DATA/TRAIN-TURK-ENCODED')\n",
        "test=pd.read_csv('/content/drive/My Drive/TURKISH-DATA/TEST-TURK-ENCODED')\n",
        "\n",
        "X_train_FULL=train[['id','tweet_initial_nontoken','tweet','tweet_initial','subtask_a']]\n",
        "Y_TRAIN_ENCODED_FULL=train['subtask_a']\n",
        "X_test_FULL=test[['id','tweet_initial_nontoken','tweet','tweet_initial','subtask_a']]\n",
        "Y_TEST_ENCODED_FULL=test['subtask_a']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grAV10Z5yIpt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "TEST MODE WİTH DOWNSAMPLING\n",
        "\"\"\"\n",
        "\n",
        "train=pd.read_csv('/content/drive/My Drive/TURKISH-DATA/TRAIN-TURK-ENCODED')\n",
        "test=pd.read_csv('/content/drive/My Drive/TURKISH-DATA/TEST-TURK-ENCODED')\n",
        "\n",
        "X_train=train[['id','tweet_initial_nontoken','tweet','tweet_initial','subtask_a']]\n",
        "y_train_encoded=train['subtask_a']\n",
        "X_test=test[['id','tweet_initial_nontoken','tweet','tweet_initial','subtask_a']]\n",
        "y_test_encoded=test['subtask_a']\n",
        "\n",
        "import csv\n",
        "X_train_FULL=pd.concat([X_train,X_test])\n",
        "X_test_FULL=pd.read_csv('/content/drive/My Drive/TURKISH-DATA/offenseval-tr-testset-v1.tsv',sep='\\t',encoding='utf8',quoting=csv.QUOTE_NONE)\n",
        "Y_TRAIN_ENCODED_FULL=pd.concat([y_train_encoded,y_test_encoded])\n",
        "Y_TEST=pd.read_csv('/content/drive/My Drive/TURKISH-DATA/turkish-goldlabels.tsv',sep=',',encoding='utf8',quoting=csv.QUOTE_NONE,header=None)\n",
        "Y_TEST_ENCODED_FULL = [1 if i ==  'OFF' else 0 for i in Y_TEST[1]]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CCKqQSlQ3ta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "Test mode \n",
        "\n",
        "\"\"\"\n",
        "olid_training=pd.read_csv(\"/content/drive/My Drive/OFFENSEVAL20-DATA/offenseval-tr-training-v1.tsv\",sep=\"\\t\")\n",
        "X_train_FULL=olid_training[[\"id\",\"tweet\",\"subtask_a\"]] \n",
        "Y_train_FULL=olid_training[\"subtask_a\"]\n",
        "X_test_FULL=pd.read_csv('/content/drive/My Drive/TURKISH-DATA/offenseval-tr-testset-v1.tsv',sep='\\t',encoding='utf8',quoting=csv.QUOTE_NONE)\n",
        "Y_TEST=pd.read_csv('/content/drive/My Drive/TURKISH-DATA/turkish-goldlabels.tsv',sep=',',encoding='utf8',quoting=csv.QUOTE_NONE,header=None)\n",
        "Y_TRAIN_ENCODED_FULL=[1 if i ==  'OFF' else 0 for i in Y_train_FULL]\n",
        "Y_TEST_ENCODED_FULL = [1 if i ==  'OFF' else 0 for i in Y_TEST[1]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmGPmm1aFY5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"VALID MODE\n",
        "\"\"\"\n",
        "from nltk.tokenize import word_tokenize \n",
        "import nltk\n",
        "olid_training=pd.read_csv(\"/content/drive/My Drive/OFFENSEVAL20-DATA/offenseval-tr-training-v1.tsv\",sep=\"\\t\")\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_olid, test_olid = train_test_split(olid_training, test_size=0.20,random_state=11)\n",
        "\n",
        "X_train_FULL=train_olid[[\"id\",\"tweet\",\"subtask_a\"]] \n",
        "Y_train_FULL=train_olid[\"subtask_a\"]\n",
        "X_test_FULL=test_olid[[\"id\",\"tweet\",\"subtask_a\"]]\n",
        "Y_test_FULL=np.array(test_olid[[\"subtask_a\"]])\n",
        "\n",
        "Y_TRAIN_ENCODED_FULL=[1 if i ==  'OFF' else 0 for i in Y_train_FULL]\n",
        "Y_TEST_ENCODED_FULL = [1 if i ==  'OFF' else 0 for i in Y_test_FULL]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVUr2tSbRFTI",
        "colab_type": "text"
      },
      "source": [
        "Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4LmRLgfQ-9o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "\t Converting all text to lowercase \n",
        "\t Removing all \"\\#\" symbols\n",
        "\t Removing all punctuation\n",
        "\t Removing @user tokens\n",
        "\"\"\"\n",
        "X_train_FULL.tweet = X_train_FULL.tweet.str.lower()\n",
        "\n",
        "filtered_tweets=[]\n",
        "for tweet in X_train_FULL[\"tweet\"]:\n",
        "    tweet_tokens = word_tokenize(tweet) \n",
        "\n",
        "    filtered_sentence = [w for w in tweet_tokens if (( w!='user' and w!='@' and w!=',' and w!= \"'\" and w!='.'and w!='#' and w!='?'))] \n",
        "      \n",
        "    filtered_tweets.append(filtered_sentence)\n",
        "\n",
        "X_train_FULL[\"tweet_initial\"]=filtered_tweets\n",
        "\n",
        "\n",
        "X_test_FULL.tweet = X_test_FULL.tweet.str.lower()\n",
        "\n",
        "filtered_tweets=[]\n",
        "for tweet in X_test_FULL[\"tweet\"]:\n",
        "    tweet_tokens = word_tokenize(tweet) \n",
        "\n",
        "    filtered_sentence = [w for w in tweet_tokens if (( w!='user' and w!='@' and w!=',' and w!= \"'\" and w!='.'and w!='#' and w!='?'))] \n",
        "      \n",
        "    filtered_tweets.append(filtered_sentence)\n",
        "\n",
        "X_test_FULL[\"tweet_initial\"]=filtered_tweets\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#FOR TEST\n",
        "\n",
        "z=[]\n",
        "for tweet in X_train_FULL[\"tweet_initial\"]:\n",
        "    d=\" \".join(tweet)\n",
        "    z.append(d)\n",
        "X_train_FULL[\"tweet_initial_nontoken\"]=z\n",
        "\n",
        "\n",
        "\n",
        "#FOR TEST\n",
        "\n",
        "z=[]\n",
        "for tweet in X_test_FULL[\"tweet_initial\"]:\n",
        "    d=\" \".join(tweet)\n",
        "    z.append(d)\n",
        "X_test_FULL[\"tweet_initial_nontoken\"]=z\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XmFNtEjRWJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Importing Keras dependencies / Write Custom Evaluation Metrices\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
        "from keras.layers.embeddings import Embedding\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "def recall_m(true_Y, pred_Y):\n",
        "        TP = K.sum(K.round(K.clip(true_Y * pred_Y, 0, 1)))\n",
        "        possible_pos = K.sum(K.round(K.clip(true_Y, 0, 1)))\n",
        "        rec = TP / (possible_pos + K.epsilon())\n",
        "        return rec\n",
        "\n",
        "def precision_m(true_Y, pred_Y):\n",
        "        true_positives = K.sum(K.round(K.clip(true_Y * pred_Y, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(pred_Y, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "def f1_m(true_Y, pred_Y):\n",
        "    pres = precision_m(true_Y, pred_Y)\n",
        "    rec = recall_m(true_Y, pred_Y)\n",
        "    return 2*((pres*rec)/(pres+rec+K.epsilon()))\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84q_PQ7zRrso",
        "colab_type": "text"
      },
      "source": [
        "## Tweeter Word2vec / Custom Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IukK4-MQRv8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fname= \"/content/drive/My Drive/Twitter/Word2Vec/w2v_model_word.vec\"\n",
        "\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "word_vectors = gensim.models.KeyedVectors.load_word2vec_format(fname)  # you can load this saved keyedvectors model later\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBfCpzlOhbX8",
        "colab_type": "text"
      },
      "source": [
        "# Tweeter FastText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKeL2xpXheOI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "from gensim.models import FastText\n",
        "\n",
        "word_vectors = gensim.models.FastText.load_fasttext_format('/content/drive/My Drive/Twitter/FastText/fastText_25022020.bin',encoding='utf-8') # use that if you want to use fasttedxt \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0MPbhyWI9KO",
        "colab_type": "text"
      },
      "source": [
        "# Public FastText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TP47sHiJBFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_vectors = gensim.models.FastText.load_fasttext_format('/content/drive/My Drive/OFFENSEVAL20-DATA/haber-P1_S0_L0.bin',encoding='utf-8') # use that if you want to use fasttedxt \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBg1YjQoys4O",
        "colab_type": "text"
      },
      "source": [
        "# Public word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBnqjhEIyxEB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "fname= \"/content/drive/My Drive/Public-Embeddings/word2vec da/haber-P0_S1_L0-100V_5E_1547559235.bin\"\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "word_vectors=gensim.models.KeyedVectors.load(fname)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4BMGRxqR6Bx",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizing / creating vocabulary and wordindex using keras functinalities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJcARFL7R_Au",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "We will use word indexes as look-up table during embedding layer.\n",
        "\"\"\"\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=98790)  #the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.\n",
        "tokenizer.fit_on_texts(X_train_FULL[\"tweet_initial_nontoken\"])\n",
        "X_train_initial = tokenizer.texts_to_sequences(X_train_FULL[\"tweet_initial_nontoken\"])\n",
        "X_test_initial = tokenizer.texts_to_sequences(X_test_FULL[\"tweet_initial_nontoken\"])\n",
        "vocab_size_initial = len(tokenizer.word_index) + 1 \n",
        "wordIndex_initial=tokenizer.word_index # it is  index\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "max_len = 50\n",
        "\n",
        "\"\"\"\n",
        "Padding\n",
        "\n",
        "\"\"\"\n",
        "X_train_initial = pad_sequences(X_train_initial, padding='post', maxlen=max_len)\n",
        "X_test_initial = pad_sequences(X_test_initial, padding='post', maxlen=max_len)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncWG_7yISOV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \"\"\"\n",
        "Custom Embedding Layer with Word2vec weights\n",
        " \"\"\"\n",
        "\n",
        "def createEmbeddingLayer(wordIndex,not_static):\n",
        "  a=[]\n",
        "  embedding_dim=300\n",
        "  vocabulary_size=len(wordIndex)+1\n",
        "  embedding_matrix = np.zeros((vocabulary_size, embedding_dim))\n",
        "  missed=0\n",
        "  for word, i in wordIndex.items():\n",
        "    \n",
        "          \n",
        "      try:\n",
        "          embedding_vector = word_vectors[word] # or fast text\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "            \n",
        "\n",
        "      except KeyError: # If word is not found in the word2vec vocabulary , assign random weights\n",
        "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),embedding_dim)\n",
        "        missed+=1\n",
        "        a.append(word)\n",
        "\n",
        "  print('missed_words :' , missed)\n",
        "\n",
        "  custom_embedding_layer = Embedding(vocabulary_size,\n",
        "                                embedding_dim,\n",
        "                                weights=[embedding_matrix],\n",
        "                                trainable=not_static )# Controls the updating weights )\n",
        "  return custom_embedding_layer\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNCtNSjWSdBD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  \"\"\" CNNLSTM Network architecture  with Keras\n",
        "\n",
        "  Inputs : Padded Sentences with indexed words ,\n",
        "\n",
        "  Outputs: Probability and Prediction (Binary) \n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  from sklearn.metrics import f1_score\n",
        "  import pickle\n",
        "\n",
        "  def cnn(vocab_size,X_train,X_test,y_train,y_test,wordIndex,trainable):\n",
        "    early_stopping = [EarlyStopping(monitor='val_loss',\n",
        "                          min_delta=0,restore_best_weights=True,\n",
        "                          patience=5,\n",
        "                          verbose=1, mode='auto')]\n",
        "    model = Sequential()\n",
        "    model.add(createEmbeddingLayer(wordIndex,trainable))\n",
        "    model.add(Dropout(0.2))\n",
        "    #model.add(Conv1D(128, 1, activation='relu'))\n",
        "    model.add(Conv1D(128, 3, activation='relu'))\n",
        "    model.add(layers.GlobalMaxPooling1D())\n",
        "    \n",
        "    #model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(layers.Dense(100, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False), metrics=['acc',f1_m,precision_m, recall_m])\n",
        "    ## Fit the model\n",
        "    model.fit(X_train, y_train, validation_split=0.1, epochs=20,callbacks=early_stopping,batch_size=32)\n",
        "    loss, accuracy, f1_score, precision, recall = model.evaluate(X_train, y_train, verbose=1)\n",
        "    print(\"cnn Training Loss: {:.4f}\".format(loss))\n",
        "    print(\"cnn Training Accuracy: {:.4f}\".format(accuracy))\n",
        "    print(\"cnn Training f1 score: {:.4f}\".format(f1_score))\n",
        "    print(\"cnn Training Precision: {:.4f}\".format(precision))\n",
        "    print(\"cnn Training Recall: {:.4f}\".format(recall))\n",
        "\n",
        "    loss, accuracy, f1_score, precision, recall = model.evaluate(X_test, y_test, verbose=1)\n",
        "    print(\"cnn Test Loss: {:.4f}\".format(loss))\n",
        "    print(\"cnn Test Accuracy: {:.4f}\".format(accuracy))\n",
        "    print(\"cnn Test f1 score: {:.4f}\".format(f1_score))\n",
        "    print(\"cnn Test Precision: {:.4f}\".format(precision))\n",
        "    print(\"cnn Test Recall: {:.4f}\".format(recall))\n",
        "\n",
        "    probs = model.predict(X_test, verbose=1)\n",
        "    predicted_classes = model.predict_classes(X_test, verbose=1)\n",
        "\n",
        "\n",
        "    #filename = 'finalized_model_lstm.sav'\n",
        "    #pickle.dump(model, open(filename, 'wb'))\n",
        "\n",
        "    print(classification_report(y_test, predicted_classes ,digits=3 ))\n",
        "\n",
        "\n",
        "    print(\"cnn  ends..\")\n",
        "    return (predicted_classes,probs)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KYhLt04DSNU",
        "colab_type": "code",
        "outputId": "a843dfe1-e24b-4e1e-ca66-60bd1efa09ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 687
        }
      },
      "source": [
        "\"\"\"\n",
        "Without Undersampled test results are in below \"public w2v\"\n",
        "\"\"\"\n",
        "prediction_cnnlstm,probs_cnnlstm=cnn(vocab_size_initial,X_train_initial,X_test_initial,Y_TRAIN_ENCODED_FULL,Y_TEST_ENCODED_FULL,wordIndex_initial,False)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "missed_words : 23841\n",
            "Train on 28149 samples, validate on 3128 samples\n",
            "Epoch 1/20\n",
            "28149/28149 [==============================] - 5s 167us/step - loss: 0.3893 - acc: 0.8412 - f1_m: 0.4517 - precision_m: 0.6268 - recall_m: 0.4004 - val_loss: 0.3313 - val_acc: 0.8664 - val_f1_m: 0.5691 - val_precision_m: 0.7534 - val_recall_m: 0.4899\n",
            "Epoch 2/20\n",
            "28149/28149 [==============================] - 4s 154us/step - loss: 0.3206 - acc: 0.8705 - f1_m: 0.5710 - precision_m: 0.7530 - recall_m: 0.5089 - val_loss: 0.3423 - val_acc: 0.8635 - val_f1_m: 0.4992 - val_precision_m: 0.8004 - val_recall_m: 0.3853\n",
            "Epoch 3/20\n",
            "28149/28149 [==============================] - 4s 159us/step - loss: 0.2869 - acc: 0.8841 - f1_m: 0.6209 - precision_m: 0.7804 - recall_m: 0.5584 - val_loss: 0.3615 - val_acc: 0.8664 - val_f1_m: 0.5261 - val_precision_m: 0.7890 - val_recall_m: 0.4253\n",
            "Epoch 4/20\n",
            "28149/28149 [==============================] - 4s 154us/step - loss: 0.2557 - acc: 0.8975 - f1_m: 0.6783 - precision_m: 0.8089 - recall_m: 0.6280 - val_loss: 0.3617 - val_acc: 0.8446 - val_f1_m: 0.5885 - val_precision_m: 0.6029 - val_recall_m: 0.6057\n",
            "Epoch 5/20\n",
            "28149/28149 [==============================] - 4s 154us/step - loss: 0.2240 - acc: 0.9090 - f1_m: 0.7197 - precision_m: 0.8328 - recall_m: 0.6776 - val_loss: 0.3843 - val_acc: 0.8571 - val_f1_m: 0.5683 - val_precision_m: 0.6901 - val_recall_m: 0.5138\n",
            "Epoch 6/20\n",
            "28149/28149 [==============================] - 4s 157us/step - loss: 0.1900 - acc: 0.9232 - f1_m: 0.7647 - precision_m: 0.8433 - recall_m: 0.7351 - val_loss: 0.4669 - val_acc: 0.8641 - val_f1_m: 0.5031 - val_precision_m: 0.7831 - val_recall_m: 0.3909\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00006: early stopping\n",
            "31277/31277 [==============================] - 2s 71us/step\n",
            "cnn Training Loss: 0.2861\n",
            "cnn Training Accuracy: 0.8855\n",
            "cnn Training f1 score: 0.6108\n",
            "cnn Training Precision: 0.7984\n",
            "cnn Training Recall: 0.5263\n",
            "3528/3528 [==============================] - 0s 78us/step\n",
            "cnn Test Loss: 0.3467\n",
            "cnn Test Accuracy: 0.8605\n",
            "cnn Test f1 score: 0.5567\n",
            "cnn Test Precision: 0.7558\n",
            "cnn Test Recall: 0.4732\n",
            "3528/3528 [==============================] - 0s 42us/step\n",
            "3528/3528 [==============================] - 0s 37us/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.877     0.960     0.916      2812\n",
            "           1      0.749     0.471     0.578       716\n",
            "\n",
            "    accuracy                          0.861      3528\n",
            "   macro avg      0.813     0.715     0.747      3528\n",
            "weighted avg      0.851     0.861     0.848      3528\n",
            "\n",
            "cnn  ends..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UImw59AaDBRt",
        "colab_type": "code",
        "outputId": "e0eca176-327d-4435-ddfc-4412b9ad7cf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        }
      },
      "source": [
        "\"\"\"\n",
        "Without Undersampled valid results are in below \"public w2v\"\n",
        "\"\"\"\n",
        "prediction_cnnlstm,probs_cnnlstm=cnn(vocab_size_initial,X_train_initial,X_test_initial,Y_TRAIN_ENCODED_FULL,Y_TEST_ENCODED_FULL,wordIndex_initial,False)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "missed_words : 18908\n",
            "Train on 22518 samples, validate on 2503 samples\n",
            "Epoch 1/20\n",
            "22518/22518 [==============================] - 4s 170us/step - loss: 0.4006 - acc: 0.8320 - f1_m: 0.4343 - precision_m: 0.6083 - recall_m: 0.3824 - val_loss: 0.3223 - val_acc: 0.8777 - val_f1_m: 0.6103 - val_precision_m: 0.7734 - val_recall_m: 0.5463\n",
            "Epoch 2/20\n",
            "22518/22518 [==============================] - 3s 152us/step - loss: 0.3229 - acc: 0.8685 - f1_m: 0.5663 - precision_m: 0.7498 - recall_m: 0.5009 - val_loss: 0.3138 - val_acc: 0.8734 - val_f1_m: 0.6112 - val_precision_m: 0.7213 - val_recall_m: 0.5764\n",
            "Epoch 3/20\n",
            "22518/22518 [==============================] - 4s 158us/step - loss: 0.2831 - acc: 0.8848 - f1_m: 0.6264 - precision_m: 0.7748 - recall_m: 0.5710 - val_loss: 0.3486 - val_acc: 0.8702 - val_f1_m: 0.4933 - val_precision_m: 0.7948 - val_recall_m: 0.3852\n",
            "Epoch 4/20\n",
            "22518/22518 [==============================] - 4s 160us/step - loss: 0.2529 - acc: 0.8987 - f1_m: 0.6783 - precision_m: 0.8093 - recall_m: 0.6292 - val_loss: 0.3244 - val_acc: 0.8761 - val_f1_m: 0.6139 - val_precision_m: 0.7290 - val_recall_m: 0.5700\n",
            "Epoch 5/20\n",
            "22518/22518 [==============================] - 4s 157us/step - loss: 0.2194 - acc: 0.9111 - f1_m: 0.7276 - precision_m: 0.8350 - recall_m: 0.6855 - val_loss: 0.3445 - val_acc: 0.8690 - val_f1_m: 0.6240 - val_precision_m: 0.6783 - val_recall_m: 0.6128\n",
            "Epoch 6/20\n",
            "22518/22518 [==============================] - 3s 154us/step - loss: 0.1869 - acc: 0.9257 - f1_m: 0.7782 - precision_m: 0.8639 - recall_m: 0.7447 - val_loss: 0.3725 - val_acc: 0.8674 - val_f1_m: 0.5807 - val_precision_m: 0.6685 - val_recall_m: 0.5456\n",
            "Epoch 7/20\n",
            "22518/22518 [==============================] - 3s 154us/step - loss: 0.1606 - acc: 0.9361 - f1_m: 0.8074 - precision_m: 0.8736 - recall_m: 0.7885 - val_loss: 0.4120 - val_acc: 0.8710 - val_f1_m: 0.5505 - val_precision_m: 0.7123 - val_recall_m: 0.4759\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00007: early stopping\n",
            "25021/25021 [==============================] - 2s 68us/step\n",
            "cnn Training Loss: 0.2500\n",
            "cnn Training Accuracy: 0.9082\n",
            "cnn Training f1 score: 0.7090\n",
            "cnn Training Precision: 0.8413\n",
            "cnn Training Recall: 0.6439\n",
            "6256/6256 [==============================] - 0s 71us/step\n",
            "cnn Test Loss: 0.3458\n",
            "cnn Test Accuracy: 0.8574\n",
            "cnn Test f1 score: 0.5246\n",
            "cnn Test Precision: 0.6744\n",
            "cnn Test Recall: 0.4635\n",
            "6256/6256 [==============================] - 0s 37us/step\n",
            "6256/6256 [==============================] - 0s 36us/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.881     0.951     0.915      5035\n",
            "           1      0.701     0.470     0.563      1221\n",
            "\n",
            "    accuracy                          0.857      6256\n",
            "   macro avg      0.791     0.711     0.739      6256\n",
            "weighted avg      0.846     0.857     0.846      6256\n",
            "\n",
            "cnn  ends..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUV_hqhYCuy7",
        "colab_type": "code",
        "outputId": "e4ec3b1d-e268-4eef-ecef-d973e12330da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        }
      },
      "source": [
        "\"\"\"\n",
        "With Undersampled test results are in below \"public w2v\"\n",
        "\"\"\"\n",
        "prediction_cnnlstm,probs_cnnlstm=cnn(vocab_size_initial,X_train_initial,X_test_initial,Y_TRAIN_ENCODED_FULL,Y_TEST_ENCODED_FULL,wordIndex_initial,False)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "missed_words : 17612\n",
            "Train on 20787 samples, validate on 2310 samples\n",
            "Epoch 1/20\n",
            "20787/20787 [==============================] - 4s 175us/step - loss: 0.4757 - acc: 0.7904 - f1_m: 0.5349 - precision_m: 0.6740 - recall_m: 0.4973 - val_loss: 0.4632 - val_acc: 0.7978 - val_f1_m: 0.5572 - val_precision_m: 0.4667 - val_recall_m: 0.7500\n",
            "Epoch 2/20\n",
            "20787/20787 [==============================] - 3s 151us/step - loss: 0.3802 - acc: 0.8370 - f1_m: 0.6426 - precision_m: 0.7653 - recall_m: 0.5901 - val_loss: 0.3291 - val_acc: 0.8658 - val_f1_m: 0.4966 - val_precision_m: 0.6489 - val_recall_m: 0.4326\n",
            "Epoch 3/20\n",
            "20787/20787 [==============================] - 3s 155us/step - loss: 0.3398 - acc: 0.8512 - f1_m: 0.6753 - precision_m: 0.7887 - recall_m: 0.6327 - val_loss: 0.3310 - val_acc: 0.8654 - val_f1_m: 0.5440 - val_precision_m: 0.6523 - val_recall_m: 0.5109\n",
            "Epoch 4/20\n",
            "20787/20787 [==============================] - 3s 155us/step - loss: 0.3032 - acc: 0.8722 - f1_m: 0.7241 - precision_m: 0.8249 - recall_m: 0.6816 - val_loss: 0.3680 - val_acc: 0.8494 - val_f1_m: 0.5708 - val_precision_m: 0.5683 - val_recall_m: 0.6152\n",
            "Epoch 5/20\n",
            "20787/20787 [==============================] - 3s 159us/step - loss: 0.2583 - acc: 0.8925 - f1_m: 0.7788 - precision_m: 0.8496 - recall_m: 0.7488 - val_loss: 0.3705 - val_acc: 0.8615 - val_f1_m: 0.5256 - val_precision_m: 0.6522 - val_recall_m: 0.4821\n",
            "Epoch 6/20\n",
            "20787/20787 [==============================] - 3s 155us/step - loss: 0.2223 - acc: 0.9089 - f1_m: 0.8139 - precision_m: 0.8725 - recall_m: 0.7890 - val_loss: 0.4386 - val_acc: 0.8247 - val_f1_m: 0.5545 - val_precision_m: 0.5064 - val_recall_m: 0.6648\n",
            "Epoch 7/20\n",
            "20787/20787 [==============================] - 3s 158us/step - loss: 0.1901 - acc: 0.9231 - f1_m: 0.8442 - precision_m: 0.8876 - recall_m: 0.8264 - val_loss: 0.4698 - val_acc: 0.8242 - val_f1_m: 0.5471 - val_precision_m: 0.5081 - val_recall_m: 0.6402\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00007: early stopping\n",
            "23097/23097 [==============================] - 2s 67us/step\n",
            "cnn Training Loss: 0.2977\n",
            "cnn Training Accuracy: 0.8676\n",
            "cnn Training f1 score: 0.6567\n",
            "cnn Training Precision: 0.8949\n",
            "cnn Training Recall: 0.5427\n",
            "3528/3528 [==============================] - 0s 77us/step\n",
            "cnn Test Loss: 0.3560\n",
            "cnn Test Accuracy: 0.8571\n",
            "cnn Test f1 score: 0.5313\n",
            "cnn Test Precision: 0.7609\n",
            "cnn Test Recall: 0.4476\n",
            "3528/3528 [==============================] - 0s 45us/step\n",
            "3528/3528 [==============================] - 0s 37us/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.873     0.961     0.915      2812\n",
            "           1      0.744     0.451     0.562       716\n",
            "\n",
            "    accuracy                          0.857      3528\n",
            "   macro avg      0.809     0.706     0.738      3528\n",
            "weighted avg      0.847     0.857     0.843      3528\n",
            "\n",
            "cnn  ends..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lp6ImAB45K6Z",
        "colab_type": "code",
        "outputId": "e8860046-5efd-4c58-a41f-670ee22a3c59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752
        }
      },
      "source": [
        "\"\"\"\n",
        "With Undersampled valid results are in below \"public w2v\"\n",
        "\"\"\"\n",
        "prediction_cnnlstm,probs_cnnlstm=cnn(vocab_size_initial,X_train_initial,X_test_initial,Y_TRAIN_ENCODED_FULL,Y_TEST_ENCODED_FULL,wordIndex_initial,False)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "missed_words : 13202\n",
            "Train on 15161 samples, validate on 1685 samples\n",
            "Epoch 1/20\n",
            "15161/15161 [==============================] - 3s 178us/step - loss: 0.4985 - acc: 0.7708 - f1_m: 0.5305 - precision_m: 0.6591 - recall_m: 0.5019 - val_loss: 0.4337 - val_acc: 0.8036 - val_f1_m: 0.6457 - val_precision_m: 0.6524 - val_recall_m: 0.6673\n",
            "Epoch 2/20\n",
            "15161/15161 [==============================] - 2s 147us/step - loss: 0.3877 - acc: 0.8298 - f1_m: 0.6584 - precision_m: 0.7722 - recall_m: 0.6127 - val_loss: 0.4279 - val_acc: 0.8160 - val_f1_m: 0.5902 - val_precision_m: 0.7794 - val_recall_m: 0.4945\n",
            "Epoch 3/20\n",
            "15161/15161 [==============================] - 2s 148us/step - loss: 0.3365 - acc: 0.8552 - f1_m: 0.7114 - precision_m: 0.8075 - recall_m: 0.6721 - val_loss: 0.4265 - val_acc: 0.8172 - val_f1_m: 0.6213 - val_precision_m: 0.7596 - val_recall_m: 0.5477\n",
            "Epoch 4/20\n",
            "15161/15161 [==============================] - 2s 154us/step - loss: 0.2905 - acc: 0.8758 - f1_m: 0.7568 - precision_m: 0.8380 - recall_m: 0.7238 - val_loss: 0.4607 - val_acc: 0.7988 - val_f1_m: 0.6471 - val_precision_m: 0.6344 - val_recall_m: 0.6836\n",
            "Epoch 5/20\n",
            "15161/15161 [==============================] - 2s 152us/step - loss: 0.2372 - acc: 0.9014 - f1_m: 0.8094 - precision_m: 0.8664 - recall_m: 0.7841 - val_loss: 0.5307 - val_acc: 0.8095 - val_f1_m: 0.5633 - val_precision_m: 0.7700 - val_recall_m: 0.4616\n",
            "Epoch 6/20\n",
            "15161/15161 [==============================] - 2s 150us/step - loss: 0.2009 - acc: 0.9184 - f1_m: 0.8463 - precision_m: 0.8877 - recall_m: 0.8280 - val_loss: 0.5434 - val_acc: 0.8107 - val_f1_m: 0.5931 - val_precision_m: 0.7453 - val_recall_m: 0.5158\n",
            "Epoch 7/20\n",
            "15161/15161 [==============================] - 2s 149us/step - loss: 0.1759 - acc: 0.9288 - f1_m: 0.8653 - precision_m: 0.8982 - recall_m: 0.8528 - val_loss: 0.5975 - val_acc: 0.8024 - val_f1_m: 0.5918 - val_precision_m: 0.7236 - val_recall_m: 0.5208\n",
            "Epoch 8/20\n",
            "15161/15161 [==============================] - 2s 153us/step - loss: 0.1452 - acc: 0.9433 - f1_m: 0.8919 - precision_m: 0.9182 - recall_m: 0.8832 - val_loss: 0.6437 - val_acc: 0.8053 - val_f1_m: 0.6128 - val_precision_m: 0.6957 - val_recall_m: 0.5698\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00008: early stopping\n",
            "16846/16846 [==============================] - 1s 68us/step\n",
            "cnn Training Loss: 0.2444\n",
            "cnn Training Accuracy: 0.9009\n",
            "cnn Training f1 score: 0.8003\n",
            "cnn Training Precision: 0.9108\n",
            "cnn Training Recall: 0.7307\n",
            "6251/6251 [==============================] - 0s 71us/step\n",
            "cnn Test Loss: 0.3823\n",
            "cnn Test Accuracy: 0.8467\n",
            "cnn Test f1 score: 0.5297\n",
            "cnn Test Precision: 0.6061\n",
            "cnn Test Recall: 0.5091\n",
            "6251/6251 [==============================] - 0s 37us/step\n",
            "6251/6251 [==============================] - 0s 35us/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.889     0.927     0.907      5057\n",
            "           1      0.621     0.508     0.559      1194\n",
            "\n",
            "    accuracy                          0.847      6251\n",
            "   macro avg      0.755     0.717     0.733      6251\n",
            "weighted avg      0.837     0.847     0.841      6251\n",
            "\n",
            "cnn  ends..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyN_BDzzw8a6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnnlstm_test_full_result= pd.DataFrame()\n",
        "cnnlstm_test_full_result['CNN-LSTM-PREDICTON']=list(np.squeeze(prediction_cnnlstm))\n",
        "cnnlstm_test_full_result['CNN-LSTM-PROBS']=probs_cnnlstm\n",
        "cnnlstm_test_full_result['Gold-label']=Y_TEST_ENCODED_FULL\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wH4yP7lb3oFM",
        "colab_type": "code",
        "outputId": "719a1b5e-b3bd-4c2d-d51b-788a6677d98e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "source": [
        "cnnlstm_test_full_result"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CNN-LSTM-PREDICTON</th>\n",
              "      <th>CNN-LSTM-PROBS</th>\n",
              "      <th>Gold-label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.057274</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0.065222</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0.042271</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0.564768</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0.223791</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3523</th>\n",
              "      <td>0</td>\n",
              "      <td>0.023876</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3524</th>\n",
              "      <td>0</td>\n",
              "      <td>0.012241</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3525</th>\n",
              "      <td>0</td>\n",
              "      <td>0.020862</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3526</th>\n",
              "      <td>0</td>\n",
              "      <td>0.043844</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3527</th>\n",
              "      <td>0</td>\n",
              "      <td>0.014096</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3528 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      CNN-LSTM-PREDICTON  CNN-LSTM-PROBS  Gold-label\n",
              "0                      0        0.057274           0\n",
              "1                      0        0.065222           0\n",
              "2                      0        0.042271           0\n",
              "3                      1        0.564768           1\n",
              "4                      0        0.223791           1\n",
              "...                  ...             ...         ...\n",
              "3523                   0        0.023876           0\n",
              "3524                   0        0.012241           0\n",
              "3525                   0        0.020862           0\n",
              "3526                   0        0.043844           0\n",
              "3527                   0        0.014096           0\n",
              "\n",
              "[3528 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHpGJPXbyijg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnnlstm_test_full_result.to_csv('cnnlstm_test_full_result.csv',header=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWwWmH-G4IZi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnnlstm_valid_full_result= pd.DataFrame()\n",
        "cnnlstm_valid_full_result['CNN-LSTM-PREDICTON']=list(np.squeeze(prediction_cnnlstm))\n",
        "cnnlstm_valid_full_result['CNN-LSTM-PROBS']=probs_cnnlstm\n",
        "cnnlstm_valid_full_result['Gold-label']=Y_TEST_ENCODED_FULL\n",
        "\n",
        "cnnlstm_valid_full_result.to_csv('cnnlstm_valid_full_result.csv',header=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIAj4rgXHCP4",
        "colab_type": "code",
        "outputId": "9f76bc06-dda2-42b5-cef1-a50fb8e98547",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "source": [
        "cnnlstm_valid_full_result"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CNN-LSTM-PREDICTON</th>\n",
              "      <th>CNN-LSTM-PROBS</th>\n",
              "      <th>Gold-label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.094757</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0.109200</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0.030299</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0.073449</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0.030752</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6251</th>\n",
              "      <td>0</td>\n",
              "      <td>0.030913</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6252</th>\n",
              "      <td>0</td>\n",
              "      <td>0.179470</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6253</th>\n",
              "      <td>0</td>\n",
              "      <td>0.026681</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6254</th>\n",
              "      <td>0</td>\n",
              "      <td>0.080385</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6255</th>\n",
              "      <td>0</td>\n",
              "      <td>0.041867</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6256 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      CNN-LSTM-PREDICTON  CNN-LSTM-PROBS  Gold-label\n",
              "0                      0        0.094757           0\n",
              "1                      0        0.109200           0\n",
              "2                      0        0.030299           0\n",
              "3                      0        0.073449           0\n",
              "4                      0        0.030752           0\n",
              "...                  ...             ...         ...\n",
              "6251                   0        0.030913           0\n",
              "6252                   0        0.179470           0\n",
              "6253                   0        0.026681           0\n",
              "6254                   0        0.080385           1\n",
              "6255                   0        0.041867           1\n",
              "\n",
              "[6256 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aK94JNTgKMtl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}